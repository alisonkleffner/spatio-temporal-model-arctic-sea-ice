---
title: A Spatio-Temporal Model for Arctic Sea Ice

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Alison Kleffner 1
  # thanks: The authors gratefully acknowledge ...
  affiliation: Department of Statistics, University of Nebraska - Lincoln
  
- name: Susan VanderPlas 2
  affiliation: Department of Statistics, University of Nebraska - Lincoln
  
- name: Yawen Guan 3
  affiliation: Department of Statistics, University of Nebraska - Lincoln

keywords:
- spatial clustering
- non-stationary
- Gaussian process. 

abstract: |
  Arctic Sea Ice is a barrier between the warm air of the ocean and the atmosphere, thus playing an important role in the climate. When narrow linear cracks (leads) form in the sea ice, the heat from the ocean is then released into the atmosphere. To estimate where cracks may form, motion data from the RADARSAT Geophysical Processing System (RGPS) are analyzed. The RGPS provides a set of trajectories (or cells) to trace the displacements of sea ice, however, chunks of data are missing due to the data collection method. We propose a spatial clustering and interpolation method that allows us to infer missing observations and estimate, where a crack may form. To do this feature inputs were created for KNN clustering by creating a bounding box around each trajectory, resulting in trajectories being assigned a cluster. A crack is considered to have formed on the boundary between different clusters. Within the clusters, spatiotemporal interpolation method is used to infer missing locations. Our clustering approach is then compared to other methods to determine ice crack formation, and cross-validation is used to assess our interpolation method.

bibliography: bibliography.bib
output: rticles::asa_article

header-includes:
   - \usepackage[dvipsnames]{xcolor} % colors
   - \newcommand{\aak}[1]{{\textcolor{blue}{#1}}}
   - \newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
   - \newcommand{\yg}[1]{{\textcolor{Green}{#1}}}
   - \usepackage[capitalise]{cleveref}
   - \newcommand\pcref[1]{(\cref{#1})}
   - \usepackage{algorithm,algpseudocode,booktabs}
---

```{r setup, include = F}
knitr::opts_chunk$set(
  echo = F, eval = T, message = F, warning = F,
  fig.width = 6, fig.height = 4,  fig.align = 'center',
  out.width = "\\linewidth", dpi = 300, 
  tidy = T, tidy.opts=list(width.cutoff=45),
  fig.pos = "tbp",
  out.extra = ""
)
```


```{r load-packages, include = F}

library(knitr)
library(FNN)
library(expss)
library(reshape2)
library(sf)
library(ggpubr)
library(tidyverse)
library(fields)
library(crosstalk)
library(plotly)
library(factoextra)
library(sp)
library(GpGp)
library(mvtnorm)
library(expss)
library(FNN)
library(gridExtra)
library(RColorBrewer)
library(kableExtra)
library(zoo)

```

```{r load-functions, include = T}

## Needed Functions
bbox_summary <- function(df) {
  # Function takes data frame with k, xmap, ymap
  # and returns a bounding box and trajectory
  df <- arrange(df, k) # sort by k
  df %>%
    summarize(
      x = mean(xmap), y = mean(ymap),
      xmin = min(xmap), xmax = max(xmap),
      ymin = min(ymap), ymax = max(ymap),
      xbox = xmax - xmin,
      ybox = ymax - ymin,
      dx = xmap[n()] - xmap[1],
      dy = ymap[n()] - ymap[1],
      angle = atan2(dy, dx),
      kmin = min(k),
      kmax = max(k),
      npts = n(),
    )
}

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
#New Cluster Membership based on 10 NN

lonely <- function(frame, tf){
  anim_p <- filter(frame, t == tf)[,c(1,5,6,9,10)]
  anim_p <- anim_p %>% group_by(gpid) %>% summarise(xmap_m = mean(xmap),ymap_m = mean(ymap),clust_m = mean(as.numeric(clust))) #Averaging to deal with mult observations at a t value
  colnames(anim_p) <- c("gpid", "xmap", "ymap", "clust")
  n = n_distinct(anim_p$gpid)
  test=get.knn(anim_p[,c(2,3)], k=10)
  df_new<- rownames_to_column(anim_p, "index")
  df_new <- data.frame(df_new,test[1])
  df_new1<- vlookup(df_new$nn.index.1,df_new,5,1)
  df_new2<- vlookup(df_new$nn.index.2,df_new,5,1)
  df_new3<- vlookup(df_new$nn.index.3,df_new,5,1)
  df_new4<- vlookup(df_new$nn.index.4,df_new,5,1)
  df_new5<- vlookup(df_new$nn.index.5,df_new,5,1)
  df_new6<- vlookup(df_new$nn.index.6,df_new,5,1)
  df_new7<- vlookup(df_new$nn.index.7,df_new,5,1)
  df_new8<- vlookup(df_new$nn.index.8,df_new,5,1)
  df_new9<- vlookup(df_new$nn.index.9,df_new,5,1)
  df_new10<- vlookup(df_new$nn.index.10,df_new,5,1)
  df_new <- data.frame(df_new, df_new1, df_new2, df_new3, df_new4, df_new5, df_new6, df_new7, df_new8, df_new9, df_new10)
  data <- data.frame(NA_col = rep(NA,n))
  for (i in 1:n){
    x = getmode(df_new[i,c(16:25)])
    data[i,] <- x
  }
  df_new <- data.frame(df_new,data)
  df_new <- df_new[,c(1:5,26)]
  anim_plot <- df_new %>%
    ggplot(aes(x = xmap, y = ymap, group = gpid, ids = gpid,
               color = factor(NA_col), fill = factor(NA_col))) + 
    geom_point() + 
    #scale_shape_manual("Imputed", values = c("FALSE" = 15, "TRUE" = 0)) + 
    scale_color_viridis_d() + 
    scale_fill_viridis_d() + ggtitle("test")
  
  plotly::ggplotly(anim_plot)
  new <- df_new[,-c(5)]
  colnames(new) <- c("index", "gpid", "xmap", "ymap", "clust")
  return(new)
}

#Getting Polygons
#Get Data into the format that we need. 
polygon_data2 <- function(frame){
  my.sf.point <- st_as_sf(x = frame, 
                          coords = c("xmap", "ymap"))
  polys = st_sf(
    aggregate(
      my.sf.point$geometry,
      list(my.sf.point$n),
      function(g){
        st_cast(st_combine(g),"POLYGON")
      }
    ))
  #Should be final data frame needed
  y <- st_cast(polys, "MULTIPOLYGON")
  return(y)
}
#Get coordinates of the polygons
coordinates2 <- function(frame, n){
  coord <- frame[,c(3,4)]
  ConvexHull = chull(coord)
  x <- data.frame(coord[ConvexHull,],n )
  return(x)
}

#Getting Intersections
get_int <- function(timeframe,a,b) {
  intersections_pp <- st_intersection(a, b) %>% mutate(int_name = paste0(Group.1, "-", Group.1.1))
  points <-st_as_sf(timeframe, coords = c("xmap", "ymap"))
  polyg <-intersections_pp$geometry
  z <- st_intersects(polyg,points) #Return index of points within each intersection
  FrameData <- lapply(z, function(x) as.data.frame(x))
  new<- melt(FrameData)[,-1] #gives list of index and intersection group
  colnames(new) <- c("index", "intersection")
  n <- merge(timeframe, new, by = "index")[,-1] #Data frame of 
  df_n <- subset(timeframe,!(timeframe$gpid%in%n$gpid))[,-1] 
  return(n) #If want data set of gpids and intersections
} #If want data set of gpids and intersections

get_int2 <- function(timeframe,a,b) {
  intersections_pp <- st_intersection(a, b) %>% mutate(int_name = paste0(Group.1, "-", Group.1.1))
  points <-st_as_sf(timeframe, coords = c("xmap", "ymap"))
  polyg <-intersections_pp$geometry
  return(polyg)
}

int_plot <- function(timeframe,a,b){
  intersections_pp <- st_intersection(a, b) %>% mutate(int_name = paste0(Group.1, "-", Group.1.1))
  ggplot() + #If want to plot the clusters
  geom_sf(data = intersections_pp) + ggtitle ("(c). Week 1/2 Intersections")
  #, aes(fill = int_name))
}

nn_df <- function(d, t, og_df){
  d_t <- filter(d, t==t)
  d_new<- rownames_to_column(d_t, "index")
  g <- knn(d_t[,1:2], og_df[,5:6], cl = d_t$cluster, k=1)
  indices <- attr(g, "nn.index")
  og_new <- data.frame(og_df,indices) #get 6th column with nn index
  og_new$ux<- vlookup(og_new$indices,d_new,6,1)
  og_new$uy<- vlookup(og_new$indices,d_new,7,1)
  og_new$xnew <- og_new$xnew+ og_new$ux
  og_new$ynew <- og_new$ynew+ og_new$uy
  og_new$t <- og_new$t+1
  final <- og_new[,-7]
  return(final)
}

week1 <- function(x_ini_disp, y_ini_disp, rho_x, rho_y, init_grid){
q_t1 <- data.frame(xmap = x_ini_disp+cg$xmap, ymap = y_ini_disp+cg$ymap, t = cg$t+1, cluster = cg$cluster) #, gpid = 1:400)
q_t2 <- data.frame(xmap = (rho_x*(q_t1$xmap-cg$x)) + q_t1$xmap, ymap = (rho_y*(q_t1$ymap-cg$y)) + q_t1$ymap, t = q_t1$t+1, cluster = cg$cluster) #, gpid = 1:400)
q_t3 <- data.frame(xmap = (rho_x*(q_t2$xmap-q_t1$xmap)) + q_t2$xmap, ymap = (rho_y*(q_t2$ymap-q_t1$ymap)) + q_t2$ymap, t = q_t2$t+1, cluster = cg$cluster) #, gpid = 1:400)
q_t4 <- data.frame(xmap = (rho_x*(q_t3$xmap-q_t2$xmap)) + q_t3$xmap, ymap = (rho_y*(q_t3$ymap-q_t2$ymap)) + q_t3$ymap, t = q_t3$t+1, cluster = cg$cluster) #, gpid = 1:400)
q_t5 <- data.frame(xmap = (rho_x*(q_t4$xmap-q_t3$xmap)) + q_t4$xmap, ymap = (rho_y*(q_t4$ymap-q_t3$ymap)) + q_t4$ymap, t = q_t4$t+1, cluster = cg$cluster) #, gpid = 1:400)
q_t6 <- data.frame(xmap = (rho_x*(q_t5$xmap-q_t4$xmap)) + q_t5$xmap, ymap = (rho_y*(q_t5$ymap-q_t4$ymap)) + q_t5$ymap, t = q_t5$t+1, cluster = cg$cluster) #, gpid = 1:400)

all_w1 <- rbind(cg,q_t1,q_t2,q_t3,q_t4,q_t5,q_t6)
return(all_w1)
}

```


```{r sim-inter-functions}

polygon_sim <- function(tp, preds){
feat_t1 <- filter(preds, t == tp)
feat_t1_og <- rownames_to_column(feat_t1, "index")
points <- st_as_sf(feat_t1, coords = c("xmap", "ymap"))
q_smallest <- filter(feat_t1_og, clust == 1)
###1###
coord <- q_smallest[,c(6,7)]
ConvexHull = chull(coord)
coord_1 <- data.frame(coord[ConvexHull,],1)
my.sf.point <- st_as_sf(x = coord_1, 
                        coords = c("xmap", "ymap"))
polys = st_sf(
  aggregate(
    my.sf.point$geometry,
    list(my.sf.point$X1),
    function(g){
      st_cast(st_combine(g),"POLYGON")
    }
  ))
#Should be final data frame needed
y1 <- st_cast(polys, "MULTIPOLYGON")
polyg <- y1$geometry
z <- st_intersects(polyg,points)
z_idex <- data.frame(z[1])
colnames(z_idex) <- "index"
df <- merge(z_idex, feat_t1_og, by= "index")
df2 <- subset(feat_t1_og,!(feat_t1_og$gpid%in%df$gpid))
###2###
coord2 <- df2[,c(6,7)]
ConvexHull2 = chull(coord2)
coord_2 <- data.frame(coord2[ConvexHull2,],2)
my.sf.point2 <- st_as_sf(x = coord_2, 
                         coords = c("xmap", "ymap"))
polys2 = st_sf(
  aggregate(
    my.sf.point2$geometry,
    list(my.sf.point2$X2),
    function(g){
      st_cast(st_combine(g),"POLYGON")
    }
  ))
#Should be final data frame needed
y2 <- st_cast(polys2, "MULTIPOLYGON")
polyg2 <- y2$geometry
z2 <- st_intersects(polyg2,points)
z_idex2 <- data.frame(z2[1])
colnames(z_idex2) <- "index"
df3 <- merge(z_idex2, df2, by= "index")
df4 <- subset(df2,!(df2$gpid%in%df3$gpid))
y = rbind(y1, y2)
return(y)
}

grid_tp_sim <- function(frame, tp, m){
  time_og <- filter(frame, t==tp)
  pts <- st_as_sf(time_og, coords = c("xmap", "ymap")) 
  grid_50 <- st_make_grid(m, cellsize = c(5, 5)) %>% 
    st_sf(grid_id = 1:length(.))
  grid_lab <- st_centroid(grid_50) %>% cbind(st_coordinates(.))
  pts_grd <- pts %>% st_join(grid_50, join = st_intersects) %>% as.data.frame
  all_pts_grd <- left_join(pts_grd,grid_lab,by= "grid_id")
  all_pts_grd2 <- all_pts_grd %>% distinct(gpid, .keep_all= TRUE)
  g1 <- all_pts_grd2[,c(1,6,8,7,9,10,11)]
  g1 <- rownames_to_column(g1, "index")
  return(g1)
}

sim_known <- function(m,int, tp, grid,w){
  int1 <- filter(m, intersection == int)[,-c(2,3,4,5,6,7)]
  int_t1 <- right_join(grid, int1, by = "gpid")
  anim_t1 <- filter(w, t == tp)
  int_t1_2 <- inner_join(int_t1, anim_t1, by = "gpid")
  final_int_t1 <- int_t1_2[,c(2,4,10,11,14,15,17)]
  t1_notimputed =  filter(final_int_t1, !is.na(xmap))
  colnames(t1_notimputed) <- c("gpid", "gpid_id", "intersection", "obs_time", "xmap","ymap", "clust")
  return(t1_notimputed)
}

sim_estimate <- function(m, int, tp, grid, w){
  int1 <- filter(m, intersection == int)[,-c(2,3,4,5,6,7)]
  int_t1 <- right_join(grid, int1, by = "gpid")
  anim_t1 <- filter(w, t == tp)
  int_t1_2 <- inner_join(int_t1, anim_t1, by = "gpid")
  t1_imputed =  filter(int_t1_2, is.na(xmap))
  final_int_t1 <- t1_imputed[,c(2,4,6,7,10,11,17)]
  colnames(final_int_t1) <- c("gpid", "gpid_id",  "xmap","ymap", "intersection", "obs_time", "clust")
  return(final_int_t1)
}


lat_est_sim <- function(int, m, m_new, tp, frame, frame2){
  grid <- grid_tp_sim(frame2,tp,m)
  known1 <- sim_known(m_new,int,tp-1,grid,frame)
  known2 <- sim_known(m_new,int,tp,grid, frame)
  known3 <- sim_known(m_new,int,tp+1,grid, frame)
  known4 <- sim_known(m_new,int,tp+2,grid, frame)
  if(nrow(known3) ==0){known <- rbind(known1, known2, known4)}
  else(known <- rbind(known1, known2, known3))
  loc <- known[,c("xmap","ymap", "obs_time")] ## locations that I will use to fit the model with time (xmap, ymap,obs_time)
  locs<-as.matrix(loc)                                ## convert them into matrix
  X <- as.matrix( rep(1,nrow(locs)))                  ## intercept
  N <- nrow(known)
  Y <- known[,5]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #needs to go to number of rows - 1
  return(fit)
}

long_est_sim <- function(int, m, m_new, tp, frame, frame2){
  grid <- grid_tp_sim(frame2,tp,m)
  known1 <- sim_known(m_new,int,tp-1,grid,frame)
  known2 <- sim_known(m_new,int,tp,grid, frame)
  known3 <- sim_known(m_new,int,tp+1,grid, frame)
  known4 <- sim_known(m_new,int,tp+2,grid, frame)
  if(nrow(known3) ==0){known <- rbind(known1, known2, known4)}
  else(known <- rbind(known1, known2, known3))
  loc <- known[,c("xmap","ymap", "obs_time")] ## locations that I will use to fit the model with time (xmap, ymap,obs_time)
  locs<-as.matrix(loc)                                ## convert them into matrix
  X <- as.matrix( rep(1,nrow(locs)))                  ## intercept
  N <- nrow(known)
  Y <- known[,6]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #needs to go to number of rows - 1
  return(fit)
}

lat_pred_sim <- function(est, int, m, m_new, tp, frame, frame2){
  grid <- grid_tp_sim(frame2,tp,m)
  
  known_grd <- sim_estimate(m_new,int,tp,grid, frame)
  
  locs_pred <- as.matrix(known_grd[,c("xmap", "ymap","obs_time")]) # Would need to create obs_time when don't have them (round number?)
  colnames(locs_pred) <- c("xmap", "ymap", "obs_time")
  X_pred <- as.matrix( rep(1,nrow(locs_pred)))
  pred_grid<-predictions(est,locs_pred, X_pred,covparms = est$covparms, covfun_name = est$covfun_name, 
                         y_obs = est$y,locs_obs = est$locs
                         ,X_obs= est$X, beta=est$betahat, m = 10, reorder = TRUE)
  return(pred_grid) 
}

long_pred_sim <- function(est, int, m, m_new, tp, frame, frame2){
  grid <- grid_tp_sim(frame2,tp,m)
  known_grd <- sim_estimate(m_new,int,tp,grid, frame)
  
  locs_pred <- as.matrix(known_grd[,c("xmap", "ymap","obs_time")]) # Would need to create obs_time when don't have them (round number?)
  colnames(locs_pred) <- c("xmap", "ymap", "obs_time")
  X_pred <- as.matrix( rep(1,nrow(locs_pred)))
  pred_grid<-predictions(est,locs_pred, X_pred,covparms = est$covparms, covfun_name = est$covfun_name, 
                         y_obs = est$y,locs_obs = est$locs
                         ,X_obs= est$X, beta=est$betahat, m = 10, reorder = TRUE)
  return(pred_grid) 
}

both_sim_lat <- function(int, m, m_new, tp, frame, frame2){
  est <- lat_est_sim(int, m, m_new, tp, frame, frame2)
  pred<- lat_pred_sim(est, int, m, m_new, tp, frame, frame2)
  return(pred)
}

both_sim_long <- function(int, m, m_new, tp, frame, frame2){
  est <- long_est_sim(int, m, m_new, tp, frame, frame2)
  pred<- long_pred_sim(est, int, m, m_new, tp, frame, frame2)
  return(pred)
}

pred_all <- NULL
all_sim_lat <- function(r,df, df2, int_list, int_df){
  for (i in 1:4){
    tryCatch({
      pred = both_sim_lat(i,int_list,int_df,r,df, df2)
      pred_df = data.frame(pred,r)
      grid_t <- grid_tp_sim(df2,r,int_list)
      known2 <- sim_estimate(int_df,i,r,grid_t, df)
      pred_df = data.frame(pred,r, known2$gpid, i)
      pred_all = rbind(pred_all, pred_df)}, error=function(e){return(NA)})
  }
  return(pred_all)
}

all_sim_long <- function(r,df, df2, int_list, int_df){
  for (i in 1:4){
    tryCatch({
      pred = both_sim_long(i,int_list,int_df,r,df, df2)
      pred_df = data.frame(pred,r)
      grid_t <- grid_tp_sim(df2,r,int_list)
      known2 <- sim_estimate(int_df,i,r,grid_t, df)
      pred_df = data.frame(pred,r, known2$gpid, i)
      pred_all = rbind(pred_all, pred_df)}, error=function(e){return(NA)})
  }
  return(pred_all)
}


#Getting Coverage Probability 

lat_sim <- function(int, m, m_new, tp, frame, frame2){
  grid <- grid_tp_sim(frame2,tp,m)
  known1 <- sim_known(m_new,int,tp-1,grid,frame)
  known2 <- sim_known(m_new,int,tp,grid, frame)
  known3 <- sim_known(m_new,int,tp+1,grid, frame)
  known4 <- sim_known(m_new,int,tp+2,grid, frame)
  if(nrow(known3) ==0){known <- rbind(known1, known2, known4)}
  else(known <- rbind(known1, known2, known3))
  loc <- known[,c("xmap","ymap", "obs_time")] ## locations that I will use to fit the model with time (xmap, ymap,obs_time)
  locs<-as.matrix(loc)                                ## convert them into matrix
  X <- as.matrix( rep(1,nrow(locs)))                  ## intercept
  N <- nrow(known)
  Y <- known[,5]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #needs to go to number of rows - 1
  known_grd <- sim_estimate(m_new,int,tp,grid, frame)
  locs_pred <- as.matrix(known_grd[,c("xmap", "ymap","obs_time")]) # Would need to create obs_time when don't have them (round number?)
  colnames(locs_pred) <- c("xmap", "ymap", "obs_time")
  X_pred <- as.matrix(rep(1,nrow(locs_pred)))
  ncondsim <- 30
  sims <- cond_sim(fit = fit, locs_pred = locs_pred, X_pred = X_pred, covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                   ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE, nsims = ncondsim)
  ##for prediction
  pred_grid<-predictions(fit,locs_pred, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, 
                         y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  b <- list(sims,pred_grid)
  return(b)
}

long_sim <- function(int, m, m_new, tp, frame, frame2){
  grid <- grid_tp_sim(frame2,tp,m)
  known1 <- sim_known(m_new,int,tp-1,grid,frame)
  known2 <- sim_known(m_new,int,tp,grid, frame)
  known3 <- sim_known(m_new,int,tp+1,grid, frame)
  known4 <- sim_known(m_new,int,tp+2,grid, frame)
  if(nrow(known3) ==0){known <- rbind(known1, known2, known4)}
  else(known <- rbind(known1, known2, known3))
  loc <- known[,c("xmap","ymap", "obs_time")] ## locations that I will use to fit the model with time (xmap, ymap,obs_time)
  locs<-as.matrix(loc)                                ## convert them into matrix
  X <- as.matrix( rep(1,nrow(locs)))                  ## intercept
  N <- nrow(known)
  Y <- known[,6]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #needs to go to number of rows - 1
  known_grd <- sim_estimate(m_new,int,tp,grid, frame)
  locs_pred <- as.matrix(known_grd[,c("xmap", "ymap","obs_time")]) # Would need to create obs_time when don't have them (round number?)
  colnames(locs_pred) <- c("xmap", "ymap", "obs_time")
  X_pred <- as.matrix(rep(1,nrow(locs_pred)))
  ncondsim <- 30
  sims <- cond_sim(fit = fit, locs_pred = locs_pred, X_pred = X_pred, covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                   ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE, nsims = ncondsim)
  ##for prediction
  pred_grid<-predictions(fit,locs_pred, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, 
                         y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  b <- list(sims,pred_grid)
  return(b)
}

all_pred <- NULL
pred_cv <- NULL
all_lat <- function(df, df2, int_list, int_df){
for (r in 1:7){
  for (i in 1:4){
    tryCatch({
      pred = lat_sim(i,int_list,int_df,r,df, df2)
      sd_df <- sd_pred_sim(pred[[1]],pred[[2]])
      grid_t <- grid_tp_sim(df2,r,int_list)
      known2 <- sim_estimate(int_df,i,r,grid_t, df)
      df_p = data.frame(sd_df, i, r, known2$gpid)
      pred_cv = rbind.data.frame(pred_cv, df_p)}, error=function(e){return(NA)})
  }
  all_pred <- rbind.data.frame(all_pred,pred_cv)
}
  return(all_pred)
}

all_long <- function(df, df2, int_list, int_df){
  for (r in 1:7){
    for (i in 1:4){
      tryCatch({
        pred = long_sim(i,int_list,int_df,r,df, df2)
        sd_df <- sd_pred_sim(pred[[1]],pred[[2]])
        grid_t <- grid_tp_sim(df2,r,int_list)
        known2 <- sim_estimate(int_df,i,r,grid_t, df)
        df_p = data.frame(sd_df, i, r, known2$gpid)
        pred_cv = rbind.data.frame(pred_cv, df_p)}, error=function(e){return(NA)})
    }
    all_pred <- rbind.data.frame(all_pred,pred_cv)
  }
  return(all_pred)
}


sd_pred_sim <- function(sim, pred_df){
  v <- NULL
  pred_df2 <- data.frame(pred_df)
  sum_squares <- rep(0,nrow(sim))
  for(j in 1:30){
    v <- sim[,j] #fill in Known with estimated
    sum_squares <- sum_squares + (pred_df - v)^2
  }
  pred_rmse <- sqrt(1/30*sum_squares)
  pred_df2$sd <- pred_rmse
  pred_df2$lc <- pred_df2$pred_df - (2*pred_rmse)
  pred_df2$uc <- pred_df2$pred_df + (2*pred_rmse)
  return(pred_df2)
}


```

```{r sim-function-noint, message=FALSE, warning = FALSE}

#Make A Polygon of The Data?

polyg_noint <- function(df, tp){
feat_t1 <- filter(df, t == tp)
feat_t1_og <- rownames_to_column(feat_t1, "index")
points <- st_as_sf(feat_t1, coords = c("xmap", "ymap"))
coord <- feat_t1_og[,c(6,7)]
ConvexHull = chull(coord)
coord_1 <- data.frame(coord[ConvexHull,],1)
my.sf.point <- st_as_sf(x = coord_1, 
                        coords = c("xmap", "ymap"))
polys = st_sf(
  aggregate(
    my.sf.point$geometry,
    list(my.sf.point$X1),
    function(g){
      st_cast(st_combine(g),"POLYGON")
    }
  ))
#Should be final data frame needed
op <- st_cast(polys, "MULTIPOLYGON")
poly <- op$geometry #(p)- polygon of the whole dataset
return(poly)
}

## Get Grid of The Polygon
grid_tp_sim_noint <- function(frame, tp, p){
  time_og <- filter(frame, t==tp)
  pts <- st_as_sf(time_og, coords = c("xmap", "ymap")) 
  grid_50 <- st_make_grid(p, cellsize = c(5, 5)) %>% 
    st_sf(grid_id = 1:length(.))
  grid_lab <- st_centroid(grid_50) %>% cbind(st_coordinates(.))
  pts_grd <- pts %>% st_join(grid_50, join = st_intersects) %>% as.data.frame
  all_pts_grd <- left_join(pts_grd,grid_lab,by= "grid_id")
  all_pts_grd2 <- all_pts_grd %>% distinct(gpid, .keep_all= TRUE)
  g1 <- all_pts_grd2[,c(1,6,8,7,9,10,11)]
  g1 <- rownames_to_column(g1, "index")
  return(g1)
}


#Get Known Data


sim_known_noint <- function(tp, grid,w){
  anim_t1 <- filter(w, t == tp)
  int_t1 <- right_join(grid, anim_t1, by = "gpid")
  final_int_t1 <- int_t1[,c(1,4,9,12,13,15)]
  t1_notimputed =  filter(final_int_t1, !is.na(xmap))
  colnames(t1_notimputed) <- c("gpid", "gpid_id","obs_time", "xmap","ymap", "clust")
  return(t1_notimputed)
}


#Get Estimate Data

sim_estimate_noint <- function(tp, grid, w){
  anim_t1 <- filter(w, t == tp)
  int_t1 <- right_join(grid, anim_t1, by = "gpid")
  t1_imputed =  filter(int_t1, is.na(xmap))
  final_int_t1 <- t1_imputed[,c(2,4,6,7,9,15)]
  colnames(final_int_t1) <- colnames(final_int_t1) <- c("gpid", "gpid_id",  "xmap","ymap", "obs_time", "clust")
  return(final_int_t1)
}


#Latitude Estimate (p is overall polygon, tp is time, frame = )
lat_est_sim_noint <- function(p, tp, frame, frame2){
  grid <- grid_tp_sim_noint(frame2,tp,p)
  known1 <- sim_known_noint(tp-1,grid,frame)
  known2 <- sim_known_noint(tp,grid, frame)
  known3 <- sim_known_noint(tp+1,grid, frame)
  known4 <- sim_known_noint(tp+2,grid, frame)
  if(nrow(known3) ==0){known <- rbind(known1, known2, known4)}
  else(known <- rbind(known1, known2, known3))
  loc <- known[,c("xmap","ymap", "obs_time")] ## locations that I will use to fit the model with time (xmap, ymap,obs_time)
  locs<-as.matrix(loc)                                ## convert them into matrix
  X <- as.matrix( rep(1,nrow(locs)))                  ## intercept
  N <- nrow(known)
  Y <- known[,4]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,N-1), silent = TRUE) #needs to go to number of rows - 1
  return(fit)
}


#Longitude Estimate

long_est_sim_noint <- function(p, tp, frame, frame2){
  grid <- grid_tp_sim_noint(frame2,tp,p)
  known1 <- sim_known_noint(tp-1, grid,frame)
  known2 <- sim_known_noint(tp, grid, frame)
  known3 <- sim_known_noint(tp+1,grid, frame)
  known4 <- sim_known_noint(tp+2,grid, frame)
  if(nrow(known3) ==0){known <- rbind(known1, known2, known4)}
  else(known <- rbind(known1, known2, known3))
  loc <- known[,c("xmap","ymap", "obs_time")] ## locations that I will use to fit the model with time (xmap, ymap,obs_time)
  locs<-as.matrix(loc)                                ## convert them into matrix
  X <- as.matrix( rep(1,nrow(locs)))                  ## intercept
  N <- nrow(known)
  Y <- known[,5]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,N-1), silent = TRUE) #needs to go to number of rows - 1
  return(fit)
}


#Latidude Prediction (estimated model, polygon, time, with_missing, without_missing(for grid))
lat_pred_sim_noint <- function(est, p, tp, frame, frame2){
  grid <- grid_tp_sim_noint(frame2,tp,p)
  known_grd <- sim_estimate_noint(tp,grid, frame)
  locs_pred <- as.matrix(known_grd[,c("xmap", "ymap","obs_time")]) # Would need to create obs_time when don't have them (round number?)
  colnames(locs_pred) <- c("xmap", "ymap", "obs_time")
  X_pred <- as.matrix( rep(1,nrow(locs_pred)))
  pred_grid<-predictions(est,locs_pred, X_pred,covparms = est$covparms, covfun_name = est$covfun_name, 
                         y_obs = est$y,locs_obs = est$locs
                         ,X_obs= est$X, beta=est$betahat, m = min(10,nrow(known_grd)-1), reorder = TRUE)
  return(pred_grid) 
}



# Longitude Prediction - Same Idea
long_pred_sim_noint <- function(est, p, tp, frame, frame2){
grid <- grid_tp_sim_noint(frame2,tp,p)
known_grd <- sim_estimate_noint(tp,grid, frame)
locs_pred <- as.matrix(known_grd[,c("xmap", "ymap","obs_time")]) # Would need to create obs_time when don't have them (round number?)
colnames(locs_pred) <- c("xmap", "ymap", "obs_time")
X_pred <- as.matrix( rep(1,nrow(locs_pred)))
pred_grid<-predictions(est,locs_pred, X_pred,covparms = est$covparms, covfun_name = est$covfun_name, 
                       y_obs = est$y,locs_obs = est$locs
                       ,X_obs= est$X, beta=est$betahat, min(10,nrow(known_grd)-1), reorder = TRUE)
return(pred_grid) 
}


#Estimate and Predict Together

both_sim_lat_noint <- function(p, tp, frame, frame2){
  est <- lat_est_sim_noint(p, tp, frame, frame2)
  pred<- lat_pred_sim_noint(est, p, tp, frame, frame2)
  return(pred)
}

both_sim_long_noint <- function(p, tp, frame, frame2){
  est <- long_est_sim_noint(p, tp, frame, frame2)
  pred<- long_pred_sim_noint(est, p, tp, frame, frame2)
  return(pred)
}

#Loop Through Each Time Point

pred_all <- NULL
all_sim_lat_noint <- function(df, df2){
  for (r in min(df$t):max(df$t)){
    tryCatch({
      p = polyg_noint(df2, r)
      pred = both_sim_lat_noint(p,r,df, df2)
      pred_df = data.frame(pred,r)
      grid_t <- grid_tp_sim_noint(df2,r,p)
      known2 <- sim_estimate_noint(r,grid_t, df)
      pred_df = data.frame(x = pred, t=r, gpid = known2$gpid)
      pred_all = rbind(pred_all, pred_df)}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  }
  return(pred_all)
}

all_sim_long_noint <- function(df, df2){
  for (r in min(df$t):max(df$t)){
    tryCatch({
      p = polyg_noint(df2, r)
      pred = both_sim_long_noint(p,r,df, df2)
      pred_df = data.frame(pred,r)
      grid_t <- grid_tp_sim_noint(df2,r, p)
      known2 <- sim_estimate_noint(r,grid_t, df)
      pred_df = data.frame(y = pred,t = r, gpid = known2$gpid)
      pred_all = rbind(pred_all, pred_df)}, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
  }
  return(pred_all)
}


```


```{r load-data}

datn = read.delim("data/lagrange_n.dat",sep="")
gpidn = unique(datn[,1]) #3364
colnames(datn) <- c("gpid","k","obs_time","xmap","ymap")
datn["location"] <- rep("n", 51926)
dato = read.delim("data/lagrange_o.dat",sep="")
gpido = unique(dato[,1]) #1941
colnames(dato) <- c("gpid","k","obs_time","xmap","ymap")
dato["location"] <- rep("o", 26106)
datp = read.delim("data/lagrange_p.dat",sep="")
gpidp = unique(datp[,1]) #2034
colnames(datp) <- c("gpid","k","obs_time","xmap","ymap")
datp["location"] <- rep("p", 26382)
datq = read.delim("data/lagrange_q.dat",sep="")
gpidq = unique(datq[,1]) #1472
colnames(datq) <- c("gpid","k","obs_time","xmap","ymap")
datq["location"] <- rep("q", 21033)

dat <- rbind(datn, dato, datp, datq)
gpid=unique(dat[,1]) #8811
n = length(gpid)
dat$t = floor(dat$obs_time) 
t = sort(unique(dat[,"t"]))
dat_time = split(dat,dat[,"t"]) 
tmp <- map2_df(dat_time, 1:length(dat_time), function(x, y) mutate(x, i = y))

```



# Introduction

Sea ice is frozen sea water in the Arctic Ocean that generally occurs as an ice pack which can drift over the ocean surface. When it is stationary, it acts as an insulator between the warm ocean and the cooler atmosphere [@peterson_evaluating_2011]. However, sea ice is constantly changing and moving due to dynamic thermodynamic and dynamic processes [@hutter_leads_2019]. These processes, which include wind and ocean currents, can cause cracks in the ice to form, as in order to move, the ice is continuously breaking up and refreezing [@peterson_evaluating_2011, @hoffman_detection_2019]. When a crack forms in the sea ice it provides a significant source of heat and moisture into the atmosphere, which reduces the overall sea ice concentration and warms the regional atmospheric boundary layer [@key_detectability_1993, @reiser_new_2020]. Additionally, the formation of cracks also has an affect on atmosphere and ocean chemical exchanges, which includes carbon dioxide and mercury [@hoffman_detection_2019]. Thus, the state of the sea ice, including crack characteristics, provides important information for weather prediction, climate models, and ocean models [@reiser_new_2020]. When cracks form, this may be an indicator of climate change, and may also be a driver of climate change due to the release of warmer air [@peterson_evaluating_2011].

## Previous Crack Detection Methods

Previous methods to determine the location of ice cracks generally either involve the use of thermal infrared satellite data or the calculation of ice deformation using movement. Thermal methods tend to use thermal channels from the Advanced Very High Resolution Radiometer (AVHRR), as the surface temperatures between a crack and the surrounding sea ice are different [@key_detectability_1993]. Using thermal channels from the AVHRR is highly dependent on clear skies as clouds can also give a higher temperature and may be mistaken for a crack. Methods have been suggested to reduce the impact of clouds. For example, @willmes_pan-arctic_2015 uses Moderate Resolution Imagery Spectroradtiometer (MODIS), which is only supposed to select only clear sky pixels in an image. A fuzzy cloud artifact filter was also introduced to remove any remaining clouds, however there is a trade-off between parameterization and error rates. Also, @rohrs_algorithm_2012 suggested using passive microwave data as clouds are transparent. However, microwave data lacks wide spatial coverage and has coarse resolution making detecting narrow leads impossible [@hoffman_detection_2019].

Deformation methods to locate ice cracks is determined by the motion of points. Points are initially assigned to a vertex of a grid cell on an ice sheet, which is tracked over time using satellite images. Then the deformation of the ice is calculated within each grid cell, where areas of high deformation may be denoted as cracks [@peterson_evaluating_2011]. These methods can provide additional information about a crack, such as size and deformation. However, satellite images typically only track each cell vertex in three day intervals, thus do not have a complete set of space-time observations to calculate deformation. Additionally, the error in the deformation calculations may be strongly underestimated as the grid cells move, they may become distorted which affects deformation calculations [@bouillon_producing_2015].

Our proposed method for ice sheet crack detection also wants to use the movement of the ice sheet. However, the movement of the ice sheet is not consistent, as the underlying process will cause the ice sheet to move differently depending on the location resulting in cracks [@peterson_evaluating_2011]. Thus, we aim to determine crack location by finding clusters of similar movement, where the boundaries between the clusters being the location of cracks. Additinoally, using the information gained from clustering, we developed a non-stationary spatio-temporal interpolation method to estimate missing points along a trajectory. Within a cluster, all trajectories move in a similar fashion, so would expect a missing point to move similarily to observed points in the cluster at that time. Hence, we can then crate a completely gridded dataset that can be used to calculate deformation. 

## Data

Data on the movement of the ice sheet comes from NASA's RADARSTAT Geophysical Processing System (RGPS). RGPS data is useful as it is independent of cloud coverage and has broad spatial coverage. Trajectories of around 30,000 points are tracked by RGPS using sequential synthetic aperture radar (SAR) images on the Arctic sea ice [@lindsay_radarstat_2003]. To track these points, an initial grid is created at the start of the study period where each cell dimension is 10 km on a side [@kwok_seasonal_2002]. The vertex of each cell is assigned an identifier $g$. In each SAR image, the points is found again using area-based and feature-based tracking [@peterson_evaluating_2011]. The trajectories of each identifier $g$ form a set of all trajectories $\mathcal{G} =\left\{g_1,...,g_n\right\}$, where $g_j = \left\{s_{jt}: t \in \mathcal{T}_j\right\}$. The location of $g_j$ is represented as $s_{jt} = (x_{jt}, y_{jt})$ or a set of latitudes and longitudes. Most $g_j$ are tracked in three day intervals, however sampling can be irregular [@peterson_evaluating_2011]. Thus, $\mathcal{T}_j \subset \left\{1,...,T\right\}$, which  is a collection of time points where identifier $g_j$ is observed. We are working with a smaller dataset than the overall, hence n = 8811 and $T$=22. This is due to keeping the creation of our methods computationally feasible.  



```{r trajectories, fig.cap = "Plot of gpid trajectories over time to show movement and direction of movement of each gpid", cache=TRUE, message=FALSE, warning = FALSE, fig.height = 5, fig.width = 9}
colnames(tmp) <- c("gpid", "k", "obs_time", "x", "y", "location", "t", "i") #Why did I do this?

tmp2 <- tmp %>% highlight_key(~gpid)

ggplot(tmp2, aes(x = x, y = y, group = gpid, 
                      hoverinfo = NULL,
                      color = factor(gpid %% 10))) + 
  geom_path(arrow = arrow(length = unit(0.7, "mm")), alpha = .4) + 
  scale_color_viridis_d() +
  theme(legend.position="none")

colnames(tmp) <- c("gpid", "k", "obs_time", "xmap", "ymap", "location", "t", "i")
```


## Nonstationary Spatio-Temporal Methods

###  Method Motivation

Since we want to detect cracks only using movement data, it is first important to visualize the movement. Thus, a plot of the trajectory of each $g_j$ is found in \cref{fig:trajectories}. Each line is a trajectory, $g_j$, where only its known locations are plotted and connected. An arrow is also given in order to indicate the direction of the movement. The coloring of each $g_j$ does not have a special meaning, they are just used to distinguish individual trajectories. This figure shows that groups of trajectories appear to move with similar patterns. For instance, a the top of \cref{fig:trajectories}, a group of trajectories are traveling upwards, while in the middle a group is traveling from right to left. Visualizing the movement led to the idea of clustering similar trajectories. Each trajectory can be summarized by a bounding box that represents it's movement over time. This allows for the creation of features that can be used in a clustering algorithm, and it circumnavigates the missing data issue. After the bounding box features are created, the trajectories can be grouped with other that have similar features. The idea is that ice cracks may form on the boundary between groups, as these groups are moving differently.  


# Methods {#methods}

## Spatio-Temporal Clustering: Bounding Box

### Bounding Box Features

The bounding box that is created around each trajectory is made up of a number of features that represent it's movement. Features of the bounding box includes the length traveled between the minimum and maximum location of a coordinate, representing the total distance traveled. However, the maximum and minimum locations may not always correspond to the first and last day of the time frame. Hence, the difference in each coordinate from the latest observation to the earliest observation in the time frame was also found. The change from latest of earliest observation is then used to calculate the angle of change to find the direction of movement of the trajectory. Further, since the focus is on the cluster boundaries, we must ensure the clusters are contiguous, meaning clusters are not co-located across multiple geographic locations. To ensure some geographic continuity, we also include the average coordinate values for each trajectory. 

A bounding box can be created around a whole trajectory, or for smaller trajectories. Sub-trajectories may provide more information while ensuring adequate data is available for the clustering process. A balance between ensuring data completeness and capturing sufficient movement is essential. If creating a bounding box for a sub-trajectory, for instance by week, previous week features may be included as inputs into the clustering algorithm. This may be done to ensure some consistency between consecutive time frames, as previous movement may imapct current movement. After all of the different features are calculated, the values are then standardized to give each feature similar weight in the clustering process.   

### K-Means

To cluster the trajectories we used k-means clustering, which partitions $n$ observations into $k$ clusters, with $k$ being a pre-specified number. The goal of this method is to minimize the squared Euclidean distance between an observation and the centroid vector of a cluster [@steinley_kmeans_2006]. This method requires each observation to be in a cluster, which is beneficial as we want to know what group each trajectory is more similar to. However, a drawback of k-means is that the number of clusters must be known and specified prior to clustering. We determine the number of clusters using the silhouette statistics, which compares within cluster distances to between cluster distances [@kodinariya_2013]. Ice movement is a dynamic process, so if clustering sub-trajectories, like weeks, then we would expect that each week may have a different number of clusters. Thus, when clustering by week, the silhouette statistic will be calculated separately for each week.  


## Spatio-Temporal Interpolation

### Finding Spatio-Temporal Neighbors

```{r int-plot, fig.cap= "(a). Polygons of week 1 clusters (b). Polygons of week 2 clusters (c). Intersection polygons of the overlap of week 1 and week 2 polygons", message=FALSE, warning = FALSE, out.width="70%"}

knitr::include_graphics("images/int-one.png")

```


Due to the data collection method, the data is susceptible to be missing in chunks due to the path of the satellite. Hence, we want to be able to interpolate these missing points to have completely gridded data. This can be challenging due to the lack of close neighbors, as all the data round a point may also be missing. Additionally, some interpolation methods are not available due to the non-stationarity of the data, as the ice is moving in patches. 

Our proposed method for interpolation missing locations involves finding and using spatio-temporal neighbors of the missing identifier. The clusters determined by the bounding box features can also be used to identify these spatio-temporal neighbors. Creation of the clusters is determined by similar movements, so we can use the information given to us by these clusters to interpolate. Meaning, if we know how points move in a cluster at a specific time, we can assume a missing identifier in the same cluster would move similarly. In this method, new clusters are created for each week. The idea is that the intersection of one week's clusters with the week before and week after would create groups. Each member of a group is then a spatio-temporal neighbor of the other members as they are in a similar geographic location over time.   
  
The first step in finding these intersections is to create polygons for each cluster. A polygon is created by finding the boundary coordinates of each of the clusters. In a sequential manner, a polygon is created around each cluster, generally starting with one on the top edge of the ice sheet followed by neighboring clusters until there is a polygon for each cluster. After each polygon is created, then all of the $g_j$ located within this polygon are removed from the dataset, even if the $g_j$ was assigned to a different cluster. This was done to reduce the amount of polygon overlapping. Additionally, if a $g_j$ is classified to a different cluster than all of it's neighbors, most likely that $g_j$ should actually be classified like it's neighbors. 
  
Once we create the polygons for each week, we can find the intersection of polygons for different weeks to define spatio-temporal neighbors. The coordinates of the overlapping polygon create an intersection, where the coordinates also form a polygon. Each $g_j$ is then assigned to an intersection based on which intersection coordinates contains its first observed location of the week. All of the points within that intersection are considered to be spatio-temporal neighbors since they are located in a similar geographic region over time. For example, if we want to interpolate missing data in week 1, would first find its spatio-temporal neighbors. Since it is the first week, there is no previous week information, so we can only use week 2 to find neighbors. Next, we identify the coordinates for the intersecting polygons for these two weeks. Then, the $g_j$ located in each intersecting polygon are spatio-temporal neighbors and will be used to create a model for interpolation in each of the intersecting polygons. \cref{fig:int-plot} shows this process with (a) and (b) showing the polygons of the clusters for week 1 and week 2 respectively, and (c) showing the intersections of the polygons. Some of the intersection polygons are now shown due to duplicate intersections (caused by overlapping polygons), but each $g_j$ is only assigned to one intersection. If a $g_j$ is not found in an intersection, it is then removed from the data during this process, which is a potential area for improvement. Spatio-temporal neighbors for week 3 are found using a similar process, with just week 2, as this was the last week in the dataset. Creating the intersecting polygons for week 2 involved the intersection of it's polygons from both week 1 and week 3.  
  
To use this interpolation method, a spatial grid encompassing the ice sheet is created for each day. The grid is used in our model as an estimation of the initial locations of missing $g_j$, where the model will adjust this location using its known neighbors. This size of our grid cells was chosen for a maximum of four $g_j$ to be located in a cell, matching with the initial grid used to track the trajectories. The centroid of the cell was used to estimate each $g_j$, so each of the $g_j$ located in that cell would have the same initial estimate. We do not want to use these as the final estimate of our missing locations as it does not take into account movement. 


### Univariate Interpolation
  
Once the grid was created for initial location estimates of the missing data, a univariate model was developed for both coordinates. These models were created using the \texttt{GpGp} package in R, which uses the Vecchia's Approximation for a Gaussian Process [@gpgp_pkg]. Generally, likelihood methods for the covariance parameter estimates of a Gaussian Process are computationally unfeasible. Hence, @vecchia1988estimation developed an approximation where the joint density of the Gaussian Process is written as a product of conditional distributions that use only a subset of the data. The subset chosen greatly affects the approximation and is formed by neighbors of the observation. Within the \texttt{GpGp} package, updates to this approximation are also made. A new ordering method to find neighbors was introduced, which sorted the data by sequentially picking the next point that has the maximum minimum distance to all previously selected points [@guinness_permutation_2018]. Additionally, @guinness_permutation_2018 introduces a grouping method, where data is partitioned into blocks and each block's input to the likelihood can be computed at the same time. Further @guinness_gaussian_2019 provides an efficient method for applying Fisher scoring to maximize the log-likelihood. These updates were shown to further increase the accuracy of the models and lowers computation time.      

To fit the model, we need to define:

 + $Y$ = $x$ or $y$ (univariate response vector of the coordinates. 
 + loc = The matrix of $x$, $y$, and time ($t$) of known data. Made of locations at the desired time ($t$), the day before ($t-1$), and the day after ($t+1$). 
 + $X$ = design matrix (single column of ones)
 + Sequence of values for number of neighbors. 

We specified the covariance function as the exponential space-time covariance function. It is defined as $$\Sigma_{\theta}(s;t) = \Sigma^{(s)}_{\theta}*\Sigma^{(t)}_{\theta} \quad (1)$$ where $\Sigma^{(s)}_{\theta} = \sigma^2\exp\left\{||x-y||/\phi\right\}$ and $\Sigma^{(t)}_{\theta} = \sigma^2\exp\left\{-|t|/\tau\right\}$. The parameters in the covariance function are $\sigma^2$ (variance), $\phi$ (spatial range), and $tau$ (temporal range). The spatial and temporal ranges are smoothness parameters that relate to dependence over space and time respectively. The output is the maximum Vecchia likelihood estimates for the covariance parameters. The model created by these estimates can be used to predict the unobserved locations at the time ($t$). The initial grid estimates of the coordinates are used as the starting locations in the prediction function and shifted based on the estimated model parameters. Predictions for the coordinate are made by finding the conditional expectation of the model developed.  


# Simulation Study {#simulation}

## Data Simulation

To test the validity of our methods, we conducted a simulation study. The data was simulated to mimic the motion of sea ice, where the movement happens in patches that are driven by external factors. Separate grids were created to simulate the observed data and the underlying process that is causing the movement. First, to create the underlying process, a find grid is created with each cell vertex representing a point. This grid is a 30x30 equally spaced grid, which is a total of 900 points. Next, initial cluster memberships are assigned to create patches. For simplicity, the points are assigned into two clusters, each with an equal number of points. Then, this grid is shifted seven times, to represent seven days, simulating movement in the underlying process. The movement at each time step of the grid decreases over time. This data is then used in to exponential space-time covariance function (defined in (1)), along with the defined parameter values to simulate a covariance matrix. The covariance function has different parameter values for each cluster. Additionally, the parameter values used for simulation may also differ slightly for $x$ and $y$ within a cluster. The covariance functions and defined mean trend are then used to simulate a Gaussian Process model of the displacement for each location on the grid at a time. Hence, $$ U_{d,c}(s,t) \sim GP(\mu_{d,c}, C_{d,c}(\theta))$$ where $c$ is cluster ($c$=1 or 2), $d$ is the coordinate ($d$ = $X$ or $Y$), and the parameter values for the parameter values for the mean ($\mu_{d,c}$) and covariance function ($C_{d,c}(\theta)$) can be found in \cref{tab:parms-table}. Thus, $U_{d,c}(s,t)$ gives the displacement, or movement, for each point on the underlying grid for each day.  

After the underlying process is created, a coarse grid representing the observed data is created in a similar fashion to the ice data given by satellite. This is a smaller grid that is encompassed by the underlying process. Furthermore, since it is coarser, it is made up of fewer points. Each point is represented by $(x_{t,j}, y_{t,j})$, where $t$ is the time, and $j$ is the identifier used to track the movement. The initial observed grid values would then be represented as $(x_{t=0,j}, y_{t=0,j})$. Movement of the observed points is determined by the value of the nearest point of the underlying process for that day, determined by Euclidean distance, to the observed point. Hence, $$(x_{t,j}, y_{t,j}) = (U^{X}_{t-1,c,g}, U^{Y}_{t-1,c,g}) + (x_{t-1,j}, y_{t-1,j}),$$ where $U^d_{t,c,g}$ is the underlying process for coordinate $d$ ($d$=$X$,$X$), cluster $c$ ($c$=1,2) at time $t-1$ ($t$=1,...,7), for grid value $g$, which is the closest grid location of the underling process to $(x_{t-1,j}, y_{t-1,j})$. This process is continued until $t=7$ in order to obtain a final simulated dataset of a week's worth of data. \cref{fig:grids-combined} shows the initial locations for the observed data plotted on top of the underlying process. The true cluster memberships for each location are also given.      

In order to evaluate our ice crack detection and interpolation method, three different datasets were simulated in this manner with different parameter values, which can be found in \cref{tab:parms-table}. A plot of the trajectories for each simulation can be found in \cref{fig:traj-wrap}. In Simulation 1 and Simulation 3, the trajectories generally start mostly linear, with curvature towards the end of the week. In Simulation 2, more curvature happens earlier in the week, with more gradual curves as the week progresses.
 

```{r initial-grid, message=FALSE, warning = FALSE}

xmap <- seq(1,30,by = 1)
ymap <- seq(1,30,by = 1)


u_grid <- as.matrix(expand.grid(xmap,ymap)) # create grid

set.seed(4)
clust <- kmeans(u_grid,2) # clusters are straight lines
cluster <- clust$cluster

cg <- data.frame(u_grid,cluster)
cg = data.frame(xmap=cg$Var1,ymap=cg$Var2,t=0) # get data frame of inital grid with cluster

cg$cluster <- 1
cg$cluster[cg$ymap > 15.5] <- 2
```


```{r og-grid, message=FALSE, warning = FALSE}
## Observed Grid

xmap2 <- seq(5,25,by = 2) #half the number of cells as underlying
ymap2 <- seq(5,25,by = 2)

og_grid <- as.matrix(expand.grid(xmap2,ymap2)) # create grid

og <- data.frame(gpid = 1:nrow(og_grid), xmap = og_grid[,1], ymap = og_grid[,2], t = 0)

```


```{r grids-combined, message=FALSE, warning = FALSE, fig.scap = "Simulation Grids", fig.cap = "Underlying Process Grid and Observed Grid together, where the true cluster is also given.", cache=TRUE, fig.height = 5, fig.width = 9, fig.align = 'center'}

og_combine <- og[,-1]
og_combine$cluster <- 1
og_combine$cluster[og_combine$ymap > 15.5] <- 2
og_combine$grid <- "Observed"

cg$grid <- "Underlying"

both_grid <- rbind(og_combine, cg)
both_grid$cluster <- as.factor(both_grid$cluster)


both_grid %>%
  ggplot(aes(x = xmap, y = ymap, frame = t, color = cluster, shape = grid)) + 
  geom_point() + 
  scale_shape_manual("Grid", values = c("Observed" = 2, "Underlying" = 20)) + 
  #scale_color_viridis_d() + 
  scale_fill_manual(values = alpha(c("blue", "orange"), .5))

```


```{r ex-1, message=FALSE, warning = FALSE, cache=TRUE}
#Creating Underlying Process Grid -> U
#seems to be doing ok - make grid bigger
xmap <- seq(1,30,by = 1)
ymap <- seq(1,30,by = 1)

u_grid <- as.matrix(expand.grid(xmap,ymap)) # create grid

set.seed(4)
clust <- kmeans(u_grid,2) # clusters are straight lines
cluster <- clust$cluster

cg <- data.frame(u_grid,cluster)
cg = data.frame(xmap=cg$Var1,ymap=cg$Var2,t=0) # get data frame of inital grid with cluster

cg$cluster <- 1
cg$cluster[cg$ymap > 15.5] <- 2

#Move Grid

all_q1 <- week1(14, 14, 0.75, 0.8, cg)

covparms_c2x <- c(40,60,10,0)
covparms_c2y <- c(80,50,10,0) # y

covparms_c1x <- c(5,5,5,0)
covparms_c1y <- c(5,5,5,0)

d_c1 <- filter(all_q1, cluster == 1)
d_c2 <- filter(all_q1, cluster == 2)

set.seed(4)
sigma_clus1x <- exponential_spacetime(covparms_c1x,as.matrix(d_c1[,1:3]))
set.seed(4)
sigma_clus2x <- exponential_spacetime(covparms_c2x,as.matrix(d_c2[,1:3]))

set.seed(44)
sigma_clus1y <- exponential_spacetime(covparms_c1y,as.matrix(d_c1[,1:3]))
set.seed(44)
sigma_clus2y <- exponential_spacetime(covparms_c2y,as.matrix(d_c2[,1:3]))

#Playing with mean and covariance function -> see if can separate. 

set.seed(44)
foo1 = rmvnorm(1, rep(2,ncol(sigma_clus1x)),sigma_clus1x) #u_x,c1
set.seed(44)
foo2 = rmvnorm(1, rep(10,ncol(sigma_clus2x)),sigma_clus2x) #u_x,c2
set.seed(44)
foo3 = rmvnorm(1, rep(1.5,ncol(sigma_clus1y)),sigma_clus1y) #u_y,c1
set.seed(44)
foo4 = rmvnorm(1, rep(6,ncol(sigma_clus2y)),sigma_clus2y) #u_y,c2


d_c1$rex <- c(foo1)
d_c2$rex <- c(foo2)

d_c1$rey <- c(foo3)
d_c2$rey <- c(foo4)


d <- rbind(d_c1, d_c2)%>% arrange(t) #Final data frame for underlying distribution

xmap2 <- seq(5,25,by = 2) #half the number of cells as underlying
ymap2 <- seq(5,25,by = 2)

og_grid <- as.matrix(expand.grid(xmap2,ymap2)) # create grid

og <- data.frame(gpid = 1:nrow(og_grid), xmap = og_grid[,1], ymap = og_grid[,2], t = 0)

#Get Data - match observed value to nearest neighbor in underlying distribution

#From t = 0 to t = 1
d_t1 <- filter(d, t==0)
d_new1<- rownames_to_column(d_t1, "index")

g <- knn(d_t1[,1:2], og[,2:3], cl = d_t1$cluster, k=1)
indices <- attr(g, "nn.index")

og_new <- data.frame(og,indices)
og_new$ux <- vlookup(og_new$indices,d_new1,6,1)
og_new$uy <- vlookup(og_new$indices,d_new1,7,1)
og_new$xnew <- og_new$xmap+ og_new$ux
og_new$ynew <- og_new$ymap+ og_new$uy
og_new$t <- og_new$t+1

t_1 <- og_new[c(1,4,6,7,8,9)]
t_2 <- nn_df(d, 1, t_1)
t_3 <- nn_df(d, 2, t_2)
t_4 <- nn_df(d, 3, t_3)
t_5 <- nn_df(d, 4, t_4)
t_6 <- nn_df(d, 5, t_5)
t_7 <- nn_df(d, 6, t_6)
t_8 <- nn_df(d, 7, t_7)
t_9 <- nn_df(d, 8, t_8)
t_10 <- nn_df(d, 9, t_9)
t_11 <- nn_df(d, 10, t_10)
t_12 <- nn_df(d, 11, t_11)
t_13 <- nn_df(d, 12, t_12)
t_14 <- nn_df(d, 13, t_13)

final_t1 <- rbind(t_1, t_2, t_3, t_4, t_5, t_6, t_7,t_8, t_9, t_10, t_11, t_12, t_13, t_14) #all days - final simulated dataset



```

```{r plot-ex1, message=FALSE, warning = FALSE}

######## Plot Trajectories of gpids

#Probably need to plays with parameter values a bit more. What should this look like?

og_clust <- og
og_clust$clust <- 1
og_clust$clust[og_clust$ymap > 15.5] <- 2
og_clust$clust <- as.factor(og_clust$clust)

final_t1_2 <- full_join(og_clust, final_t1[,-2], by = "gpid")
final_t1_2$simulation <- 1

```

```{r ex-2 , cache=TRUE, message=FALSE, warning = FALSE}
#Creating Underlying Process Grid -> U
#seems to be doing ok - make grid bigger
xmap <- seq(1,30,by = 1)
ymap <- seq(1,30,by = 1)

u_grid <- as.matrix(expand.grid(xmap,ymap)) # create grid

set.seed(4)
clust <- kmeans(u_grid,2) # clusters are straight lines
cluster <- clust$cluster

cg <- data.frame(u_grid,cluster)
cg = data.frame(xmap=cg$Var1,ymap=cg$Var2,t=0) # get data frame of inital grid with cluster

cg$cluster <- 1
cg$cluster[cg$ymap > 15.5] <- 2

#Move Grid
all_q2 <- week1(10, 10, 0.75, 0.8, cg)


covparms_c2x <- c(20,20,2,0)
covparms_c2y <- c(25,20,3,0) # y

covparms_c1x <- c(1,10,5,0)
covparms_c1y <- c(2,10,7,0)


d_c1 <- filter(all_q2, cluster == 1)
d_c2 <- filter(all_q2, cluster == 2)

set.seed(4)
sigma_clus1x <- exponential_spacetime(covparms_c1x,as.matrix(d_c1[,1:3]))
set.seed(4)
sigma_clus2x <- exponential_spacetime(covparms_c2x,as.matrix(d_c2[,1:3]))

set.seed(44)
sigma_clus1y <- exponential_spacetime(covparms_c1y,as.matrix(d_c1[,1:3]))
set.seed(44)
sigma_clus2y <- exponential_spacetime(covparms_c2y,as.matrix(d_c2[,1:3]))

#Playing with mean and covariance function -> see if can separate. 
set.seed(44)
foo1 = rmvnorm(1, rep(0.5,ncol(sigma_clus1x)),sigma_clus1x) #u_x,c1
set.seed(44)
foo2 = rmvnorm(1, rep(2,ncol(sigma_clus2x)),sigma_clus2x) #u_x,c2
set.seed(44)
foo3 = rmvnorm(1, rep(2,ncol(sigma_clus1y)),sigma_clus1y) #u_y,c1
set.seed(44)
foo4 = rmvnorm(1, rep(4,ncol(sigma_clus2y)),sigma_clus2y) #u_y,c2



d_c1$rex <- c(foo1)
d_c2$rex <- c(foo2)

d_c1$rey <- c(foo3)
d_c2$rey <- c(foo4)


d <- rbind(d_c1, d_c2)%>% arrange(t) #Final data frame for underlying distribution


## Observed Grid

xmap2 <- seq(5,25,by = 2) #half the number of cells as underlying
ymap2 <- seq(5,25,by = 2)

og_grid <- as.matrix(expand.grid(xmap2,ymap2)) # create grid

og <- data.frame(gpid = 1:nrow(og_grid), xmap = og_grid[,1], ymap = og_grid[,2], t = 0)

og_clust <- og
og_clust$cluster <- 1
og_clust$cluster[og_clust$ymap > 15.5] <- 2

#Get Data - match observed value to nearest neighbor in underlying distribution

#From t = 0 to t = 1
d_t1 <- filter(d, t==0)
d_new1<- rownames_to_column(d_t1, "index")

g <- knn(d_t1[,1:2], og[,2:3], cl = d_t1$cluster, k=1)
indices <- attr(g, "nn.index")

og_new <- data.frame(og,indices)
og_new$ux <- vlookup(og_new$indices,d_new1,6,1)
og_new$uy <- vlookup(og_new$indices,d_new1,7,1)
og_new$xnew <- og_new$xmap+ og_new$ux
og_new$ynew <- og_new$ymap+ og_new$uy
og_new$t <- og_new$t+1

t_1 <- og_new[c(1,4,6,7,8,9)]
t_2 <- nn_df(d, 1, t_1)
t_3 <- nn_df(d, 2, t_2)
t_4 <- nn_df(d, 3, t_3)
t_5 <- nn_df(d, 4, t_4)
t_6 <- nn_df(d, 5, t_5)
t_7 <- nn_df(d, 6, t_6)
t_8 <- nn_df(d, 7, t_7)
t_9 <- nn_df(d, 8, t_8)
t_10 <- nn_df(d, 9, t_9)
t_11 <- nn_df(d, 10, t_10)
t_12 <- nn_df(d, 11, t_11)
t_13 <- nn_df(d, 12, t_12)
t_14 <- nn_df(d, 13, t_13)

final_t2 <- rbind(t_1, t_2, t_3, t_4, t_5, t_6, t_7,t_8, t_9, t_10, t_11, t_12, t_13, t_14) #all days - final simulated dataset

```


```{r plot-ex2, message=FALSE, warning = FALSE}
######## Plot Trajectories of gpids

#Probably need to plays with parameter values a bit more. What should this look like?

og_clust <- og
og_clust$clust <- 1
og_clust$clust[og_clust$ymap > 15.5] <- 2
og_clust$clust <- as.factor(og_clust$clust)

final_t2_2 <- full_join(og_clust, final_t2[,-2], by = "gpid")
final_t2_2$simulation <- 2

```


```{r ex-3, cache=TRUE, message=FALSE, warning = FALSE}
#Creating Underlying Process Grid -> U
#seems to be doing ok - make grid bigger
xmap <- seq(1,30,by = 1)
ymap <- seq(1,30,by = 1)

u_grid <- as.matrix(expand.grid(xmap,ymap)) # create grid

set.seed(4)
clust <- kmeans(u_grid,2) # clusters are straight lines
cluster <- clust$cluster

cg <- data.frame(u_grid,cluster)
cg = data.frame(xmap=cg$Var1,ymap=cg$Var2,t=0) # get data frame of inital grid with cluster

cg$cluster <- 1
cg$cluster[cg$ymap > 15.5] <- 2

#Move Grid
all_q3 <- week1(10, 10, 0.75, 0.8, cg)

covparms_c2x <- c(20,20,10,0)
covparms_c2y <- c(20,30,10,0) # y

covparms_c1x <- c(2,10,5,0)
covparms_c1y <- c(2,10,5,0)

d_c1 <- filter(all_q3, cluster == 1)
d_c2 <- filter(all_q3, cluster == 2)

set.seed(4)
sigma_clus1x <- exponential_spacetime(covparms_c1x,as.matrix(d_c1[,1:3]))
set.seed(4)
sigma_clus2x <- exponential_spacetime(covparms_c2x,as.matrix(d_c2[,1:3]))

set.seed(44)
sigma_clus1y <- exponential_spacetime(covparms_c1y,as.matrix(d_c1[,1:3]))
set.seed(44)
sigma_clus2y <- exponential_spacetime(covparms_c2y,as.matrix(d_c2[,1:3]))

#Playing with mean and covariance function -> see if can separate. 
set.seed(44)
foo1 = rmvnorm(1, rep(1.5,ncol(sigma_clus1x)),sigma_clus1x) #u_x,c1
set.seed(44)
foo2 = rmvnorm(1, rep(6,ncol(sigma_clus2x)),sigma_clus2x) #u_x,c2
set.seed(44)
foo3 = rmvnorm(1, rep(1,ncol(sigma_clus1y)),sigma_clus1y) #u_y,c1
set.seed(44)
foo4 = rmvnorm(1, rep(3,ncol(sigma_clus2y)),sigma_clus2y) #u_y,c2


d_c1$rex <- c(foo1)
d_c2$rex <- c(foo2)

d_c1$rey <- c(foo3)
d_c2$rey <- c(foo4)

d <- rbind(d_c1, d_c2)%>% arrange(t) #Final data frame for underlying distribution


## Observed Grid

xmap2 <- seq(5,25,by = 2) #half the number of cells as underlying
ymap2 <- seq(5,25,by = 2)

og_grid <- as.matrix(expand.grid(xmap2,ymap2)) # create grid

og <- data.frame(gpid = 1:nrow(og_grid), xmap = og_grid[,1], ymap = og_grid[,2], t = 0)


og_clust <- og
og_clust$cluster <- 1
og_clust$cluster[og_clust$ymap > 15.5] <- 2

#Get Data - match observed value to nearest neighbor in underlying distribution

#From t = 0 to t = 1
d_t1 <- filter(d, t==0)
d_new1<- rownames_to_column(d_t1, "index")

g <- knn(d_t1[,1:2], og[,2:3], cl = d_t1$cluster, k=1)
indices <- attr(g, "nn.index")

og_new <- data.frame(og,indices)
og_new$ux <- vlookup(og_new$indices,d_new1,6,1)
og_new$uy <- vlookup(og_new$indices,d_new1,7,1)
og_new$xnew <- og_new$xmap+ og_new$ux
og_new$ynew <- og_new$ymap+ og_new$uy
og_new$t <- og_new$t+1

t_1 <- og_new[c(1,4,6,7,8,9)]
t_2 <- nn_df(d, 1, t_1)
t_3 <- nn_df(d, 2, t_2)
t_4 <- nn_df(d, 3, t_3)
t_5 <- nn_df(d, 4, t_4)
t_6 <- nn_df(d, 5, t_5)
t_7 <- nn_df(d, 6, t_6)
t_8 <- nn_df(d, 7, t_7)
t_9 <- nn_df(d, 8, t_8)
t_10 <- nn_df(d, 9, t_9)
t_11 <- nn_df(d, 10, t_10)
t_12 <- nn_df(d, 11, t_11)
t_13 <- nn_df(d, 12, t_12)
t_14 <- nn_df(d, 13, t_13)

final_t3 <- rbind(t_1, t_2, t_3, t_4, t_5, t_6, t_7,t_8, t_9, t_10, t_11, t_12, t_13, t_14) #all days - final simulated dataset

```

```{r plot-ex3, message=FALSE, warning = FALSE}
######## Plot Trajectories of gpids

og_clust <- og
og_clust$clust <- 1
og_clust$clust[og_clust$ymap > 15.5] <- 2
og_clust$clust <- as.factor(og_clust$clust)

final_t3_2 <- full_join(og_clust, final_t3[,-2], by = "gpid")
final_t3_2$simulation <- 3

```

```{r traj-wrap, fig.cap="Trajectory Plots for each Simulated Data Set ", cache = TRUE, message=FALSE, warning = FALSE, fig.height=5, fig.width=9}

final_traj <- rbind(final_t1_2, final_t2_2, final_t3_2)

final_traj2 <- final_traj %>% highlight_key(~gpid)

ggplot(final_traj2, aes(x = xnew, y = ynew, group = gpid, hoverinfo = NULL,
                   color = clust)) + 
  geom_path(arrow = arrow(length = unit(1, "mm")), alpha = .5) + ggtitle("Simulation Trajectories") + scale_fill_manual(values = alpha(c("blue", "orange"), .5)) + 
  facet_wrap(vars(simulation))

```

```{r parms-table}

parms_data <- data.frame(Cluster = c(1,2,1,2,1,2), "Var1" = c(5,40,1,20,2,20), "Spatial1" = c(5,60,10,20,10,20), "Temporal1" = c(5,10,5,2,5,10), "Nugget1" = c(0,0,0,0,0,0), "Mean1" = c(2,10,0.5,2,1.5,6), "Var2" = c(5,80,2,25,2,20), "Spatial2" = c(5,50,10,20,10,30), "Temporal2" = c(5,10,7,3,5,10), "Nugget2" = c(0,0,0,0,0,0), "Mean2" = c(1.5,6,2,4,1,3))
# 
kableExtra::kable(parms_data, booktabs = TRUE, caption = "Parameters Y for Underlying Process for Each Cluster", col.names = c("Cluster", "$\\sigma^{2}_{x}$", "$\\phi_{x,s}$", "$\\tau_{x}$", "$Nugget_x$", "$\\mu_x$", "$\\sigma^{2}_{y}$", "$\\phi_{y,s}$", "$\\tau_{y}$", "$Nugget_y$", "$\\mu_y$"), escape = FALSE) %>% pack_rows(
  index = (c("Simulation 1" = 2, "Simulation 2" = 2, "Simulation 3" = 2))
) %>% 
  add_header_above(c(" " ,"X" = 5, "Y" = 5))

```


## Clustering Method

```{r cluster1, message=FALSE, warning = FALSE, cache=TRUE}
## Clustering ------------------------------------------------------------
#What is the true cluster group? What is was at t=0, or at t=7 (comparing at each time point?)

d_new <- data.frame(final_t1, k=1)
colnames(d_new) <- c("gpid", "t", "ux", "uy", "xmap", "ymap", "k")

d_new2 <- filter(d_new, t < 8)

sim_bb <- d_new2 %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)


data_scale = data.frame(scale(sim_bb[,3:13]))
names(data_scale) <- c("x_s1", "y_s1", "xmin_s1", "xmax_s1", "ymin_s1", "ymax_s1", "xbox_s1", "ybox_s1", "dx_s1", "dy_s1", "angle_s1")
sim_bb_feat <- data.frame(sim_bb, data_scale)

set.seed(4)
km= kmeans(select(sim_bb_feat, c(x_s1,y_s1:angle_s1)), 2, nstart=25)
clusters = as.factor(km$cluster)

feat1 = data.frame(sim_bb_feat, clusters)

feat_data1_1 <- d_new2 %>%
  left_join(select(feat1, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

```

```{r clusters2, message=FALSE, warning = FALSE, cache=TRUE}
## Clustering ------------------------------------------------------------

#What is the true cluster group? What is was at t=0, or at t=7 (comparing at each time point?)

d_new <- data.frame(final_t2, k=1)
colnames(d_new) <- c("gpid", "t", "ux", "uy", "xmap", "ymap", "k")

d_new2 <- filter(d_new, t < 8)

sim_bb <- d_new2 %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)

data_scale = data.frame(scale(sim_bb[,3:13]))
names(data_scale) <- c("x_s1", "y_s1", "xmin_s1", "xmax_s1", "ymin_s1", "ymax_s1", "xbox_s1", "ybox_s1", "dx_s1", "dy_s1", "angle_s1")
sim_bb_feat <- data.frame(sim_bb, data_scale)

set.seed(4)
km = kmeans(select(sim_bb_feat, c(x_s1,y_s1:angle_s1)), 2, nstart=25)
clusters = as.factor(km$cluster)

feat2 = data.frame(sim_bb_feat, clusters)

feat_data1_2 <- d_new2 %>%
  left_join(select(feat2, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

```

```{r clusters3, message=FALSE, warning = FALSE, cache=TRUE}
## Clustering ----------------------------------------------

d_new <- data.frame(final_t3, k=1)
colnames(d_new) <- c("gpid", "t", "ux", "uy", "xmap", "ymap", "k")

d_new2 <- filter(d_new, t < 8)

sim_bb <- d_new2 %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)

data_scale = data.frame(scale(sim_bb[,3:13]))
names(data_scale) <- c("x_s1", "y_s1", "xmin_s1", "xmax_s1", "ymin_s1", "ymax_s1", "xbox_s1", "ybox_s1", "dx_s1", "dy_s1", "angle_s1")
sim_bb_feat <- data.frame(sim_bb, data_scale)

set.seed(4)
km = kmeans(select(sim_bb_feat, c(x_s1,y_s1:angle_s1)), 2, nstart=25)
clusters = as.factor(km$cluster)

feat3 = data.frame(sim_bb_feat, clusters)

feat_data1_3 <- d_new2 %>%
  left_join(select(feat3, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

```

```{r clust-og, message=FALSE, warning = FALSE}

feat_data_1 <- og %>%
  left_join(select(feat1, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

feat_data_2 <- og %>%
  left_join(select(feat2, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

feat_data_3 <- og %>%
  left_join(select(feat3, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

```

```{r plot-og-clus,  fig.cap= "Clusterings of Observed Data on Original Grid", message=FALSE, warning = FALSE, fig.height = 5, fig.width = 9, fig.align = 'center'}

feat_data_1$sim <- 1
feat_data_2$sim <- 2
feat_data_3$sim <- 3

f3 <- rbind(feat_data_1, feat_data_2, feat_data_3)


f3 %>%
  ggplot(aes(x = xmap, y = ymap, group = gpid, ids = gpid,
             color = clust, fill = clust)) + 
  geom_point() +
  scale_color_viridis_d() + 
  scale_fill_viridis_d() + ggtitle("Simulation at t=0") +  geom_hline(yintercept=15.5, linetype="dashed", color = "red", size=0.5) +
  facet_wrap(vars(sim))

```




```{r all-clus, fig.cap= "Clusterings of Observed Data on Final Day of Data Set", message=FALSE, warning = FALSE, fig.height = 5, fig.width = 9, fig.align = 'center'}

feat_data1_1$sim <- 1
feat_data1_2$sim <- 2
feat_data1_3$sim <- 3

f2 <- rbind(feat_data1_1, feat_data1_2, feat_data1_3)


filter(f2, t ==7) %>%
  ggplot(aes(x = xmap, y = ymap, group = gpid, ids = gpid,
             color = clust, fill = clust)) + 
  geom_point() +
  scale_color_viridis_d() + 
  scale_fill_viridis_d() + ggtitle("Simulation Plot at t=7") +
  facet_wrap(vars(sim))


```


Now our proposed spatio-temporal clustering method, using a bounding box, is performed on each of the simulated datasets. Since the true number of clusters is two, two is used as the $k$ input in the k-means clustering algorithm. The results are shown at two different time points. First, the k-means determined clusters are plotted on the initial grid to visualize how well our method performs when we can see the true clusterings (see \cref{fig:plot-og-clus}). The cut-off for the initially assigned cluster groupings is given by the red-dashed line. In this figure, a majority of points seemed to be clustered correctly, however, there are a number of misclassified points in each simulation. In Simulation 1, most of the misclassifications happen along the border, but seem to increase from left to right. Most of cluster 2 is clustered correctly in Simulation 2, with the majority of the misclassification in cluster 1 happening along the edges and boundary. Similarly, in Simulation 3 most of the misclassifications are near the border, with a handful along the right edge of cluster 2. 

The clusters are also visualized on the last day of the week to see if the clusters determined by the bounding box can distinguish movement over time. In \cref{fig:all-clus}, there are distinguishable boundaries between clusters for each of the simulations. Simulation 3 has a little overlap along the boundary, but can still distinguish two continguous clusters. Thus, by clustering using the movement features of a trajectory, we are able to distinguish the different movement patterns of the data with a clearly defined boundary. Some of the misclassifications on the original grid may be due to the value of the underlying process that was added to get the new location. This value was determined by the closest grid cell of the underlying process by Euclidean distance. If the point eventually becomes closer to the other cluster, meaning that cluster's underlying process values are being added to cause the movement, it will eventually start to move like it. So, if it spends more time moving like cluster 1 than cluster 2, as an example, it will most likely be classified as cluster 1, even if that was not its initial cluster assignment.  


## Interpolation Method

<!----------------- Simulation 1 Interpolation----------------------->

```{r week2-clust-1, message=FALSE, warning = FALSE}

d_new <- data.frame(final_t1, k=1)
colnames(d_new) <- c("gpid", "t", "ux", "uy", "xmap", "ymap", "k")


sim_bb <- d_new %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)


data_scale = data.frame(scale(sim_bb[,3:13]))
names(data_scale) <- c("x_s1", "y_s1", "xmin_s1", "xmax_s1", "ymin_s1", "ymax_s1", "xbox_s1", "ybox_s1", "dx_s1", "dy_s1", "angle_s1")
sim_bb_feat <- data.frame(sim_bb, data_scale)

set.seed(4)
km= kmeans(select(sim_bb_feat, c(x_s1,y_s1:angle_s1)), 2, nstart=25)
clusters = as.factor(km$cluster)

feat1 = data.frame(sim_bb_feat, clusters)

feat_data1_1 <- d_new %>%
  left_join(select(feat1, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)




```


```{r sim1-week2}


f1 <- filter(feat_data1_1, t < 8)
f2 <- filter(feat_data1_1, t > 7)

```

```{r sim1-int, message=FALSE, warning = FALSE}

y1 <- polygon_sim(7, f1)
y2 <- polygon_sim(8, f2)



## Get Intersection

intersections_pp <- st_intersection(y1, y2) %>% mutate(int_name = paste0(Group.1, "-", Group.1.1))
feat_t1 <- filter(f1, t == 7)
#feat_t1 <- filter(feat_data1_1, t == 7)
points <-st_as_sf(feat_t1, coords = c("xmap", "ymap"))
polyg1 <-intersections_pp$geometry # LIST OF INTERSECTION POLYGONS
z <- st_intersects(polyg1,points) #Return index of points within each intersection
FrameData <- lapply(z, function(x) as.data.frame(x))
new<- melt(FrameData)[,-1] #gives list of index and intersection group
colnames(new) <- c("gpid", "intersection")
n1 <- merge(feat_t1, new, by = "gpid") #Data frame of all of the gpids with interactions


```

```{r missing-df-sim1, message=FALSE, warning = FALSE}
#w1 <- feat_data1_1
w1 <- f1
f_sim1 <- f1

p <- 0.1 #proportion of missing values
set.seed(5)
sel <- sample( nrow(w1), size = p*nrow(w1))
# Final Data Frame that has missing values (gpid, t, ux, uy, xmap, ymap, k, clust)
for(t in sel){
  w1[t,c(5,6)] <- NA #w1[t,c(3,4)] <- NA
}


```

```{r sim1-res, message=FALSE, warning = FALSE, cache=TRUE}

#Gives Location for X
int_1x = all_lat(w1, f1,polyg1,n1)

#Gives Location for Y
int_1y = all_long(w1,f1,polyg1,n1)

xy1 <- data.frame(int_1x$known2.gpid, int_1x$r, int_1x$i, int_1x$pred_df, int_1x$sd, int_1x$lc, int_1x$uc, int_1y$pred_df, int_1y$sd, int_1y$lc, int_1y$uc)
colnames(xy1) <- c("gpid", "t", "intersection", "xmap_pred", "x_sd", "x_lc", "x_uc", "ymap_pred", "y_sd", "y_lc", "y_uc")


xy_join1 <- right_join(f1, xy1, by = c("gpid", "t"))
xy_join1 <- xy_join1 %>% distinct(gpid,t,.keep_all= TRUE)


error_x1 <- sum((xy_join1$xmap-xy_join1$xmap_pred)^2)/nrow(xy_join1) #6.24
error_y1 <- sum((xy_join1$ymap-xy_join1$ymap_pred)^2)/nrow(xy_join1) #5.03


```

```{r lin-int1, message=FALSE, warning = FALSE}

w1$missing <- "no"
w1$missing[is.na(w1$xmap)] <- "yes"

#estimates everything except if missing first or last day of data set
lin1 <- w1 %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

miss1 <- filter(lin1, missing == "yes")
colnames(miss1) <- c("gpid", "t", "ux", "uy", "xmap_pred", "ymap_pred", "k", "clust","missing")
miss_join1 <- right_join(feat_data1_1, miss1, by = c("gpid", "t"))

miss_na1 <- filter(miss_join1, !is.na(xmap_pred))

error_x_lin1 <- sum((miss_na1$xmap-miss_na1$xmap_pred)^2)/nrow(miss_na1) #2.89
error_y_lin1 <- sum((miss_na1$ymap-miss_na1$ymap_pred)^2)/nrow(miss_na1) #3.515

```

<!----------------- Simulation 2 Interpolation----------------------->

```{r week2-clust-2, message=FALSE, warning = FALSE}

d_new <- data.frame(final_t2, k=1)
colnames(d_new) <- c("gpid", "t", "ux", "uy", "xmap", "ymap", "k")


sim_bb <- d_new %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)


data_scale = data.frame(scale(sim_bb[,3:13]))
names(data_scale) <- c("x_s1", "y_s1", "xmin_s1", "xmax_s1", "ymin_s1", "ymax_s1", "xbox_s1", "ybox_s1", "dx_s1", "dy_s1", "angle_s1")
sim_bb_feat <- data.frame(sim_bb, data_scale)

set.seed(4)
km= kmeans(select(sim_bb_feat, c(x_s1,y_s1:angle_s1)), 2, nstart=25)
clusters = as.factor(km$cluster)

feat1 = data.frame(sim_bb_feat, clusters)

feat_data1_2 <- d_new %>%
  left_join(select(feat1, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)



```


```{r sim2-week2, message=FALSE, warning = FALSE}
#Week 2

f1 <- filter(feat_data1_2, t < 8)
f2 <- filter(feat_data1_2, t > 7)

```

```{r sim2-int, message=FALSE, warning = FALSE}

y1 <- polygon_sim(7, f1)
y2 <- polygon_sim(8, f2)

## Get Intersection

intersections_pp <- st_intersection(y1, y2) %>% mutate(int_name = paste0(Group.1, "-", Group.1.1))
#feat_t1 <- filter(feat_data1_2, t == 7)
feat_t1 <- filter(f1, t == 7)
points <-st_as_sf(feat_t1, coords = c("xmap", "ymap"))
polyg2 <-intersections_pp$geometry # LIST OF INTERSECTION POLYGONS
z <- st_intersects(polyg2,points) #Return index of points within each intersection
FrameData <- lapply(z, function(x) as.data.frame(x))
new<- melt(FrameData)[,-1] #gives list of index and intersection group
colnames(new) <- c("gpid", "intersection")
n2 <- merge(feat_t1, new, by = "gpid")#Data frame of all of the gpids with interactions


```

```{r missing-df-sim2, message=FALSE, warning = FALSE}

w2 <- f1
f_sim2 <- f1

p <- 0.1 #proportion of missing values
set.seed(5)
sel <- sample(nrow(w2), size = p*nrow(w2))
# Final Data Frame that has missing values (gpid, t, ux, uy, xmap, ymap, k, clust)
for(t in sel){
  w2[t,c(5,6)] <- NA #w1[t,c(3,4)] <- NA
}


```

```{r sim2-res, message=FALSE, warning = FALSE, cache=TRUE}

#Gives Location for X
int_2x = all_lat(w2, f1,polyg2,n2)


#Gives Location for Y
int_2y = all_long(w2,f1,polyg2,n2)

xy2 <- data.frame(int_2x$known2.gpid, int_2x$r, int_2x$i, int_2x$pred_df, int_2x$sd, int_2x$lc, int_2x$uc, int_2y$pred_df, int_2y$sd, int_2y$lc, int_2y$uc)
colnames(xy2) <- c("gpid", "t", "intersection", "xmap_pred", "x_sd", "x_lc", "x_uc", "ymap_pred", "y_sd", "y_lc", "y_uc")


xy_join2 <- right_join(f1, xy2, by = c("gpid", "t"))
xy_join2 <- xy_join2 %>% distinct(gpid,t,.keep_all= TRUE)


error_x2 <- sum((xy_join2$xmap-xy_join2$xmap_pred)^2)/nrow(xy_join2) #6.24
error_y2 <- sum((xy_join2$ymap-xy_join2$ymap_pred)^2)/nrow(xy_join2) #5.03


```

```{r lin-int2, message=FALSE, warning = FALSE}


w2$missing <- "no"
w2$missing[is.na(w2$xmap)] <- "yes"

#estimates everything except if missing first or last day of data set
lin2 <- w2 %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

miss2 <- filter(lin2, missing == "yes")
colnames(miss2) <- c("gpid", "t", "ux", "uy", "xmap_pred", "ymap_pred", "k", "clust","missing")
miss_join2 <- right_join(feat_data1_2, miss2, by = c("gpid", "t"))

miss_na2 <- filter(miss_join2, !is.na(xmap_pred))

error_x_lin2 <- sum((miss_na2$xmap-miss_na2$xmap_pred)^2)/nrow(miss_na2) #2.89
error_y_lin2 <- sum((miss_na2$ymap-miss_na2$ymap_pred)^2)/nrow(miss_na2) #3.515



```

<!----------------- Simulation 3 Interpolation----------------------->

```{r week2-clust-3, message=FALSE, warning = FALSE}

d_new <- data.frame(final_t3, k=1)
colnames(d_new) <- c("gpid", "t", "ux", "uy", "xmap", "ymap", "k")


sim_bb <- d_new %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)



data_scale = data.frame(scale(sim_bb[,3:13]))
names(data_scale) <- c("x_s1", "y_s1", "xmin_s1", "xmax_s1", "ymin_s1", "ymax_s1", "xbox_s1", "ybox_s1", "dx_s1", "dy_s1", "angle_s1")
sim_bb_feat <- data.frame(sim_bb, data_scale)

set.seed(4)
km= kmeans(select(sim_bb_feat, c(x_s1,y_s1:angle_s1)), 2, nstart=25)
clusters = as.factor(km$cluster)

feat1 = data.frame(sim_bb_feat, clusters)

feat_data1_3 <- d_new %>%
  left_join(select(feat1, gpid, clust = clusters)) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)


```


```{r sim3-w2-clus, message=FALSE, warning = FALSE}

f1 <- filter(feat_data1_3, t < 8)
f2 <- filter(feat_data1_3, t > 7)

```


```{r sim3-int, message=FALSE, warning = FALSE}

y1 <- polygon_sim(7, f1)
y2 <- polygon_sim(8, f2)


## Get Intersection
intersections_pp <- st_intersection(y1, y2) %>% mutate(int_name = paste0(Group.1, "-", Group.1.1))
#feat_t1 <- filter(feat_data1_3, t == 7)
feat_t1 <- filter(f1, t == 7)
points <-st_as_sf(feat_t1, coords = c("xmap", "ymap"))
polyg3 <-intersections_pp$geometry # LIST OF INTERSECTION POLYGONS
z <- st_intersects(polyg3,points) #Return index of points within each intersection
FrameData <- lapply(z, function(x) as.data.frame(x))
new<- melt(FrameData)[,-1] #gives list of index and intersection group
colnames(new) <- c("gpid", "intersection")
n3 <- merge(feat_t1, new, by = "gpid") #Data frame of all of the gpids with interactions


```

```{r missing-df-sim3, message=FALSE, warning = FALSE}

w3 <- f1
f_sim3 <- f1


p <- 0.1 #proportion of missing values
set.seed(5)
sel <- sample( nrow(w3), size = p*nrow(w3))
# Final Data Frame that has missing values (gpid, t, ux, uy, xmap, ymap, k, clust)
for(t in sel){
  w3[t,c(5,6)] <- NA #w1[t,c(3,4)] <- NA
}


```


```{r sim3-res, message=FALSE, warning = FALSE, cache=TRUE}

#Gives Location for X
int_3x = all_lat(w3, f1,polyg3,n3)

#Gives Location for Y
int_3y = all_long(w3,f1,polyg3,n3)

xy3 <- data.frame(int_3x$known2.gpid, int_3x$r, int_3x$i, int_3x$pred_df, int_3x$sd, int_3x$lc, int_3x$uc, int_3y$pred_df, int_3y$sd, int_3y$lc, int_3y$uc)
colnames(xy3) <- c("gpid", "t", "intersection", "xmap_pred", "x_sd", "x_lc", "x_uc", "ymap_pred", "y_sd", "y_lc", "y_uc")

xy_join3 <- right_join(f1, xy3, by = c("gpid", "t"))
xy_join3 <- xy_join3 %>% distinct(gpid,t, .keep_all= TRUE)

error_x3 <- sum((xy_join3$xmap-xy_join3$xmap_pred)^2)/nrow(xy_join3) #6.24
error_y3 <- sum((xy_join3$ymap-xy_join3$ymap_pred)^2)/nrow(xy_join3) #5.03


```


```{r lin-int3, message=FALSE, warning = FALSE}

w3$missing <- "no"
w3$missing[is.na(w3$xmap)] <- "yes"


#estimates everything except if missing first or last day of data set
lin3 <- w3 %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))


miss3 <- filter(lin3, missing == "yes")
colnames(miss3) <- c("gpid", "t", "ux", "uy", "xmap_pred", "ymap_pred", "k", "clust","missing")
miss_join3 <- right_join(feat_data1_3, miss3, by = c("gpid", "t"))

miss_na3 <- filter(miss_join3, !is.na(xmap_pred))

error_x_lin3 <- sum((miss_na3$xmap-miss_na3$xmap_pred)^2)/nrow(miss_na3) #2.89
error_y_lin3 <- sum((miss_na3$ymap-miss_na3$ymap_pred)^2)/nrow(miss_na3) #3.515

```

<!-- No Intersection Models -->

```{r sim1-noint, message=FALSE, warning = FALSE, cache = TRUE}


x_sim1_noint <- all_sim_lat_noint(w1, f_sim1)
y_sim1_noint <- all_sim_long_noint(w1,f_sim1)

xy_noint1 <- data.frame(x_sim1_noint$gpid, x_sim1_noint$t, x_sim1_noint$x, y_sim1_noint$y)
colnames(xy_noint1) <- c("gpid", "t", "xmap_pred", "ymap_pred")
xy_join_noint1 <- right_join(f_sim1, xy_noint1, by = c("gpid", "t"))

noint_x_sim1 <- sum((xy_join_noint1$xmap-xy_join_noint1$xmap_pred)^2)/nrow(xy_join_noint1) #1.829172
noint_y_sim1 <- sum((xy_join_noint1$ymap-xy_join_noint1$ymap_pred)^2)/nrow(xy_join_noint1) #2.510814

```

```{r sim2-noint, message=FALSE, warning = FALSE, cache= TRUE}

x_sim2_noint <- all_sim_lat_noint(w2, f_sim2)
y_sim2_noint <- all_sim_long_noint(w2,f_sim2)

xy_noint2 <- data.frame(x_sim2_noint$gpid, x_sim2_noint$t, x_sim2_noint$x, y_sim2_noint$y)
colnames(xy_noint2) <- c("gpid", "t", "xmap_pred", "ymap_pred")
xy_join_noint2 <- right_join(f_sim2, xy_noint2, by = c("gpid", "t"))

noint_x_sim2 <- sum((xy_join_noint2$xmap-xy_join_noint2$xmap_pred)^2)/nrow(xy_join_noint2) #1.829172
noint_y_sim2 <- sum((xy_join_noint2$ymap-xy_join_noint2$ymap_pred)^2)/nrow(xy_join_noint2) #2.510814

```

```{r sim3-noint, message=FALSE, warning = FALSE, cache=TRUE}

x_sim3_noint <- all_sim_lat_noint(w3, f_sim3)
y_sim3_noint <- all_sim_long_noint(w3,f_sim3)

xy_noint3 <- data.frame(x_sim3_noint$gpid, x_sim3_noint$t, x_sim3_noint$x, y_sim3_noint$y)
colnames(xy_noint3) <- c("gpid", "t", "xmap_pred", "ymap_pred")
xy_join_noint3 <- right_join(f_sim3, xy_noint3, by = c("gpid", "t"))

noint_x_sim3 <- sum((xy_join_noint3$xmap-xy_join_noint3$xmap_pred)^2)/nrow(xy_join_noint3) #1.829172
noint_y_sim3 <- sum((xy_join_noint3$ymap-xy_join_noint3$ymap_pred)^2)/nrow(xy_join_noint3) #2.510814


```

<!-- Initial Results Table -->

```{r results-table, message=FALSE, warning = FALSE, cache = TRUE}

result_data <- data.frame(X1 = c(sqrt(error_x1), sqrt(error_x2), sqrt(error_x3)), Y1 = c(sqrt(error_y1), sqrt(error_y2), sqrt(error_y3)), X2 = c(sqrt(error_x_lin1), sqrt(error_x_lin2), sqrt(error_x_lin3)), Y2 = c(sqrt(error_y_lin1), sqrt(error_y_lin2), sqrt(error_y_lin3)), X3 = c(sqrt(noint_x_sim1), sqrt(noint_x_sim2), sqrt(noint_x_sim3)), Y3 = c(sqrt(noint_y_sim1), sqrt(noint_y_sim2), sqrt(noint_y_sim3)))
 
kableExtra::kable(result_data, booktabs = TRUE, caption = "RMSE for Interpolation Methods", col.names = c("$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{no int}$", "$Y_{no int}$"), escape = FALSE) %>% pack_rows(
  index = (c("Simulation 1" = 1, "Simulation 2" = 1, "Simulation 3" = 1))
) %>% 
  add_header_above(c("Intersection" = 2, "Linear" = 2, "No Intersection" = 2))


```


<!-- Coverage Probability -->

```{r cp-sim1, message=FALSE, warning = FALSE, cache = TRUE}
xy_join1$inintx <- 0
xy_join1$inintx[xy_join1$x_lc < xy_join1$xmap & xy_join1$xmap < xy_join1$x_uc] <- 1
cp_sim1_x <- sum(xy_join1$inintx)/nrow(xy_join1) #coverage probability
sd_sim1_x <- mean(xy_join1$x_sd)

xy_join1$ininty <- 0
xy_join1$ininty[xy_join1$y_lc < xy_join1$ymap & xy_join1$ymap < xy_join1$y_uc] <- 1
cp_sim1_y <- sum(xy_join1$ininty)/nrow(xy_join1) #coverage probability
sd_sim1_y <- mean(xy_join1$y_sd)


```


```{r cp-sim2, message=FALSE, warning = FALSE, cache = TRUE}

xy_join2$inintx <- 0
xy_join2$inintx[xy_join2$x_lc < xy_join2$xmap & xy_join2$xmap < xy_join2$x_uc] <- 1
cp_sim2_x <- sum(xy_join2$inintx)/nrow(xy_join2) #coverage probability
sd_sim2_x <- mean(xy_join2$x_sd)

xy_join2$ininty <- 0
xy_join2$ininty[xy_join2$y_lc < xy_join2$ymap & xy_join2$ymap < xy_join2$y_uc] <- 1
cp_sim2_y <- sum(xy_join2$ininty)/nrow(xy_join2) #coverage probability
sd_sim2_y <- mean(xy_join2$y_sd)

```


```{r cp-sim3, message=FALSE, warning = FALSE, cache = TRUE}

xy_join3$inintx <- 0
xy_join3$inintx[xy_join3$x_lc < xy_join3$xmap & xy_join3$xmap < xy_join3$x_uc] <- 1
cp_sim3_x <- sum(xy_join3$inintx)/nrow(xy_join3) #coverage probability
sd_sim3_x <- mean(xy_join3$x_sd)

xy_join3$ininty <- 0
xy_join3$ininty[xy_join3$y_lc < xy_join3$ymap & xy_join3$ymap < xy_join3$y_uc] <- 1
cp_sim3_y <- sum(xy_join3$ininty)/nrow(xy_join3) #coverage probability
sd_sim3_y <- mean(xy_join3$y_sd)


```


The above simulations are also used to check the performance of our spatio-temporal interpolation model. For each of the simulations, another week of data is similarly created and clustered. Polygons for the two clusters are created. Then, the intersection polygons of the weeks polygons are found. Once again, an intersections represent the spatio-temporal neighbors of the data within that intersection polygon. Next, 10% of the data for the first week are randomly assigned to be missing. Then a univariate model for x and y are developed in order to obtain our estimates of the missing locations. Additionally, in order to have a baseline to compare our method to, we also ran linear interpolation on the same data. Linear interpolation estimates an objects unknown location along a straight-line path between two known locations. It has been shown to work well for trajectories that follow a straight-line path and have a lot of sampled points. On the other hand, it tends to not work as well when a trajectory follows a more curved path or is not heavily sampled [@wentz_comparison_2003; @guo_improved_2021]. A third method was used for comparison that is similar to our intersection method. Here, instead of running the model inside an intersection, the intersections were ignored, and a model was developed using all known points for t-1, t, and t+1 (this essentially ignores the nonstationarity aspect of our data). The Root Mean Square Error (RMSE) for each of the simulations and interpolation methods can be found in \cref{tab:results-table}. 

Our interpolation method never seems to outperform linear interpolation. However, outside of Simulation 3, the results are not very different. In Simulation 3, running the model for each intersection performs better than the overall model. Further, since the clusters were created with different movements, in order to see if there are different areas where our method may perform better, \cref{tab:results-table-by-clust} breaks out the RMSE calculations by cluster. In this table, we see similar results. For Simulation 1, linear interpolation always performs the best. In Simulation 2, our method outperforms linear interpolation for Y in cluster 2. However, the model using all the data performs the best here. Simulation 3 is similar to Simulation 1. Nonetheless, linear interpolation performing the best may not be surprising, as there are long periods of linear movement in each of the simulations, so the results may be dependent on what points where randomly removed. Additionally, linear interpolation can not estimate the first or last point of a trajectory, so there are fewer predicted locations used to calculate the RMSE. Thus, these simulations show that linear interpolation may generally outperform our method, but our method may have some promise with curved data that may not be highly sampled. As an example, in $Y_{s2,c2}$, the data is more spread out and curved than the others. The other simulations may have curves, but samples are taken closer together, or the data may be spread out but mostly is linear. The results on if creating the intersections is necessary is mixed. However, these results may be impacted by some of the intersection not containing much data, which may impact model development. Through the simulations, we also found that having a fine enough grid for estimates of the missing data as starting values in the prediction function is key. If the grid is too coarse, this can impact the accuracy of the estimations.

<!-- By Cluster -->

```{r sim-by-clus1, message=FALSE, warning = FALSE}

#Intersection Method
x1_c1 <- filter(xy_join1, clust == 1)
x1_c2 <- filter(xy_join1, clust == 2)

sim1_x_c1 <- sum((x1_c1$xmap-x1_c1$xmap_pred)^2)/nrow(x1_c1)
sim1_y_c1 <- sum((x1_c1$ymap-x1_c1$ymap_pred)^2)/nrow(x1_c1)
sim1_x_c2 <- sum((x1_c2$xmap-x1_c2$xmap_pred)^2)/nrow(x1_c2)
sim1_y_c2 <- sum((x1_c2$ymap-x1_c2$ymap_pred)^2)/nrow(x1_c2)

#Linear Interpolation
miss_na_1 <- filter(miss_na1, clust.x == 1)
miss_na_2 <- filter(miss_na1, clust.x == 2)

lin1_x_c1 <-sum((miss_na_1$xmap-miss_na_1$xmap_pred)^2)/nrow(miss_na_1) 
lin1_y_c1 <- sum((miss_na_1$ymap-miss_na_1$ymap_pred)^2)/nrow(miss_na_1) 
lin1_x_c2 <- sum((miss_na_2$xmap-miss_na_2$xmap_pred)^2)/nrow(miss_na_2)
lin1_y_c2 <- sum((miss_na_2$ymap-miss_na_2$ymap_pred)^2)/nrow(miss_na_2) 


#Model without Intersection

x1_c1_noint <- filter(xy_join_noint1, clust == 1)
x1_c2_noint <- filter(xy_join_noint1, clust == 2)

sim1_x_c1_noi <- sum((x1_c1_noint$xmap-x1_c1_noint$xmap_pred)^2)/nrow(x1_c1_noint)
sim1_y_c1_noi <- sum((x1_c1_noint$ymap-x1_c1_noint$ymap_pred)^2)/nrow(x1_c1_noint)
sim1_x_c2_noi <- sum((x1_c2_noint$xmap-x1_c2_noint$xmap_pred)^2)/nrow(x1_c2_noint)
sim1_y_c2_noi <- sum((x1_c2_noint$ymap-x1_c2_noint$ymap_pred)^2)/nrow(x1_c2_noint)


```

```{r sim-by-clus2, message=FALSE, warning = FALSE}

#Intersection Method
x2_c1 <- filter(xy_join2, clust == 1)
x2_c2 <- filter(xy_join2, clust == 2)

sim2_x_c1 <- sum((x2_c1$xmap-x2_c1$xmap_pred)^2)/nrow(x2_c1)
sim2_y_c1 <- sum((x2_c1$ymap-x2_c1$ymap_pred)^2)/nrow(x2_c1)
sim2_x_c2 <- sum((x2_c2$xmap-x2_c2$xmap_pred)^2)/nrow(x2_c2)
sim2_y_c2 <- sum((x2_c2$ymap-x2_c2$ymap_pred)^2)/nrow(x2_c2)

#Linear Interpolation

miss_na2_1 <- filter(miss_na2, clust.x == 1)
miss_na2_2 <- filter(miss_na2, clust.x == 2)


lin2_x_c1 <- sum((miss_na2_1$xmap-miss_na2_1$xmap_pred)^2)/nrow(miss_na2_1) 
lin2_y_c1 <- sum((miss_na2_1$ymap-miss_na2_1$ymap_pred)^2)/nrow(miss_na2_1) 
lin2_x_c2 <- sum((miss_na2_2$xmap-miss_na2_2$xmap_pred)^2)/nrow(miss_na2_2)
lin2_y_c2 <- sum((miss_na2_2$ymap-miss_na2_2$ymap_pred)^2)/nrow(miss_na2_2) 

#Model without Intersection

x2_c1_noint <- filter(xy_join_noint2, clust == 1)
x2_c2_noint <- filter(xy_join_noint2, clust == 2)


sim2_x_c1_noi <- sum((x2_c1_noint$xmap-x2_c1_noint$xmap_pred)^2)/nrow(x2_c1_noint)
sim2_y_c1_noi <- sum((x2_c1_noint$ymap-x2_c1_noint$ymap_pred)^2)/nrow(x2_c1_noint)
sim2_x_c2_noi <- sum((x2_c2_noint$xmap-x2_c2_noint$xmap_pred)^2)/nrow(x2_c2_noint)
sim2_y_c2_noi <- sum((x2_c2_noint$ymap-x2_c2_noint$ymap_pred)^2)/nrow(x2_c2_noint)


```

```{r sim-by-clus3, message=FALSE, warning = FALSE}

#Intersection Method
x3_c1 <- filter(xy_join3, clust == 1)
x3_c2 <- filter(xy_join3, clust == 2)

sim3_x_c1 <- sum((x3_c1$xmap-x3_c1$xmap_pred)^2)/nrow(x3_c1)
sim3_y_c1 <- sum((x3_c1$ymap-x3_c1$ymap_pred)^2)/nrow(x3_c1)
sim3_x_c2 <- sum((x3_c2$xmap-x3_c2$xmap_pred)^2)/nrow(x3_c2)
sim3_y_c2 <- sum((x3_c2$ymap-x3_c2$ymap_pred)^2)/nrow(x3_c2)

#Linear Interpolation
miss_na3_1 <- filter(miss_na3, clust.x == 1)
miss_na3_2 <- filter(miss_na3, clust.x == 2)

lin3_x_c1 <- sum((miss_na3_1$xmap-miss_na3_1$xmap_pred)^2)/nrow(miss_na3_1) 
lin3_y_c1 <- sum((miss_na3_1$ymap-miss_na3_1$ymap_pred)^2)/nrow(miss_na3_1) 
lin3_x_c2 <- sum((miss_na3_2$xmap-miss_na3_2$xmap_pred)^2)/nrow(miss_na3_2)
lin3_y_c2 <- sum((miss_na3_2$ymap-miss_na3_2$ymap_pred)^2)/nrow(miss_na3_2) 

#Model without Intersection


x3_c1_noint <- filter(xy_join_noint3, clust == 1)
x3_c2_noint <- filter(xy_join_noint3, clust == 2)

sim3_x_c1_noi <- sum((x3_c1_noint$xmap-x3_c1_noint$xmap_pred)^2)/nrow(x3_c1_noint)
sim3_y_c1_noi <- sum((x3_c1_noint$ymap-x3_c1_noint$ymap_pred)^2)/nrow(x3_c1_noint)
sim3_x_c2_noi <- sum((x3_c2_noint$xmap-x3_c2_noint$xmap_pred)^2)/nrow(x3_c2_noint)
sim3_y_c2_noi <- sum((x3_c2_noint$ymap-x3_c2_noint$ymap_pred)^2)/nrow(x3_c2_noint)


```

<!-- Results Table -->


```{r results-table-by-clust, message=FALSE, warning = FALSE}

result_data_clust <- data.frame(Cluster = c(1,2,1,2,1,2), X1 = c(sqrt(sim1_x_c1), sqrt(sim1_x_c2), sqrt(sim2_x_c1), sqrt(sim2_x_c2), sqrt(sim3_x_c1), sqrt(sim3_x_c2)), Y1 = c(sqrt(sim1_y_c1), sqrt(sim1_y_c2), sqrt(sim2_y_c1), sqrt(sim2_y_c2), sqrt(sim3_y_c1), sqrt(sim3_y_c2)), X2 = c(sqrt(lin1_x_c1), sqrt(lin1_x_c2), sqrt(lin2_x_c1), sqrt(lin2_x_c2), sqrt(lin3_x_c1), sqrt(lin3_x_c2)), Y2 = c(sqrt(lin1_y_c1), sqrt(lin1_y_c2), sqrt(lin2_y_c1), sqrt(lin2_y_c2), sqrt(lin3_y_c1), sqrt(lin3_y_c2)), X3 = c(sqrt(sim1_x_c1_noi), sqrt(sim1_x_c2_noi), sqrt(sim2_x_c1_noi), sqrt(sim2_x_c2_noi), sqrt(sim3_x_c1_noi), sqrt(sim3_x_c2_noi)), Y3 = c(sqrt(sim1_y_c1_noi), sqrt(sim1_y_c2_noi), sqrt(sim2_y_c1_noi), sqrt(sim2_y_c2_noi), sqrt(sim3_y_c1_noi), sqrt(sim3_y_c2_noi)))

kableExtra::kable(result_data_clust, booktabs = TRUE, caption = "RMSE for Interpolation Methods by cluster", col.names = c("Cluster", "$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{no int}$", "$Y_{no int}$"), escape = FALSE) %>% pack_rows(
  index = (c("Simulation 1" = 2, "Simulation 2" = 2, "Simulation 3" = 2))
) %>% 
  add_header_above(c(" ", "Intersection" = 2, "Linear" = 2, "No Intersection" = 2))

```


```{r cp-table-sim, message=FALSE, warning = FALSE, cache = TRUE}

cp_simdata <- data.frame(sim = c("1", "2", "3"), cov_probx = c(cp_sim1_x, cp_sim2_x, cp_sim3_x), sdx = c(sd_sim1_x, sd_sim2_x, sd_sim3_x), cov_proby = c(cp_sim1_y, cp_sim2_y , cp_sim3_y), sdx = c(sd_sim1_y, sd_sim2_y, sd_sim3_y))

kableExtra::kable(cp_simdata, booktabs = TRUE, col.names = c("Simulation", "Proportion of X", "Avg SD of X", "Proportion of Y", "Avg SD of Y"), caption = "Proportion of Prediction interval containing observed and Average Standard Deviation of Estimates for Simulated Data") 


```


```{r cp-clust-sim, message=FALSE, warning=FALSE, cache=TRUE}

cp_sim1_x <- xy_join1 %>% group_by(clust) %>% summarise(cp = sum((inintx)/n()))
cp_sim1_y <- xy_join1 %>% group_by(clust) %>% summarise(cp = sum((ininty)/n()))

cp_sim2_x <- xy_join2 %>% group_by(clust) %>% summarise(cp = sum((inintx)/n()))
cp_sim2_y <- xy_join2 %>% group_by(clust) %>% summarise(cp = sum((ininty)/n()))

cp_sim3_x <- xy_join3 %>% group_by(clust) %>% summarise(cp = sum((inintx)/n()))
cp_sim3_y <- xy_join3 %>% group_by(clust) %>% summarise(cp = sum((ininty)/n()))


```


```{r cp-sim-table, message=FALSE, warning=FALSE, cache=TRUE}

cp_by_clust_sim <- data.frame(Cluster = c(1,2), w1x = c(round(cp_sim1_x[,2],4)), w1y = c(round(cp_sim1_y[,2],4)), w2x = round(cp_sim2_x[,2],4), w2y = round(cp_sim2_y[,2],4), w3x = round(cp_sim3_x[,2],4),  w3y = c(round(cp_sim3_y[,2],4)))
#
kableExtra::kable(cp_by_clust_sim , booktabs = TRUE, caption = "Proportion of Prediction interval containing observed by Cluster for Simulated Data", col.names = c("Cluster", "$X_{s1}$", "$Y_{s1}$", "$X_{s2}$", "$Y_{s2}$", "$X_{s3}$", "$Y_{s3}$"), escape = FALSE) %>% 
  add_header_above(c(" ", "Simulation 1" = 2, "Simulation 2" = 2, "Simulation 3" = 2))

```


A benefit of using a model-based approach to interpolate missing locations is that we are able to determine the uncertainty of the estimate. In order to do so, conditional draws of the unobserved values given the observed values can be used to quantify the uncertainty. This is accomplished by exploiting an advantage of Vecchia's Approximation that approximate draws from a Gaussian Process model can be made through the inverse Cholesky Factor [@guinness_permutation_2018]. Therefore, for each model, 30 simulations of predictions are done and used to calculate the standard deviation. These can be used to create an interval of our estimates.  The intervals are found by $\hat{x} \pm (2*sd_x)$ and $\hat{y} \pm (2*sd_y)$, where $\hat{x}$ and $\hat{y}$ are the predictions, and $sd_x$ and $sd_y$ are the standard deviations determined by the conditional simulations. Next, found the proportion of these intervals that contain the true value. For each simulation, the proportions can be found in \cref{tab:cp-table-sim}, along with the average standard deviation values. The proportions in this table are low, mainly in the 20%-30% range. Like the RMSE values, the proportions can be separated by cluster, which are found in \cref{tab:cp-sim-table}. These values are also low, with the x values for cluster 1 in Simulation 3 having almost no intervals that contain the actual value. Finally, \cref{tab:cp-table-over-under} shows the proportion of intervals that overestimate or underestimate the actual value. For Simulations 2 and 3, the x intervals underestimate, whereas the Y intervals are an overestimate. For Simulation 1, the proportion of overestimates is slightly higher than the underestimates for Y-values, but almost identical for x. 


# Results {#results}


```{r week1-clust, message=FALSE, warning = FALSE, cache = TRUE}

Week1 <- filter(tmp, i %in% c(1:7))


tmp_features_W1<- Week1 %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)

names(tmp_features_W1) <- c("gpid", "data_1", "x_1", "y_1", "xmin_1", "xmax_1", "ymin_1", "ymax_1", "xbox_1", "ybox_1", "dx_1", "dy_1", "angle_1", "kmin_1", "kmax_1", "npts_1")


data_scale_w1 = data.frame(scale(tmp_features_W1[,3:13]))
names(data_scale_w1) <- c("x_s1", "y_s1", "xmin_s1", "xmax_s1", "ymin_s1", "ymax_s1", "xbox_s1", "ybox_s1", "dx_s1", "dy_s1", "angle_s1")
tmp_features_s1= data.frame(tmp_features_W1, data_scale_w1)


set.seed(4)
km1= kmeans(select(tmp_features_s1, c(x_s1,y_s1,xbox_s1:angle_s1)), 6, nstart=25)
cluster_K1 = as.factor(km1$cluster)

tmp_features_km1 = data.frame(tmp_features_s1, cluster_K1)

anim_plot_data1 <- Week1 %>%
  left_join(select(tmp_features_km1, gpid, clust = cluster_K1)) %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  fill(xmap, ymap, matches("clust"), .direction = "downup") %>%
  arrange(gpid)


```


```{r week2-clust, message=FALSE, warning = FALSE, cache = TRUE}


Week2 <- filter(tmp, i %in% c(7:14))


tmp_features_W2<- Week2 %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)

names(tmp_features_W2) <- c("gpid", "data_2", "x_2", "y_2", "xmin_2", "xmax_2", "ymin_2", "ymax_2", "xbox_2", "ybox_2", "dx_2", "dy_2", "angle_2", "kmin_2", "kmax_2", "npts_2")


data_scale_w2 = data.frame(scale(tmp_features_W2[,3:13]))
names(data_scale_w2) <- c("x_s2", "y_s2", "xmin_s2", "xmax_s2", "ymin_s2", "ymax_s2", "xbox_s2", "ybox_s2", "dx_s2", "dy_s2", "angle_s2")
tmp_features_s2= data.frame(tmp_features_W2, data_scale_w2)
tmp_features_W12 <- full_join(tmp_features_s1, tmp_features_s2, by = 'gpid')
tmp_features_W12 <- tmp_features_W12[complete.cases(tmp_features_W12$x_2),] #Removing where we have Week 1 but not week 2
tmp_features_W12[is.na(tmp_features_W12)] <- 0 #Didn't know what to do about Na's so a fix for now
tmp_features_W12$npts <- tmp_features_W12$npts_1 + tmp_features_W12$npts_2

set.seed(4)
km2= kmeans(select(tmp_features_W12, c(xbox_s1, ybox_s1, angle_s1, dx_s1, dy_s1, x_s2, y_s2, xbox_s2:angle_s2)), 5, nstart=25)
cluster_K2 = as.factor(km2$cluster)

tmp_features_km2 = data.frame(tmp_features_W12, cluster_K2)


anim_plot_data2 <- Week2 %>%
  left_join(select(tmp_features_km2, gpid, clust = cluster_K2)) %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  fill(xmap, ymap, matches("clust"), .direction = "downup") %>%
  arrange(gpid)
```


```{r week3-clust, message=FALSE, warning = FALSE, cache = TRUE}


Week3 <- filter(tmp, i %in% c(14:22)) 


tmp_features_W3<- Week3 %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)


names(tmp_features_W3) <- c("gpid", "data_3", "x_3", "y_3", "xmin_3", "xmax_3", "ymin_3", "ymax_3", "xbox_3", "ybox_3", "dx_3", "dy_3", "angle_3", "kmin_3", "kmax_3", "npts_3")


data_scale_w3 = data.frame(scale(tmp_features_W3[,3:13]))
names(data_scale_w3) <- c("x_s3", "y_s3", "xmin_s3", "xmax_s3", "ymin_s3", "ymax_s3", "xbox_s3", "ybox_s3", "dx_s3", "dy_s3", "angle_s3")
tmp_features_s3= data.frame(tmp_features_W3, data_scale_w3)
tmp_features_W23 <- full_join(tmp_features_s2, tmp_features_s3, by = 'gpid')
tmp_features_W23 <- tmp_features_W23[complete.cases(tmp_features_W23$x_3),] #Removing where we have Week 2 but not week 3
tmp_features_W23[is.na(tmp_features_W23)] <- 0 #Didn't know what to do about Na's so a fix for now
tmp_features_W23$npts <- tmp_features_W23$npts_2 + tmp_features_W23$npts_3

#Clustering

set.seed(4)
km3= kmeans(select(tmp_features_W23, c(xbox_s2:angle_s2, x_s3, y_s3, xbox_s3:angle_s3)), 6, nstart=25)
cluster_K3 = as.factor(km3$cluster)

tmp_features_km3 = data.frame(tmp_features_W23, cluster_K3)

anim_plot_data3 <- Week3 %>%
  left_join(select(tmp_features_km3, gpid, clust = cluster_K3)) %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  fill(xmap, ymap, matches("clust"), .direction = "downup") %>%
  arrange(gpid)


```

```{r polygon-week1, message=FALSE, warning = FALSE, cache = TRUE}

#######################
###Weekly Polygons 
#######################

###########
###Week 1
###########
time_1_og <- lonely(anim_plot_data1,51)

time1 <- filter(time_1_og, clust ==2 & xmap > -1950  & ymap < -270)
time1$clust <- 3
time2 <- subset(time_1_og, !(time_1_og$gpid%in%time1$gpid))
time_1 <- rbind(time1, time2)

time1 <- filter(time_1, clust ==6 & ymap < 170)
time1$clust <- 3
time2 <- subset(time_1, !(time_1$gpid%in%time1$gpid))
time_1 <- rbind(time1, time2)

time1 <- filter(time_1, ymap > 570 & xmap > -1910 & xmap < -1830)
time1$clust <- 1
time2 <- subset(time_1, !(time_1$gpid%in%time1$gpid))
time_1 <- rbind(time1, time2)

time_1$clust[time_1$gpid==81677] <- 2

time_1$clust[time_1$gpid==85085] <- 1

poly_clust <- function(og_frame, frame, a, b, c, d, f, h){
  points <-st_as_sf(og_frame, coords = c("xmap", "ymap"))
  q_smallest <- filter(frame, clust == a)
  ###1###
  coord_1 <- coordinates2(q_smallest,a)
  y_1<- polygon_data2(coord_1)
  polyg <- y_1$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df <- merge(z_idex, frame, by= "index")
  df2 <- subset(frame,!(frame$gpid%in%df$gpid))
  ###2###
  clus2 <- subset(df2, clust==b)
  coord_2 <- coordinates2(clus2,b)
  y_2<- polygon_data2(coord_2)
  polyg <- y_2$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df3 <- merge(z_idex, df2, by= "index")
  df4 <- subset(df2,!(df2$gpid%in%df3$gpid))
  ###3###
  clus3 <- subset(df4, clust==c)
  coord_3 <- coordinates2(clus3,c)
  y_3<- polygon_data2(coord_3)
  polyg <- y_3$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df5 <- merge(z_idex, df4, by= "index")
  df6 <- subset(df4,!(df4$gpid%in%df5$gpid))
  ###4###
  clus4 <- subset(df6, clust==d)
  coord_4 <- coordinates2(clus4,d)
  y_4<- polygon_data2(coord_4)
  polyg <- y_4$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df7 <- merge(z_idex, df6, by= "index")
  df8 <- subset(df6,!(df6$gpid%in%df7$gpid))
  ###5###
  clus5 <- subset(df8, clust==f)
  coord_5 <- coordinates2(clus5,f)
  y_5<- polygon_data2(coord_5)
  polyg <- y_5$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df9 <- merge(z_idex, df8, by= "index")
  df10 <- subset(df8,!(df8$gpid%in%df9$gpid))
  ###6###
  clus8 <- filter(df10, ymap > 0)
  coord_8 <- coordinates2(clus8,h)
  y_8<- polygon_data2(coord_8)
  polyg <- y_8$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df15<- merge(z_idex, df10, by= "index")
  df16 <- subset(df10,!(df10$gpid%in%df15$gpid))
  ##6_2##
  clus9 <- filter(df10, ymap < 0)
  coord_9 <- coordinates2(clus9,7)
  y_9<- polygon_data2(coord_9)
  polyg <- y_9$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df17<- merge(z_idex, df16, by= "index")
  df18 <- subset(df16,!(df16$gpid%in%df17$gpid))
  y = rbind(y_1, y_2, y_3, y_4, y_5,y_8,y_9)
  return(y)
}

y1 <- poly_clust(time_1_og,time_1,1,6,4,3,5,2) 


```

```{r polygon-week2, message=FALSE, warning = FALSE, cache=TRUE}
###########
###Week 2
###########

time_8_og <- lonely(anim_plot_data2,58)

time1 <- filter(time_8_og, clust ==4 & xmap > -1855  & ymap < -267)
time1$clust <- 2
time2 <- subset(time_8_og, !(time_8_og$gpid%in%time1$gpid))
time_8 <- rbind(time1, time2)

time1 <- filter(time_8, clust ==5 & xmap > -1855  & ymap < -267)
time1$clust <- 2
time2 <- subset(time_8, !(time_8$gpid%in%time1$gpid))
time_8 <- rbind(time1, time2)

time1 <- filter(time_8, clust ==5 & xmap > -1940  & ymap < -293)
time1$clust <- 4
time2 <- subset(time_8, !(time_8$gpid%in%time1$gpid))
time_8 <- rbind(time1, time2)

time1 <- filter(time_8, clust ==3 & xmap < -1639  & ymap < 325)
time1$clust <- 4
time2 <- subset(time_8, !(time_8$gpid%in%time1$gpid))
time_8 <- rbind(time1, time2)

time1 <- filter(time_8_og, clust ==3 & xmap < -1870  & ymap < 390)
time1$clust <- 4
time2 <- subset(time_8, !(time_8$gpid%in%time1$gpid))
time_8 <- rbind(time1, time2)


poly_clust <- function(og_frame, frame, a, b, c, d, f){
  points <-st_as_sf(og_frame, coords = c("xmap", "ymap"))
  q_smallest <- filter(frame, clust == a)
  ###1###
  coord_1 <- coordinates2(q_smallest,a)
  y_1<- polygon_data2(coord_1)
  polyg <- y_1$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df <- merge(z_idex, frame, by= "index")
  df2 <- subset(frame,!(frame$gpid%in%df$gpid))
  ###2###
  clus2 <- subset(df2, clust==b)
  coord_2 <- coordinates2(clus2,b)
  y_2<- polygon_data2(coord_2)
  polyg <- y_2$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df3 <- merge(z_idex, df2, by= "index")
  df4 <- subset(df2,!(df2$gpid%in%df3$gpid))
  ###3###
  clus3 <- subset(df4, clust==c)
  coord_3 <- coordinates2(clus3,c)
  y_3<- polygon_data2(coord_3)
  polyg <- y_3$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df5 <- merge(z_idex, df4, by= "index")
  df6 <- subset(df4,!(df4$gpid%in%df5$gpid))
  ###4###
  clus4 <- subset(df6, clust==d)
  coord_4 <- coordinates2(clus4,d)
  y_4<- polygon_data2(coord_4)
  polyg <- y_4$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df7 <- merge(z_idex, df6, by= "index")
  df8 <- subset(df6,!(df6$gpid%in%df7$gpid))
  ###5###
  clus8 <- filter(df8, ymap > 0)
  coord_8 <- coordinates2(clus8,f)
  y_8<- polygon_data2(coord_8)
  polyg <- y_8$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df15<- merge(z_idex, df8, by= "index")
  df16 <- subset(df8,!(df8$gpid%in%df15$gpid))
  ##5_2##
  clus9 <- filter(df8, ymap < 0)
  coord_9 <- coordinates2(clus9,6)
  y_9<- polygon_data2(coord_9)
  polyg <- y_9$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df17<- merge(z_idex, df16, by= "index")
  df18 <- subset(df16,!(df16$gpid%in%df17$gpid))
  y = rbind(y_1, y_2, y_3, y_4,y_8,y_9)
  return(y)
}


y2 <- poly_clust(time_8_og,time_8,1,3,2,4,5) 


```



```{r polygon-week3, message=FALSE, warning = FALSE, cache = TRUE}
###########
###Week 3
###########

time_15_og <- lonely(anim_plot_data3,65)

time1 <- filter(time_15_og, clust ==1 & ymap > 547 & xmap > -1530)
time1$clust <- 2
time2 <- subset(time_15_og, !(time_15_og$gpid%in%time1$gpid))
time_15 <- rbind(time1, time2)

time1 <- filter(time_15, clust ==5 & ymap > 400)
time1$clust <- 3
time2 <- subset(time_15, !(time_15$gpid%in%time1$gpid))
time_15 <- rbind(time1, time2)

time1 <- filter(time_15, clust ==5 & ymap > 184, xmap > -1800)
time1$clust <- 6
time2 <- subset(time_15, !(time_15$gpid%in%time1$gpid))
time_15 <- rbind(time1, time2)

time1 <- filter(time_15, clust ==5 & xmap < -2098)
time1$clust <- 3
time2 <- subset(time_15, !(time_15$gpid%in%time1$gpid))
time_15 <- rbind(time1, time2)

time1 <- filter(time_15, clust ==5 & ymap > 251)
time1$clust <- 3
time2 <- subset(time_15, !(time_15$gpid%in%time1$gpid))
time_15 <- rbind(time1, time2)


poly_clust <- function(og_frame, frame, a, b, c, d, f, h){
  points <-st_as_sf(og_frame, coords = c("xmap", "ymap"))
  q_smallest <- filter(frame, clust == a)
  ###1###
  coord_1 <- coordinates2(q_smallest,a)
  y_1<- polygon_data2(coord_1)
  polyg <- y_1$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df <- merge(z_idex, frame, by= "index")
  df2 <- subset(frame,!(frame$gpid%in%df$gpid))
  ###2###
  clus2 <- subset(df2, clust==b)
  coord_2 <- coordinates2(clus2,b)
  y_2<- polygon_data2(coord_2)
  polyg <- y_2$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df3 <- merge(z_idex, df2, by= "index")
  df4 <- subset(df2,!(df2$gpid%in%df3$gpid))
  ###3###
  clus3 <- subset(df4, clust==c)
  coord_3 <- coordinates2(clus3,c)
  y_3<- polygon_data2(coord_3)
  polyg <- y_3$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df5 <- merge(z_idex, df4, by= "index")
  df6 <- subset(df4,!(df4$gpid%in%df5$gpid))
  ###4###
  clus4 <- subset(df6, clust==d)
  coord_4 <- coordinates2(clus4,d)
  y_4<- polygon_data2(coord_4)
  polyg <- y_4$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df7 <- merge(z_idex, df6, by= "index")
  df8 <- subset(df6,!(df6$gpid%in%df7$gpid))
  ###5###
  clus5 <- subset(df8, clust==f)
  coord_5 <- coordinates2(clus5,f)
  y_5<- polygon_data2(coord_5)
  polyg <- y_5$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df9 <- merge(z_idex, df8, by= "index")
  df10 <- subset(df8,!(df8$gpid%in%df9$gpid))
  ###6###
  clus8 <- filter(df10, clust == h)
  coord_8 <- coordinates2(clus8,h)
  y_8<- polygon_data2(coord_8)
  polyg <- y_8$geometry
  z <- st_intersects(polyg,points)
  z_idex <- data.frame(z[1])
  colnames(z_idex) <- "index"
  df15<- merge(z_idex, df10, by= "index")
  df16 <- subset(df10,!(df10$gpid%in%df15$gpid))
  y = rbind(y_1, y_2, y_3, y_4, y_5,y_8)
  return(y)
}


y3 <- poly_clust(time_15_og,time_15,4,2,1,6,5,3) 

```


```{r get-int, message=FALSE, warning = FALSE}

## Week 1
m1 <- get_int(time_1_og,y1,y2)#2 Hidden
m3 <- get_int2(time_1_og,y1,y2) #Poylgon Coordinates for Grid


#Week 3
m2 <- get_int(time_15_og,y2,y3) #4 Hidden
m4 <- get_int2(time_15_og,y2,y3) #4 Hidden

#Week 2 - Intersection of Week 1 and 3
m5 <- get_int(time_8_og,y1,y3) 
m6 <- get_int2(time_8_og,y1,y3)


```


```{r ex-result,  fig.cap= "Results of Clustering using Bounding Box Method for all the Data", cache=TRUE, message=FALSE, warning = FALSE}

tmp_features<- tmp %>% 
  tidyr::nest(data = -gpid) %>% 
  mutate(summary = map(data, bbox_summary)) %>%
  unnest(summary)
#Standardize Variables
data_scale = data.frame(scale(tmp_features[,3:13]))

names(data_scale) <- c("x_s", "y_s", "xmin_s", "xmax_s", "ymin_s", "ymax_s", "xbox_s", "ybox_s", "dx_s", "dy_s", "angle_s")

tmp_features_s= data.frame(tmp_features, data_scale)
tmp_features_s = tmp_features_s[,-c(3:13)]

km= kmeans(select(tmp_features_s, c(x_s,y_s,xbox_s:angle_s)), 5, nstart=25)
cluster_K = as.factor(km$cluster)

tmp_features_km = data.frame(tmp_features_s, cluster_K)

anim_plot_data <- tmp %>%
  left_join(select(tmp_features_km, gpid, clust = cluster_K)) %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  fill(xmap, ymap, matches("clust"), .direction = "downup") %>%
  arrange(gpid)

anim_plot_data51 <- filter(anim_plot_data, t==72)

anim_plot <- anim_plot_data51 %>%
  ggplot(aes(x = xmap, y = ymap, frame = t, group = gpid, ids = gpid,
             color = clust, fill = clust, shape = imputed)) + 
  geom_point(size = 0.75) + 
  scale_shape_manual("Imputed", values = c("FALSE" = 15, "TRUE" = 0)) + 
  scale_color_viridis_d() + 
  scale_fill_viridis_d() + ggtitle("Clustering using all data on last day")

anim_plot
```



```{r all-week-comp, fig.cap = "Comparison of our method to results of a previous method, which find cracks using a kinematic algorithm", out.width="100%", fig.align = 'center'}

knitr::include_graphics("images/all_weeks_comp_correct.png")

```

The simulation results showed some promise for our methods, so next the methods are applied on the ice motion dataset.


## Clustering Using All Data

First we used our spatio-temporal clustering method by creating bounding boxes around the full trajectory of each gpid. Once the bounding box features are created and standardized, the number of clusters were determined using the silhouette statistic. This silhouette statistic for the full trajectory data suggested five clusters to be appropriate. The results from the clustering algorithm are shown in \cref{fig:ex-result}. In \cref{fig:ex-result}, each of the colors represent one cluster; each of the squares represent a gpid. The locations of the gpids on this map are determined by its last observed location in the data set. We hypothesize that cracks in the sea ice will be found along the boundaries of the clusters as those gpids have different movements. In the middle of some of the clusters, there are some singular gpids whose cluster membership does not match its neighbors. These gpids in practicality will belong to the same clusters as its neighbors and a crack is not considered to have formed here. 

To see how well our method is performing, we can compare it to other sea ice crack detection algorithms. One such method is by plotting the deformation data using a kinematic crack algorithm calculated using the RGPS data [@peterson_evaluating_2011]. \cref{fig:all-week-comp} gives a comparison of our bounding box clustering method and the kinematic crack algorithm using side-by-side graphs. It is important to note that this image does not represent the true ice cracks, just the cracks determined by this method. The larger cracks, which are represented by the yellow and dark blue lines are more certain. The graphs are aligned so that their axes match up for easier comparison. The lines given in the left figure are derived from the opening magnitude of the ice sheet. Most of the figure has a color around zero, denoting no opening, whereas potential large cracks are found with the dark blue or yellow. Many of our cluster boundaries match up with the cracks identified by the kinematic crack algorithm, though we are limited due to the number of clusters. For example, on the bottom left side of both plots, we see a line that goes down and to the right. At the bottom right of both plots, we see a line moving through the hole in the gpids. Additionally, towards the top of both figures, we see a curved boundary that starts at the top of the ice sheet and moves to the bottom left in a curved manner. 


## Clusterings By Weeks

Since trajectories can be long, it may also be useful to look at sub-trajectories, as we may lose information by clustering the whole trajectory, which may impact results. We split the data into three separate weeks. Through some testing of our methods, we found that by week was the smallest time period we could successfully cluster, as anything smaller led to large changes in the clusterings between each weeks (unstable). We would expect the weeks to have different clusterings, but would also expect there to be some continuity between them. The clusters by week can be found in \cref{fig:by-week-cluster-plot}, where can see some similarities in clusters over time. 

With these weekly clusters we can also compare them similarly to \cref{fig:all-week-comp}, instead manipulating the days used to create the magnitudes by the proper weeks time frame. Additionally, we had a picture from Day 66 of our data set, which gives an image of recurring leads based on visible and infrared Advanced Very High Resolution Radiometer (AVHRR) data [@lead_pic_2009]. \cref{fig:week-comp}, includes grid lines to aid in comparison of the pictures, due to the aspect ratios not aligning perfectly. One major cracks stands out in both images. Our boundary between cluster 1 and 3 aligns until about (-1880,400), at this point our boundary curves in more steeply. Eventually, the upper section of the cluster boundary between clusters 5 and 6 continue the crack. Additionally, we can see some of the lighter cracks in our graph. For example, we can also see another curved boundary above the one just discussed, and can see one on the bottom left, similar to what was seen in \cref{fig:all-week-comp}.


```{r by-week-cluster-plot, fig.cap= "Results of Clustering using Bounding Box Method by Week", cache=TRUE, message=FALSE, warning = FALSE, fig.height = 5, fig.width = 9}

# anim_plot1 <- anim_plot_data1 %>%
#   ggplot(aes(x = xmap, y = ymap, frame = t, group = gpid, ids = gpid,
#              color = clust, fill = clust)) + #, shape = imputed)) + 
#   geom_point() + 
#   #scale_shape_manual("Imputed", values = c("FALSE" = 15, "TRUE" = 0)) + 
#   scale_color_viridis_d() + 
#   scale_fill_viridis_d() + ggtitle("Week 1")
# 
# 
# anim_plot2 <- anim_plot_data2 %>%
#   ggplot(aes(x = xmap, y = ymap, frame = t, group = gpid, ids = gpid,
#              color = clust, fill = clust)) + #, shape = imputed)) + 
#   geom_point() + 
#  # scale_shape_manual("Imputed", values = c("FALSE" = 15, "TRUE" = 0)) + 
#   scale_color_viridis_d() + 
#   scale_fill_viridis_d() + ggtitle("Week 2")
# 
# anim_plot3 <- anim_plot_data3 %>%
#   ggplot(aes(x = xmap, y = ymap, frame = t, group = gpid, ids = gpid,
#              color = clust, fill = clust)) + #, shape = imputed)) + 
#   geom_point() + 
#   #scale_shape_manual("Imputed", values = c("FALSE" = 15, "TRUE" = 0)) + 
#   scale_color_viridis_d() + 
#   scale_fill_viridis_d() + ggtitle("Week 3")
# 

#ggarrange(anim_plot1, anim_plot2, anim_plot3, ncol = 2, nrow = 2)

anim_plot_data1_2 <-anim_plot_data1
anim_plot_data2_2 <-anim_plot_data2
anim_plot_data3_2 <-anim_plot_data3

anim_plot_data1_2$week <- 1
anim_plot_data2_2$week <- 2
anim_plot_data3_2$week <- 3

anim_plot_data_all <- rbind(anim_plot_data1_2, anim_plot_data2_2, anim_plot_data3_2)


anim_plot_data_all %>%
  ggplot(aes(x = xmap, y = ymap, frame = t, group = gpid, ids = gpid,
             color = clust, fill = clust)) + #, shape = imputed)) + 
  geom_point() + 
  #scale_shape_manual("Imputed", values = c("FALSE" = 15, "TRUE" = 0)) + 
  scale_color_viridis_d() + 
  scale_fill_viridis_d() + ggtitle("Cluster By Week") + facet_wrap(vars(week))

```


```{r week-comp, fig.cap = "Comparison of our method to results of a previous method for Week 3", out.width="100%", fig.align = 'center'}

knitr::include_graphics("images/week-comp.png")

```

## Univariate Interpolation

```{r data-for-clust-plots, message=FALSE, warning = FALSE, cache=TRUE }
apd1_nomiss <- filter(anim_plot_data1, imputed == FALSE)
apd2_nomiss <- filter(anim_plot_data2, imputed == FALSE)
apd3_nomiss <- filter(anim_plot_data3, imputed == FALSE)

#Squiggly Plots Example

apd1_nomiss_c21 <- filter(apd2_nomiss, clust == 1)
apd2_nomiss_c25 <- filter(apd2_nomiss, clust == 5)

tmp1 <-  apd1_nomiss_c21 %>% highlight_key(~gpid)
 
p1 <- ggplot(tmp1, aes(x = xmap, y = ymap, group = gpid, 
                 hoverinfo = NULL,
                 color = factor(gpid %% 10))) + 
  geom_path(arrow = arrow(length = unit(1, "mm")), alpha = .5) + 
  scale_color_viridis_d() +
  theme(legend.position="none") + ggtitle("a)")

tmp2 <-  apd2_nomiss_c25 %>% highlight_key(~gpid)

p2 <- ggplot(tmp2, aes(x = xmap, y = ymap, group = gpid, 
                 hoverinfo = NULL,
                 color = factor(gpid %% 10))) + 
  geom_path(arrow = arrow(length = unit(1, "mm")), alpha = .5) + 
  scale_color_viridis_d() +
  theme(legend.position="none")+ ggtitle("b)")

```


```{r int-best-plots, message=FALSE, warning = FALSE, cache=TRUE, fig.cap = "Trajectories for a) Week 2 Cluster 1, where our method peforms best and b) Week 2 Cluster 5, where linear interpolation performs best. These are instances where our method outperforms linear interpolation"}

ggarrange(p1, p2, ncol = 2)

```



<!----------------- Intersection Method ----------------------->

```{r int-functions}

grid_tp <- function(frame, t, m){
  time_og <- lonely(frame,t)
  pts <- st_as_sf(time_og, coords = c("xmap", "ymap")) 
  grid_50 <- st_make_grid(m, cellsize = c(10, 10)) %>% 
    st_sf(grid_id = 1:length(.))
  grid_lab <- st_centroid(grid_50) %>% cbind(st_coordinates(.))
  pts_grd <- pts %>% st_join(grid_50, join = st_intersects) %>% as.data.frame
  all_pts_grd <- left_join(pts_grd,grid_lab,by= "grid_id")
  all_pts_grd2 <- all_pts_grd %>% distinct(gpid, .keep_all= TRUE)
  return(all_pts_grd2)
}

known_data <- function(m, int, tp, grid, frame){ #Grid doesn't really matter as much (only when doing cross validation)
  int1 <- filter(m, intersection == int)[,-c(2,3)]
  int_t1 <- right_join(grid[,-5], int1, by = "gpid")
  anim_t1 <- filter(frame, t == tp)
  int_t1_2 <- inner_join(int_t1, anim_t1, by = "gpid")
  final_int_t1 <- int_t1_2[,c(2,4,5,6,9,10, 13,14,17,18)]
  t1_notimputed =  filter(final_int_t1, imputed == "FALSE")[,c(1,2,5,6,7,8,9)]
  colnames(t1_notimputed) <- c("gpid", "gpid_id", "intersection", "obs_time", "xmap","ymap", "clust")
  t1_notimputed <- t1_notimputed[complete.cases(t1_notimputed),]
  return(t1_notimputed)
}

known_data_grid <- function(m, int, tp, grid, frame){ #Grid doesn't really matter as much (only when doing cross validation)
  int1 <- filter(m, intersection == int)[,-c(2,3)]
  int_t1 <- right_join(grid[,-5], int1, by = "gpid")
  anim_t1 <- filter(frame, t == tp)
  int_t1_2 <- inner_join(int_t1, anim_t1, by = "gpid")
  final_int_t1 <- int_t1_2[,c(2,4,5,6,9,10, 13,14,17,18)]
  t1_notimputed =  filter(final_int_t1, imputed == "FALSE")[,c(1,2,3,4,5,6,9)]
  colnames(t1_notimputed) <- c("gpid", "gpid_id", "xmap","ymap", "intersection", "obs_time", "clust")
  t1_notimputed <- t1_notimputed[complete.cases(t1_notimputed),]
  return(t1_notimputed)
}


xmap_10CV <- function(m_new, m, int, tp, frame){
  grid <- grid_tp(frame, tp , m)
  obs1 <- known_data(m_new,int,tp-1,grid, frame)
  obs2 <- known_data(m_new,int,tp,grid, frame)
  obs3 <- known_data(m_new,int,tp+1,grid, frame)
  obs4 <- known_data(m_new,int,tp+2,grid, frame)
  set.seed(4)
  rand_df <- obs2[sample(nrow(obs2), size=(round(0.9*nrow(obs2)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,5]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_data_grid(m_new,int,tp,grid, frame)
  est_df <- subset(predicted_locs,!(predicted_locs$gpid%in%rand_df$gpid))
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  return(preds)
}

ymap_10CV <- function(m_new, m, int, tp, frame){
  grid <- grid_tp(frame, tp , m)
  obs1 <- known_data(m_new,int,tp-1,grid, frame)
  obs2 <- known_data(m_new,int,tp,grid, frame)
  obs3 <- known_data(m_new,int,tp+1,grid, frame)
  obs4 <- known_data(m_new,int,tp+2,grid, frame)
  set.seed(4)
  rand_df <- obs2[sample(nrow(obs2), size=(round(0.9*nrow(obs2)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,6]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_data_grid(m_new,int,tp,grid, frame)
  est_df <- subset(predicted_locs,!(predicted_locs$gpid%in%rand_df$gpid))
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  return(preds)
}


all_pred <- data.frame(NULL)
pred_cv <- data.frame(NULL)

#xmap
all_cv_x <- function(int_df,int_list,df){
  for (g in min(df$t):max(df$t)){
    for (i in 1:max(int_df$intersection)){
      tryCatch({
        pred = xmap_10CV(int_df, int_list, i, g, df)
        pred_df = data.frame(pred,i,g)
        pred_cv = rbind.data.frame(pred_cv, pred_df)}, error=function(e){return(NA)})
    }
    all_pred <- rbind.data.frame(all_pred,pred_cv)
  }
  return(all_pred)
}

#ymap
all_cv_y <- function(int_df,int_list,df){
  for (g in min(df$t):max(df$t)){
    for (i in 1:max(int_df$intersection)){
      tryCatch({
        pred = ymap_10CV(int_df, int_list, i, g, df)
        pred_df = data.frame(pred,i,g)
        pred_cv = rbind.data.frame(pred_cv, pred_df)}, error=function(e){return(NA)})
    }
    all_pred <- rbind.data.frame(all_pred,pred_cv)
  }
  return(all_pred)
}


##Get Interval

xmap_10CV_pred <- function(m_new, m, int, tp, frame){
  grid <- grid_tp(frame, tp , m)
  obs1 <- known_data(m_new,int,tp-1,grid, frame)
  obs2 <- known_data(m_new,int,tp,grid, frame)
  obs3 <- known_data(m_new,int,tp+1,grid, frame)
  obs4 <- known_data(m_new,int,tp+2,grid, frame)
  set.seed(4)
  rand_df <- obs2[sample(nrow(obs2), size=(round(0.9*nrow(obs2)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,5]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_data_grid(m_new,int,tp,grid, frame)
  est_df <- subset(predicted_locs,!(predicted_locs$gpid%in%rand_df$gpid))
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ncondsim <- 30
  sims <- cond_sim(fit = fit, locs_pred = pred_loc, X_pred = X_pred, covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                   ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE, nsims = ncondsim)
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  b <- list(sims,preds)
  return(b)
}

ymap_10CV_pred <- function(m_new, m, int, tp, frame){
  grid <- grid_tp(frame, tp , m)
  obs1 <- known_data(m_new,int,tp-1,grid, frame)
  obs2 <- known_data(m_new,int,tp,grid, frame)
  obs3 <- known_data(m_new,int,tp+1,grid, frame)
  obs4 <- known_data(m_new,int,tp+2,grid, frame)
  set.seed(4)
  rand_df <- obs2[sample(nrow(obs2), size=(round(0.9*nrow(obs2)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,6]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_data_grid(m_new,int,tp,grid, frame)
  est_df <- subset(predicted_locs,!(predicted_locs$gpid%in%rand_df$gpid))
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ncondsim <- 30
  sims <- cond_sim(fit = fit, locs_pred = pred_loc, X_pred = X_pred, covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                   ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE, nsims = ncondsim)
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  b <- list(sims,preds)
  return(b)
}

sd_pred <- function(sim, pred_df){
v <- NULL
sum_squares <- rep(0,nrow(sim))
for(j in 1:30){
  v <- sim[,j] #fill in Known with estimated
  sum_squares <- sum_squares + (pred_df$pred_grid - v)^2
}
pred_rmse <- sqrt(1/30*sum_squares)
pred_df$sd <- pred_rmse
pred_df$lc <- pred_df$pred_grid - pred_rmse
pred_df$uc <- pred_df$pred_grid + pred_rmse
return(pred_df)
}


all_cv_x_pred <- function(int_df,int_list,df){
   for (p in min(df$t):max(df$t)){
     for (q in 1:max(int_df$intersection)){
      tryCatch({
        pred = xmap_10CV_pred(int_df, int_list, q, p, df)
        sd_df <- sd_pred(pred[[1]],pred[[2]])
        df_p = data.frame(sd_df, q, p)
        pred_cv = rbind.data.frame(pred_cv, df_p)}, error=function(e){return(NA)})
    }
    #all_pred <- rbind.data.frame(all_pred,pred_cv)
  }
  all_pred <- rbind.data.frame(all_pred,pred_cv)
  return(all_pred)
}

all_cv_y_pred <- function(int_df,int_list,df){
   for (p in min(df$t):max(df$t)){
     for (q in 1:max(int_df$intersection)){
      tryCatch({
        pred = ymap_10CV_pred(int_df, int_list, q, p, df)
        sd_df <- sd_pred(pred[[1]],pred[[2]])
        df_p = data.frame(sd_df, q, p)
        pred_cv = rbind.data.frame(pred_cv, df_p)}, error=function(e){return(NA)})
    }
    #all_pred <- rbind.data.frame(all_pred,pred_cv)
  }
  all_pred <- rbind.data.frame(all_pred,pred_cv)
  return(all_pred)
}


```

```{r week1-x-cv, cache=TRUE, message=FALSE, warning = FALSE}

w1_xcv <- all_cv_x_pred(m1,m3,anim_plot_data1)
```

```{r week2-x-cv, cache=TRUE, message=FALSE, warning = FALSE}
w2_xcv <- all_cv_x_pred(m5,m6,anim_plot_data2)
```

```{r week3-x-cv, cache=TRUE, message=FALSE, warning = FALSE}
w3_xcv <- all_cv_x_pred(m2,m4,anim_plot_data3)
```

```{r week1-y-cv, cache=TRUE, message=FALSE, warning = FALSE}
w1_ycv <- all_cv_y_pred(m1,m3,anim_plot_data1) #for some reason has less y values (weird)
```

```{r week2-y-cv, cache=TRUE, message=FALSE, warning = FALSE}
w2_ycv <- all_cv_y_pred(m5,m6,anim_plot_data2)
```

```{r week3-y-cv, cache=TRUE, message=FALSE, warning = FALSE}
w3_ycv <- all_cv_y_pred(m2,m4,anim_plot_data3)
```


```{r cv-error-functions}

cv_error_x <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed =="FALSE")[,c(1,2,3,5,6,7)]
  colnames(pred_df) <- c("gpid", "pred", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data)
  error <- sum((all_data_distinct$pred-all_data_distinct$xmap)^2)/nrow(all_data_distinct)
  return(error)
}

cv_error_y <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed == "FALSE")[,c(1,2,3,5,6,7)]
  colnames(pred_df) <- c("gpid", "pred", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data)
  error <- sum((all_data_distinct$pred-all_data_distinct$ymap)^2)/nrow(all_data_distinct)
  return(error)
}



#Gives Dataframe with actual and predicted
cv_df <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed == "FALSE")[,c(1,2,3,5,6,7,10)]
  colnames(pred_df) <- c("gpid", "pred", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data)
  #error <- sum((all_data_distinct$pred-all_data_distinct$xmap)^2)/nrow(all_data_distinct)
  return(all_data_distinct)
}

cv_error_x_p <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed =="FALSE")[,c(1,2,3,5,6,7)]
  colnames(pred_df) <- c("gpid", "pred", "sd", "low", "up", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data)
  error <- sum((all_data_distinct$pred-all_data_distinct$xmap)^2)/nrow(all_data_distinct)
  return(error)
}

cv_error_y_p <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed =="FALSE")[,c(1,2,3,5,6,7)]
  colnames(pred_df) <- c("gpid", "pred", "sd", "low", "up", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data)
  error <- sum((all_data_distinct$pred-all_data_distinct$ymap)^2)/nrow(all_data_distinct)
  return(error)
}

cv_df_p <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed == "FALSE")[,c(1,2,3,5,6,7,10)]
  colnames(pred_df) <- c("gpid", "pred", "sd", "low", "up", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data)
  #error <- sum((all_data_distinct$pred-all_data_distinct$xmap)^2)/nrow(all_data_distinct)
  return(all_data_distinct)
}


```





```{r actual-int-error, message=FALSE, warning = FALSE}

e_w1_x <- cv_error_x_p(m1, anim_plot_data1, w1_xcv) #99.56
e_w2_x <- cv_error_x_p(m5, anim_plot_data2, w2_xcv) #27.43
e_w3_x <- cv_error_x_p(m2, anim_plot_data3, w3_xcv) #65.61

e_w1_y <- cv_error_y_p(m1, anim_plot_data1, w1_ycv) #115.96
e_w2_y <- cv_error_y_p(m5, anim_plot_data2, w2_ycv) #68.64
e_w3_y <- cv_error_y_p(m2, anim_plot_data3, w3_ycv) #97.1
```

<!----------------- Linear Interpolation ----------------------->

```{r linear-int-actual-bigdf, message=FALSE, warning = FALSE}

## Linear Interpolation

anim_plot_data <- tmp %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

anim_plot_data$missing <- "no"
anim_plot_data$missing[is.na(anim_plot_data$xmap)] <- "yes"

```


```{r combine-df, message=FALSE, warning = FALSE, cache=TRUE}


#### Trying to get a sense of where large differences come from
e_w1_df_x <- cv_df_p(m1, anim_plot_data1,w1_xcv) 
e_w1_df_xdiff <- data.frame(e_w1_df_x, diffx = e_w1_df_x$xmap-e_w1_df_x$pred)
e_w2_df_x <- cv_df_p(m5, anim_plot_data2,w2_xcv) 
e_w2_df_xdiff <- data.frame(e_w2_df_x, diffx = e_w2_df_x$xmap-e_w2_df_x$pred)
e_w3_df_x <- cv_df_p(m2, anim_plot_data3,w3_xcv)
e_w3_df_xdiff <- data.frame(e_w3_df_x, diffx = e_w3_df_x$xmap-e_w3_df_x$pred)

e_w1_df_y <- cv_df_p(m1, anim_plot_data1,w1_ycv) 
e_w1_df_ydiff <- data.frame(e_w1_df_y, diffy = e_w1_df_y$ymap-e_w1_df_y$pred)
e_w2_df_y <- cv_df_p(m5, anim_plot_data2,w2_ycv)
e_w2_df_ydiff <- data.frame(e_w2_df_y, diffy = e_w2_df_y$ymap-e_w2_df_y$pred)
e_w3_df_y <- cv_df_p(m2, anim_plot_data3,w3_ycv)
e_w3_df_ydiff <- data.frame(e_w3_df_y, diffy = e_w3_df_y$ymap-e_w3_df_y$pred)

#Need to think this through a bit
e_w1_df =  inner_join(e_w1_df_xdiff, e_w1_df_ydiff, by= c("gpid", "t"))[c(1:11,13:16)]
e_w2_df =  inner_join(e_w2_df_xdiff, e_w2_df_ydiff, by= c("gpid", "t"))[c(1:11,13:16)]
e_w3_df =  inner_join(e_w3_df_xdiff, e_w3_df_ydiff, by= c("gpid", "t"))[c(1:11,13:16)]
```




```{r linear-int-actual-w1, message=FALSE, warning = FALSE, cache=TRUE}

anim_plot <- filter(anim_plot_data, t < 58)

#combine cv_data sets to get actual data
cv_data <- e_w1_df[c(1,7,9,10,11)]
colnames(cv_data) <- c("gpid", "t","xmap2", "ymap2", "cluster")

## need to get linear interpolations on cross validated data
join1 <- full_join(cv_data, anim_plot, by = c("gpid", "t")) #df with og data and cv data set
join_nocv <- filter(join1, is.na(xmap2)) #data not used in cv
join_cv <- filter(join1, !is.na(xmap2)) #data used in cv

join_cv$xmap<- NA #want data used in cv to be miss
join_cv$ymap<- NA

back_toget <- rbind(join_nocv, join_cv) #bring them back together so now have missing cv data in the dataset

#but in order
back_toget <- back_toget %>%
  arrange(gpid, t)


back_toget <- back_toget[-c(3,4,6,7,10,11)] #remove unnecessary columns

#second missing variable to help denote if cv data or not
back_toget$missing2 <- "no"
back_toget$missing2[is.na(back_toget$xmap)] <- "yes"

#linear interpolation
lin <- back_toget %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

#get data used in cv (not origingally missing, but missing now)
miss <- filter(lin, missing == "no" & missing2 == "yes")
colnames(miss) <- c("gpid", "t", "cluster" , "xmap_pred", "ymap_pred","imputed","missing", "missing2")
miss_join <- left_join(cv_data, miss[,-3], by = c("gpid", "t"))#combine with actual values dataset

miss_na <- filter(miss_join, !is.na(xmap_pred))#remove NAs if first day of week
miss_na <- miss_na %>% arrange(gpid,t)
miss_na1 <- miss_na %>% distinct(gpid,t, .keep_all= TRUE) #Remove duplicated data

#MSE
error_x_lin_actual_w1 <- sum((miss_na1$xmap2-miss_na1$xmap_pred)^2)/nrow(miss_na1) #4.735
error_y_lin_actual_w1 <- sum((miss_na1$ymap2-miss_na1$ymap_pred)^2)/nrow(miss_na1) #8.91

```


```{r linear-int-actual-w2, message=FALSE, warning = FALSE, cache=TRUE}
# 
anim_plot <- filter(anim_plot_data, t > 57 & t < 65)

#combine cv_data sets to get actual data
cv_data <- e_w2_df[c(1,7,9,10,11)]
colnames(cv_data) <- c("gpid", "t","xmap2", "ymap2", "cluster")

## need to get linear interpolations on cross validated data
join1 <- full_join(cv_data, anim_plot, by = c("gpid", "t")) #df with og data and cv data set
join_nocv <- filter(join1, is.na(xmap2)) #data not used in cv
join_cv <- filter(join1, !is.na(xmap2)) #data used in cv

join_cv$xmap<- NA #want data used in cv to be miss
join_cv$ymap<- NA

back_toget <- rbind(join_nocv, join_cv) #bring them back together so now have missing cv data in the dataset

#but in order
back_toget <- back_toget %>%
  arrange(gpid, t)

back_toget <- back_toget[-c(3,4,6,7,10,11)] #remove unnecessary columns

#second missing variable to help denote if cv data or not
back_toget$missing2 <- "no"
back_toget$missing2[is.na(back_toget$xmap)] <- "yes"

#linear interpolation
lin <- back_toget %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

#get data used in cv (not origingally missing, but missing now)
miss <- filter(lin, missing == "no" & missing2 == "yes")
colnames(miss) <- c("gpid", "t", "cluster" , "xmap_pred", "ymap_pred","imputed","missing", "missing2")
miss_join <- left_join(cv_data, miss[,-3], by = c("gpid", "t")) #combine with actual values dataset

miss_na <- filter(miss_join, !is.na(xmap_pred)) #remove NAs if first day of week
miss_na <- miss_na %>% arrange(gpid,t)
miss_na2 <- miss_na %>% distinct(gpid,t, .keep_all= TRUE) #Remove duplicated data

#MSE
error_x_lin_actual_w2 <- sum((miss_na2$xmap2-miss_na2$xmap_pred)^2)/nrow(miss_na2) #4.735
error_y_lin_actual_w2 <- sum((miss_na2$ymap2-miss_na2$ymap_pred)^2)/nrow(miss_na2) #8.91


```

```{r linear-int-actual-w3, message=FALSE, warning = FALSE, cache=TRUE}

anim_plot <- filter(anim_plot_data, t > 64 & t < 73)

#combine cv_data sets to get actual data
cv_data <- e_w3_df[c(1,7,9,10,11)]
colnames(cv_data) <- c("gpid", "t","xmap2", "ymap2", "cluster")

## need to get linear interpolations on cross validated data
join1 <- full_join(cv_data, anim_plot, by = c("gpid", "t")) #df with og data and cv data set
join_nocv <- filter(join1, is.na(xmap2)) #data not used in cv
join_cv <- filter(join1, !is.na(xmap2)) #data used in cv

join_cv$xmap<- NA #want data used in cv to be miss
join_cv$ymap<- NA

back_toget <- rbind(join_nocv, join_cv) #bring them back together so now have missing cv data in the dataset

#but in order
back_toget <- back_toget %>%
  arrange(gpid, t)

back_toget <- back_toget[-c(3,4,6,7,10,11)] #remove unnecessary columns

#second missing variable to help denote if cv data or not
back_toget$missing2 <- "no"
back_toget$missing2[is.na(back_toget$xmap)] <- "yes"

#linear interpolation
lin <- back_toget %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

#get data used in cv (not origingally missing, but missing now)
miss <- filter(lin, missing == "no" & missing2 == "yes")
colnames(miss) <- c("gpid", "t", "cluster" , "xmap_pred", "ymap_pred","imputed","missing", "missing2")
miss_join <- left_join(cv_data, miss[,-3], by = c("gpid", "t")) #combine with actual values dataset

miss_na <- filter(miss_join, !is.na(xmap_pred)) #remove NAs if first day of week
miss_na <- miss_na %>% arrange(gpid,t)
miss_na3 <- miss_na %>% distinct(gpid,t, .keep_all= TRUE) #Remove duplicated data

#MSE
error_x_lin_actual_w3 <- sum((miss_na3$xmap2-miss_na3$xmap_pred)^2)/nrow(miss_na3) #4.735
error_y_lin_actual_w3 <- sum((miss_na3$ymap2-miss_na3$ymap_pred)^2)/nrow(miss_na3) #8.91
# 

```

<!----------------- No Intersection Model ----------------------->

```{r no-int-actual-functions}

polyg_noint <- function(df, tp){
  feat_t1 <- filter(df, t == tp)
  feat_t1_og <- rownames_to_column(feat_t1, "index")
  points <- st_as_sf(feat_t1, coords = c("xmap", "ymap"))
  coord <- feat_t1_og[,c(6,7)]
  ConvexHull = chull(coord)
  coord_1 <- data.frame(coord[ConvexHull,],1)
  my.sf.point <- st_as_sf(x = coord_1, 
                          coords = c("xmap", "ymap"))
  polys = st_sf(
    aggregate(
      my.sf.point$geometry,
      list(my.sf.point$X1),
      function(g){
        st_cast(st_combine(g),"POLYGON")
      }
    ))
  #Should be final data frame needed
  op <- st_cast(polys, "MULTIPOLYGON")
  poly <- op$geometry #(p)- polygon of the whole dataset
  return(poly)
}

grid_tp <- function(frame, t, m){
  time_og <- lonely(frame,t)
  pts <- st_as_sf(time_og, coords = c("xmap", "ymap")) 
  grid_50 <- st_make_grid(m, cellsize = c(10, 10)) %>% 
    st_sf(grid_id = 1:length(.))
  grid_lab <- st_centroid(grid_50) %>% cbind(st_coordinates(.))
  pts_grd <- pts %>% st_join(grid_50, join = st_intersects) %>% as.data.frame
  all_pts_grd <- left_join(pts_grd,grid_lab,by= "grid_id")
  all_pts_grd2 <- all_pts_grd %>% distinct(gpid, .keep_all= TRUE)
  return(all_pts_grd2)
}


#Get Known Data

known_all <- function(frame, tp, grid){
  anim_t1 <- filter(frame, t == tp)
  int_t1 <- inner_join(grid, anim_t1, by = "gpid")
  final_int_t1 <- int_t1[,c(2,3,6,7,9,12,13,17)]
  t_known <- filter(final_int_t1, imputed == FALSE)[,-c(3,4,8)]
  colnames(t_known) <- c("gpid", "cluster", "obs_time", "xmap", "ymap")
  t_known <- t_known[complete.cases(t_known),]
  return(t_known)
}

#Get Unknown Grid Data
#Get Known Data
known_all_grid <- function(frame, tp, grid){
  anim_t1 <- filter(frame, t == tp)
  int_t1 <- inner_join(grid, anim_t1, by = "gpid")
  final_int_t1 <- int_t1[,c(2,3,6,7,9,12,13,17)]
  t_unknown <- filter(final_int_t1, imputed == FALSE)[,-c(6,7,8)]
  colnames(t_unknown) <- c("gpid", "cluster", "xmap", "ymap", "obs_time" )
  t_unknown <- t_unknown[complete.cases(t_unknown),]
  return(t_unknown)
}


xmap_10CV_all <- function(tp, frame){
  p <- polyg_noint(frame, tp)
  grid <- grid_tp(frame, tp , p)
  obs1 <- known_all(frame, tp-1, grid)
  obs2 <- known_all(frame, tp, grid)
  obs3 <- known_all(frame, tp+1, grid)
  obs4 <- known_all(frame, tp+2, grid)
  set.seed(4)
  rand_df <- obs2[sample(nrow(obs2), size=(round(0.9*nrow(obs2)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,4]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(N-1,30)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_all_grid(frame, tp, grid)
  est_df <- subset(predicted_locs,!predicted_locs$gpid%in%rand_df$gpid)
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  return(preds)
}

ymap_10CV_all <- function(tp, frame){
   p <- polyg_noint(frame, tp)
  grid <- grid_tp(frame, tp , p)
  obs1 <- known_all(frame, tp-1, grid)
  obs2 <- known_all(frame, tp, grid)
  obs3 <- known_all(frame, tp+1, grid)
  obs4 <- known_all(frame, tp+2, grid)
  set.seed(4)
  rand_df <- obs2[sample(nrow(obs2), size=(round(0.9*nrow(obs2)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,5]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(N-1,30)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_all_grid(frame, tp, grid)
  est_df <- subset(predicted_locs,!predicted_locs$gpid%in%rand_df$gpid)
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  return(preds)
}


all_pred_noi <- data.frame(NULL)
pred_cv_noi <- data.frame(NULL)


#xmap
all_cv_x_noint <- function(df){
  for (g in min(df$t):max(df$t)){
  #for (g in 51:52){
      tryCatch({
        pred = xmap_10CV_all(g, df)
        pred_df = data.frame(pred,g)
        pred_cv_noi = rbind.data.frame(pred_cv_noi, pred_df)}, error=function(e){return(NA)})
  }
  return(pred_cv_noi)
}

#ymap
all_cv_y_noint <- function(df){
  for (g in min(df$t):max(df$t)){
    tryCatch({
      pred = ymap_10CV_all(g, df)
      pred_df = data.frame(pred,g)
      pred_cv_noi = rbind.data.frame(pred_cv_noi, pred_df)}, error=function(e){return(NA)})
  }
  return(pred_cv_noi)
}

## Get Error
cv_error_x_noi <- function(frame, pred_df){
  final <- filter(frame, imputed =="FALSE")[,c(1,2,5,6,9)]
  colnames(pred_df) <- c("gpid", "pred", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t"))
  all_data_distinct <- distinct(all_data)
  error <- sum((all_data_distinct$pred-all_data_distinct$xmap)^2)/nrow(all_data_distinct)
  return(error)
}

cv_error_y_noi <- function(frame, pred_df){
  final <- filter(frame, imputed =="FALSE")[,c(1,2,5,6,9)]
  colnames(pred_df) <- c("gpid", "pred", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t"))
  all_data_distinct <- distinct(all_data)
  error <- sum((all_data_distinct$pred-all_data_distinct$ymap)^2)/nrow(all_data_distinct)
  return(error)
}

#Gives Dataframe with actual and predicted
cv_df_noi <- function(frame, pred_df){
  final <- filter(frame, imputed =="FALSE")[,c(1,2,5,6,9)]
  colnames(pred_df) <- c("gpid", "pred", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t"))
  all_data_distinct <- distinct(all_data)
  return(all_data_distinct)
}

```


```{r week1-x-cv-noint, cache=TRUE, message=FALSE, warning = FALSE}

h1_x <- all_cv_x_noint(anim_plot_data1)

```


```{r week2-x-cv-noint, cache=TRUE, message=FALSE, warning = FALSE}

h2_x <- all_cv_x_noint(anim_plot_data2)

```

```{r week3-x-cv-noint, cache=TRUE, message=FALSE, warning = FALSE}

h3_x <- all_cv_x_noint(anim_plot_data3)

```

```{r w1-y-cv-noint, message=FALSE, warning = FALSE, cache=TRUE}

h1y <-all_cv_y_noint(anim_plot_data1)


```


```{r w2-y-cv-noint, message=FALSE, warning = FALSE ,cache=TRUE}


h2y <-all_cv_y_noint(anim_plot_data2)


```


```{r w3-y-cv-noint, message=FALSE, warning = FALSE, cache=TRUE}

h3y <-all_cv_y_noint(anim_plot_data3)


```



```{r no-int-error-actual, cache=TRUE, message=FALSE, warning = FALSE}
w1x <- cv_error_x_noi(anim_plot_data1,h1_x) #9.567 - 3.093
w2x <- cv_error_x_noi(anim_plot_data2,h2_x) #10.227 - 3.198
w3x <- cv_error_x_noi(anim_plot_data3,h3_x) #10.178 - 3.1903
w1y <- cv_error_y_noi(anim_plot_data1,h1y) #9.409 - 3.067
w2y <- cv_error_y_noi(anim_plot_data2,h2y) #9.928 - 3.151
w3y <- cv_error_y_noi(anim_plot_data3,h3y) # 9.924 - 3.150238

```



```{r results-table2, message=FALSE, warning = FALSE}

result_data2 <- data.frame(X1 = c(sqrt(e_w1_x), sqrt(e_w2_x), sqrt(e_w3_x)), Y1 = c(sqrt(e_w1_y), sqrt(e_w2_y), sqrt(e_w3_y)), X2 = c(sqrt(error_x_lin_actual_w1), sqrt(error_x_lin_actual_w2), sqrt(error_x_lin_actual_w3)), Y2 = c(sqrt(error_y_lin_actual_w1), sqrt(error_y_lin_actual_w2), sqrt(error_y_lin_actual_w3)), X3 = c(sqrt(w1x), sqrt(w2x), sqrt(w3x)), Y3 = c(sqrt(w1y), sqrt(w2y), sqrt(w3y)))
#
kableExtra::kable(result_data2, booktabs = TRUE, caption = "RMSE for Interpolation Methods",  col.names = c("$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{noint}$", "$Y_{noint}$"), escape = FALSE) %>% pack_rows(
  index = (c("Week 1" = 1, "Week 2" = 1, "Week 3" = 1))
) %>%
  add_header_above(c("Intersection" = 2, "Linear" = 2, "No Intersection" = 2))


```

Next, we tested our spatio-temporal interpolation method on the ice movement dataset in order to see how it performs on real data. We did not predict on the already missing data in the dataset for validation purposes. For cross validation, we can create a model for an intersection and time. In a the intersection and time combination, 10% of the known data were removed from the dataset. Next, we found all of the observed data in that intersection for t-1, t, and t+1 in order to obtain the spatio-temporal neighbors. Then, using the observed data, the model was fit and used to estimate the missing data points. This was then done for all intersection and time combinations for the three weeks. The total RMSE for each week can be found in \cref{tab:results-table2}. Additionally, linear interpolation and the model with no intersection was performed on the same missing data for comparison, like what was done for the simulated data. However, since linear interpolation is unable to estimate data without a point before or after (essentially the beginning or end of the week), it is not exactly a one-to-one comparison. Also, for the no intersection model, models were able to be created for each day for Week 2 and Week 3, but were only able to create a model for one day in Week 1 due to the log-likelihood failing to converge. So, the Week 1 RMSE value for this week is not useful for comparison. In \cref{tab:results-table2}, for the most part, linear interpolation looks to outperform the other methods, except for Y-values in Week 1. 

```{r cp_w1, message=FALSE, warning = FALSE, cache=TRUE}
sd_mean_w1x <- mean(e_w1_df_x$sd)
sd_mean_w1y <- mean(e_w1_df_y$sd)

e_w1_df_x$low2 <- e_w1_df_x$pred - (2*e_w1_df_x$sd)
e_w1_df_x$up2 <- e_w1_df_x$pred + (2*e_w1_df_x$sd)

e_w1_df_y$low2 <- e_w1_df_y$pred - (2*e_w1_df_y$sd)
e_w1_df_y$up2 <- e_w1_df_y$pred + (2*e_w1_df_y$sd)

e_w1_df_x$inint2 <- 0
e_w1_df_x$inint2[e_w1_df_x$low2 < e_w1_df_x$xmap & e_w1_df_x$xmap < e_w1_df_x$up2] <- 1

e_w1_df_y$inint2 <- 0
e_w1_df_y$inint2[e_w1_df_y$low2 < e_w1_df_y$ymap & e_w1_df_y$ymap < e_w1_df_y$up2] <- 1

w1_cp_x2 <- sum(e_w1_df_x$inint2)/nrow(e_w1_df_x) #coverage probability - 0.32
w1_cp_y2 <- sum(e_w1_df_y$inint2)/nrow(e_w1_df_y) #coverage probability - 0.344


```


```{r cp_w2, message=FALSE, warning = FALSE, cache=TRUE}

sd_mean_w2x <- mean(e_w2_df_x$sd)
sd_mean_w2y <- mean(e_w2_df_y$sd)

e_w2_df_x$low2 <- e_w2_df_x$pred - (2*e_w2_df_x$sd)
e_w2_df_x$up2 <- e_w2_df_x$pred + (2*e_w2_df_x$sd)

e_w2_df_y$low2 <- e_w2_df_y$pred - (2*e_w2_df_y$sd)
e_w2_df_y$up2 <- e_w2_df_y$pred + (2*e_w2_df_y$sd)

e_w2_df_x$inint2 <- 0
e_w2_df_x$inint2[e_w2_df_x$low2 < e_w2_df_x$xmap & e_w2_df_x$xmap < e_w2_df_x$up2] <- 1

e_w2_df_y$inint2 <- 0
e_w2_df_y$inint2[e_w2_df_y$low2 < e_w2_df_y$ymap & e_w2_df_y$ymap < e_w2_df_y$up2] <- 1

w2_cp_x2 <- sum(e_w2_df_x$inint2)/nrow(e_w2_df_x) #coverage probability - 0.267
w2_cp_y2 <- sum(e_w2_df_y$inint2)/nrow(e_w2_df_y) #coverage probability - 0.267


```


```{r cp_w3, message=FALSE, warning = FALSE, cache=TRUE}

sd_mean_w3x <- mean(e_w3_df_x$sd)
sd_mean_w3y <- mean(e_w3_df_y$sd)

e_w3_df_x$low2 <- e_w3_df_x$pred - (2*e_w3_df_x$sd)
e_w3_df_x$up2 <- e_w3_df_x$pred + (2*e_w3_df_x$sd)

e_w3_df_y$low2 <- e_w3_df_y$pred - (2*e_w3_df_y$sd)
e_w3_df_y$up2 <- e_w3_df_y$pred + (2*e_w3_df_y$sd)

e_w3_df_x$inint2 <- 0
e_w3_df_x$inint2[e_w3_df_x$low2 < e_w3_df_x$xmap & e_w3_df_x$xmap < e_w3_df_x$up2] <- 1

e_w3_df_y$inint2 <- 0
e_w3_df_y$inint2[e_w3_df_y$low2 < e_w3_df_y$ymap & e_w3_df_y$ymap < e_w3_df_y$up2] <- 1

w3_cp_x2 <- sum(e_w3_df_x$inint2)/nrow(e_w3_df_x) #coverage probability - 0.277
w3_cp_y2 <- sum(e_w3_df_y$inint2)/nrow(e_w3_df_y) #coverage probability - 0.283

```





However, when looking at the trajectory plot in \cref{fig:trajectories}, can see that some of the trajectories in the plot are more linear than others (ie. have different movements defined by the clusters). Based on our simulation study, our method may have some improvement over linear interpolation on curved data and less-sampled trajectories. Thus, the RMSE values are additionally broken out by cluster, as these are the groups of movement patterns in the data. These values can be found for each week in \cref{tab:results-clust1-actual}, \cref{tab:results-clust2-actual}, and \cref{tab:results-clust3-actual}. Once again, linear interpolation generally outperforms our method, but there are two exceptions. For Week 1 and Week 2, linear interpolation performed the worst of the other two methods for Cluster 1. In \cref{fig:by-week-cluster-plot}, Cluster 1 for both of these weeks is at the top of the plot. When comparing this cluster location to the trajectory plot in \cref{fig:trajectories}, the top of the figure looks to be where the most curved movement happens with longer distance traveled over time. So linear interpolation may not work as well here. Further, \cref{fig:int-best-plots} and \cref{fig:lin-best-plots}, zoom in on examples where our intersection model method performs best and linear interpolation performs the best respectively. When comparing both figures, our method seems to perform better on longer trajectories, as observed points are more spread out along the trajectory. 

As previously stated, though the RMSE for the non-intersection model was lower than the intersection model in week 1, this number is only representing one day's worth of data. So for Cluster 1 in week 1, our proposed method seems to be performing the best. For week 2 cluster 1, there mixed results between the two model methods, as one performs better for x and the other for y. When comparing the two model-based approaches, find mixed results when compare their performance, but the intersection model performs better more often. These results, once again, shows that our method has some promise with non-linear and less-sampled data. 




```{r actual-by-clust-int, message=FALSE, warning = FALSE}

c_w1_x <- e_w1_df_x %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w1_y <- e_w1_df_y %>% group_by(clust) %>% summarise(y = sqrt((sum((pred-ymap)^2)/n())))
c_w2_x <- e_w2_df_x %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w2_y <- e_w2_df_y %>% group_by(clust) %>% summarise(y = sqrt((sum((pred-ymap)^2)/n())))
c_w3_x <- e_w3_df_x %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w3_y <- e_w3_df_y %>% group_by(clust) %>% summarise(y = sqrt((sum((pred-ymap)^2)/n())))


```

```{r cp-by-clust-int, message=FALSE, warning = FALSE, cache=TRUE}

c_w1_x2 <- e_w1_df_x %>% group_by(clust) %>% summarise(cp = sum((inint2)/n()))
c_w1_y2 <- e_w1_df_y %>% group_by(clust) %>% summarise(cp = sum((inint2)/n()))
c_w2_x2 <- e_w2_df_x %>% group_by(clust) %>% summarise(cp = sum((inint2)/n()))
c_w2_y2 <- e_w2_df_y %>% group_by(clust) %>% summarise(cp = sum((inint2)/n()))
c_w3_x2 <- e_w3_df_x %>% group_by(clust) %>% summarise(cp = sum((inint2)/n()))
c_w3_y2 <- e_w3_df_y %>% group_by(clust) %>% summarise(cp = sum((inint2)/n()))

```


```{r lin-actual-by-clust, message=FALSE, warning = FALSE}
lin_w1_x <- miss_na1 %>% group_by(cluster) %>% summarise(x = sqrt((sum((xmap_pred-xmap2)^2)/n())))
lin_w1_y <- miss_na1 %>% group_by(cluster) %>% summarise(y = sqrt((sum((ymap_pred-ymap2)^2)/n())))
lin_w2_x <- miss_na2 %>% group_by(cluster) %>% summarise(x = sqrt((sum((xmap_pred-xmap2)^2)/n())))
lin_w2_y <- miss_na2 %>% group_by(cluster) %>% summarise(y = sqrt((sum((ymap_pred-ymap2)^2)/n())))
lin_w3_x <- miss_na3 %>% group_by(cluster) %>% summarise(x = sqrt((sum((xmap_pred-xmap2)^2)/n())))
lin_w3_y <- miss_na3 %>% group_by(cluster) %>% summarise(y = sqrt((sum((ymap_pred-ymap2)^2)/n())))

```


```{r noint-actual-by-clust, message=FALSE, warning=FALSE}
w1x_df <- cv_df_noi(anim_plot_data1, h1_x)
w2x_df <- cv_df_noi(anim_plot_data2, h2_x)
w3x_df <- cv_df_noi(anim_plot_data3, h3_x)

w1y_df <- cv_df_noi(anim_plot_data1, h1y)
w2y_df <- cv_df_noi(anim_plot_data2, h2y)
w3y_df <- cv_df_noi(anim_plot_data3, h3y)


c_w1_x_noi <- w1x_df %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w2_x_noi <- w2x_df %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w3_x_noi <- w3x_df %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))


c_w1_y_noi <- w1y_df %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-ymap)^2)/n())))
c_w2_y_noi <- w2y_df %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-ymap)^2)/n())))
c_w3_y_noi <- w3y_df %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-ymap)^2)/n())))


```


```{r results-clust1-actual, message=FALSE, warning = FALSE}

result_cluster1_ice <- data.frame(Cluster = c(1,2,3,4,5,6), X1 = c(c_w1_x2[,2]), Y1 = c(c_w1_y2[,2]), X2 <- c(lin_w1_x[,2]), Y2 <- c(lin_w1_y[,2]), X3 = c(c_w1_x_noi[,2]), Y3 = c(c_w1_y_noi[,2]))

#
kableExtra::kable(result_cluster1_ice, booktabs = TRUE, caption = "RMSE by Cluster for Week 1 for Interpolation Methods", col.names = c("Cluster", "$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{noint}$", "$Y_{noint}$"), escape = FALSE) %>%
  add_header_above(c(" ", "Intersection" = 2, "Linear" = 2, "No Intersection" = 2))



```


```{r results-clust2-actual, message=FALSE, warning = FALSE}

result_cluster2_ice <- data.frame(Cluster = c(1,2,3,4,5), X1 = c(c_w2_x[,2]), Y1 = c(c_w2_y[,2]), X2 <- c(lin_w2_x[,2]), Y2 <- c(lin_w2_y[,2]), X3 = c(c_w2_x_noi[,2]), Y3 = c(c_w2_y_noi[,2]))
#
kableExtra::kable(result_cluster2_ice, booktabs = TRUE, caption = "RMSE by Cluster for Week 2 for Interpolation Methods", col.names = c("Cluster", "$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{noint}$", "$Y_{noint}$"), escape = FALSE) %>%
  add_header_above(c(" ", "Intersection" = 2, "Linear" = 2, "No Intersection" = 2))


```


```{r results-clust3-actual, message=FALSE, warning = FALSE, cache=TRUE}

result_cluster3_ice <- data.frame(Cluster = c(1,2,3,4,5,6), X1 = c(c_w3_x[,2]), Y1 = c(c_w3_y[,2]), X2 <- c(lin_w3_x[,2]), Y2 <- c(lin_w3_y[,2]), X3 = c(c_w3_x_noi[,2]), Y3 = c(c_w3_y_noi[,2]))
#
kableExtra::kable(result_cluster3_ice, booktabs = TRUE, caption = "RMSE by Cluster for Week 3 for Interpolation Methods", col.names = c("Cluster", "$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{noint}$", "$Y_{noint}$"), escape = FALSE) %>%
  add_header_above(c(" ", "Intersection" = 2, "Linear" = 2, "No Intersection" = 2))


```


The proportion of the prediction intervals containing the observed data, along with the average standard deviations are given for the ice data in \cref{tab:cp-table}. These values were found in the same way as the simulated data. The standard deviations are higher than what was seen with the simulated data, but the proportions are similar (ranging from 25%-31%). Once again, this can be broken down by cluster in \cref{tab:cp-clust-actual}. However, none of the large proportions are as large as would b desired, but in cluster 1, week 1 the proportion for X is above 50%, with the proportion for Y close to 50%. 

```{r cp-table, message=FALSE, warning = FALSE}

cp_data <- data.frame(week = c("1", "2", "3"), cov_probx = c(w1_cp_x2, w2_cp_x2, w3_cp_x2), sdx = c(sd_mean_w1x, sd_mean_w2x, sd_mean_w3x), cov_proby = c(w1_cp_y2, w2_cp_y2, w3_cp_y2), sdx = c(sd_mean_w1y, sd_mean_w2y, sd_mean_w3y))

kableExtra::kable(cp_data, booktabs = TRUE, col.names = c("Week", "Proportion of X", "Avg SD of X", "Proportion of Y", "Avg SD of Y"), caption = "Proportion of Prediction interval containing observed and Average Standard Deviation for Ice Data")


```



```{r cp-clust-actual, message=FALSE, warning = FALSE, cache=TRUE}

cp_by_clust_act <- data.frame(Cluster = c(1,2,3,4,5,6), w1x = c(round(c_w1_x2[,2],4)), w2x = rbind(round(c_w2_x2[,2],4), "NA"), w3x = c(round(c_w3_x2[,2],4)), w1y = c(round(c_w1_y2[,2],4)), w2y = rbind(round(c_w2_y2[,2],4), NA), w3y = c(round(c_w3_y2[,2],4)))
#
kableExtra::kable(cp_by_clust_act , booktabs = TRUE, caption = "Proportion of Prediction interval containing observed by Cluster for Ice Data", col.names = c("Cluster", "$W1_{x}$", "$W2_{x}$", "$W3_{x}$", "$W1_{y}$", "$W2_{y}$", "$W3_{y}$"), escape = FALSE)


```

<!----------------- Cross Validation Along Borders ----------------------->

```{r border-cv-functions}
##-------------------- Functions Needed -------------------------##

known_data_border <- function(m, int, tp, grid, frame){ #Grid doesn't really matter as much (only when doing cross validation)
  anim_p <- filter(frame, t == tp)[,c(1,2,4,5,6,9)]
  anim_p <- anim_p %>% group_by(gpid) %>% summarise(xmap_m = mean(xmap),ymap_m = mean(ymap),clust_m = mean(as.numeric(clust))) #Averaging to deal with mult observations at a t value
  colnames(anim_p) <- c("gpid", "xmap", "ymap", "clust")
  n = n_distinct(anim_p$gpid)
  test=get.knn(anim_p[,c(2,3)], k=10)
  df_new<- rownames_to_column(anim_p, "index")
  df_new <- data.frame(df_new,test[1])
  df_new1<- vlookup(df_new$nn.index.1,df_new,5,1)
  df_new2<- vlookup(df_new$nn.index.2,df_new,5,1)
  df_new3<- vlookup(df_new$nn.index.3,df_new,5,1)
  df_new4<- vlookup(df_new$nn.index.4,df_new,5,1)
  df_new5<- vlookup(df_new$nn.index.5,df_new,5,1)
  df_new6<- vlookup(df_new$nn.index.6,df_new,5,1)
  df_new7<- vlookup(df_new$nn.index.7,df_new,5,1)
  df_new8<- vlookup(df_new$nn.index.8,df_new,5,1)
  df_new9<- vlookup(df_new$nn.index.9,df_new,5,1)
  df_new10<- vlookup(df_new$nn.index.10,df_new,5,1)
  df_new <- data.frame(df_new, df_new1, df_new2, df_new3, df_new4, df_new5, df_new6, df_new7, df_new8, df_new9, df_new10)
  for (i in 1:n){
    x = quantile(as.matrix(df_new[i,16:25]))[2]
    df_new$border[i] <- "no"
    df_new$border[i][x != df_new$clust[i]] <- "yes"
  }
  df_new <- df_new[,-c(6:25, 27:36)]
  

  int1 <- filter(m, intersection == int)[,-c(2,3)]
  int_t1 <- right_join(grid[,-5], int1, by = "gpid")
  anim_t1 <- filter(frame, t == tp)
  int_t1_2 <- inner_join(int_t1, anim_t1, by = "gpid")
  final_int_t1 <- int_t1_2[,c(2,4,5,6,9,10, 13,14,17,18)]
  t1_notimputed =  filter(final_int_t1, imputed == "FALSE")[,c(1,2,5,6,7,8,9)]
  colnames(t1_notimputed) <- c("gpid", "gpid_id", "intersection", "obs_time", "xmap","ymap", "clust")
  t1_notimputed <- t1_notimputed[complete.cases(t1_notimputed),]
  h <- inner_join(t1_notimputed, df_new[,c(2,6)], by = "gpid")
  return(h)
}

xmap_10CV_pred_border <- function(m_new, m, int, tp, frame){
  grid <- grid_tp(frame, tp , m)
  obs1 <- known_data_border(m_new,int,tp-1,grid, frame)
  obs2 <- known_data_border(m_new,int,tp,grid, frame)
  obs3 <- known_data_border(m_new,int,tp+1,grid, frame)
  obs4 <- known_data_border(m_new,int,tp+2,grid, frame)
  bord_df <- filter(obs2, border == "no")
  set.seed(4)
  rand_df <- bord_df[sample(nrow(bord_df), size=(round(0.9*nrow(bord_df)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,5]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_data_grid(m_new,int,tp,grid, frame)
  est_df <- subset(predicted_locs,!(predicted_locs$gpid%in%rand_df$gpid))
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ncondsim <- 30
  sims <- cond_sim(fit = fit, locs_pred = pred_loc, X_pred = X_pred, covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                   ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE, nsims = ncondsim)
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  b <- list(sims,preds)
  return(b)
}

ymap_10CV_pred_border <- function(m_new, m, int, tp, frame){
  grid <- grid_tp(frame, tp , m)
  obs1 <- known_data_border(m_new,int,tp-1,grid, frame)
  obs2 <- known_data_border(m_new,int,tp,grid, frame)
  obs3 <- known_data_border(m_new,int,tp+1,grid, frame)
  obs4 <- known_data_border(m_new,int,tp+2,grid, frame)
  bord_df <- filter(obs2, border == "no")
  set.seed(4)
  rand_df <- bord_df[sample(nrow(bord_df), size=(round(0.9*nrow(bord_df)))),]
  if(nrow(obs3) ==0){obs <- rbind(obs1, rand_df, obs4)}else{obs <- rbind(obs1, rand_df, obs3)}
  loc <- obs[,c("xmap","ymap", "obs_time")]
  locs<-as.matrix(loc)
  N <- nrow(locs)
  ## convert them into matrix
  X<- as.matrix( rep(1,nrow(locs)))
  Y<- obs[,6]
  fit <- fit_model(Y,locs, X, "exponential_spacetime", max_iter = 1000, convtol = 1e-04,reorder = TRUE, m_seq = c(10,min(30,N-1)), silent = TRUE) #Silent option doesn't show the iterations. Maybe nice so get list of errors
  predicted_locs <- known_data_grid(m_new,int,tp,grid, frame)
  est_df <- subset(predicted_locs,!(predicted_locs$gpid%in%rand_df$gpid))
  pred_loc <- as.matrix(est_df[ ,c("xmap","ymap", "obs_time")])
  X_pred<- as.matrix( rep(1,nrow(pred_loc)))
  ncondsim <- 30
  sims <- cond_sim(fit = fit, locs_pred = pred_loc, X_pred = X_pred, covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                   ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE, nsims = ncondsim)
  ## for prediction
  pred_grid<-predictions(fit,pred_loc, X_pred,covparms = fit$covparms, covfun_name = fit$covfun_name, y_obs = fit$y,locs_obs = fit$locs
                         ,X_obs= fit$X, beta=fit$betahat, m = 10, reorder = TRUE)
  preds <- data.frame(est_df$gpid,pred_grid)
  b <- list(sims,preds)
  return(b)
}

all_cv_x_pred_border <- function(int_df,int_list,df){
  for (p in min(df$t):max(df$t)){
    for (q in 1:max(int_df$intersection)){
  # for (p in 66:67){
  #   for (q in 1:6){
      tryCatch({
        pred = xmap_10CV_pred_border(int_df, int_list, q, p, df)
        sd_df <- sd_pred(pred[[1]],pred[[2]])
        df_p = data.frame(sd_df, q, p)
        pred_cv = rbind.data.frame(pred_cv, df_p)}, error=function(e){return(NA)})
    }
    #all_pred <- rbind.data.frame(all_pred,pred_cv)
  }
  all_pred <- rbind.data.frame(all_pred,pred_cv)
  return(all_pred)
}

all_cv_y_pred_border <- function(int_df,int_list,df){
  for (p in min(df$t):max(df$t)){
    for (q in 1:max(int_df$intersection)){
  # for (p in 66:67){
  #   for (q in 1:6){
      tryCatch({
        pred = ymap_10CV_pred_border(int_df, int_list, q, p, df)
        sd_df <- sd_pred(pred[[1]],pred[[2]])
        df_p = data.frame(sd_df, q, p)
        pred_cv = rbind.data.frame(pred_cv, df_p)}, error=function(e){return(NA)})
    }
    #all_pred <- rbind.data.frame(all_pred,pred_cv)
  }
  all_pred <- rbind.data.frame(all_pred,pred_cv)
  return(all_pred)
}


cv_error_x_p <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed =="FALSE")[,c(1,2,3,6,7)]
  colnames(pred_df) <- c("gpid", "pred", "sd", "low", "up", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data, gpid, t, .keep_all = TRUE)
  error <- sum((all_data_distinct$pred-all_data_distinct$xmap)^2)/nrow(all_data_distinct)
  return(error)
}

cv_error_y_p <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed =="FALSE")[,c(1,2,3,6,7)]
  colnames(pred_df) <- c("gpid", "pred", "sd", "low", "up", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data, gpid, t, .keep_all = TRUE)
  error <- sum((all_data_distinct$pred-all_data_distinct$ymap)^2)/nrow(all_data_distinct)
  return(error)
}

cv_df_p <- function(m_df,frame, pred_df){
  int1 <- m_df[,-c(2:4)]
  int_t1_2 <- right_join(int1, frame, by = "gpid")
  final <- filter(int_t1_2, imputed =="FALSE")[,c(1,2,3,6,7,10)]
  colnames(pred_df) <- c("gpid", "pred", "sd", "low", "up", "intersection", "t")
  pred_df$pred <- round(pred_df$pred, 3)
  all_data <- left_join(pred_df,final, by = c("gpid", "t", "intersection"))
  all_data_distinct <- distinct(all_data, gpid, t, .keep_all = TRUE)
  return(all_data_distinct)
}

```


```{r pred-week1x-cv, cache=TRUE, message=FALSE, warning = FALSE}
w1_xcvb <- all_cv_x_pred_border(m1,m3,anim_plot_data1)

```

```{r pred-week2x-cv, cache=TRUE, message=FALSE, warning = FALSE}
w2_xcvb <- all_cv_x_pred_border(m5,m6,anim_plot_data2)

```

```{r pred-week3x-cv, cache=TRUE, message=FALSE, warning = FALSE}
w3_xcvb <- all_cv_x_pred_border(m2,m4,anim_plot_data3)

```

```{r pred-week1y-cv, cache=TRUE, message=FALSE, warning = FALSE}
w1_ycvb <- all_cv_y_pred_border(m1,m3,anim_plot_data1)
```

```{r pred-week2y-cv, cache=TRUE, message=FALSE, warning = FALSE}
w2_ycvb <- all_cv_y_pred_border(m5,m6,anim_plot_data2)
```

```{r pred-week3y-cv, cache=TRUE, message=FALSE, warning = FALSE}
w3_ycvb <- all_cv_y_pred_border(m2,m4,anim_plot_data3)
```

```{r error-border-cv, cache=TRUE, message=FALSE, warning = FALSE}
e_w1_x <- cv_error_x_p(m1, anim_plot_data1, w1_xcvb) #10.338
e_w1_y <- cv_error_y_p(m1, anim_plot_data1, w1_ycvb) #11.613

e_w2_x <- cv_error_x_p(m5, anim_plot_data2, w2_xcvb) #12.526
e_w2_y <- cv_error_y_p(m5, anim_plot_data2, w2_ycvb) #10.599

e_w3_x <- cv_error_x_p(m2, anim_plot_data3, w3_xcvb) #9.19
e_w3_y <- cv_error_y_p(m2, anim_plot_data3, w3_ycvb) #8.749
```

```{r lin-border-w1, cache=TRUE, message=FALSE, warning = FALSE}

e_w1_x <- cv_error_x_p(m1, anim_plot_data1, w1_xcvb) #10.338
e_w1_y <- cv_error_y_p(m1, anim_plot_data1, w1_ycvb) #11.613

anim_plot_data <- tmp %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

anim_plot_data$missing <- "no"
anim_plot_data$missing[is.na(anim_plot_data$xmap)] <- "yes"



e_w1_df_x <- cv_df_p(m1, anim_plot_data1,w1_xcvb)
e_w1_df_y <- cv_df_p(m1, anim_plot_data1,w1_ycvb)


#Need to think this through a bit

e_w1_df =  inner_join(e_w1_df_x, e_w1_df_y, by= c("gpid", "t"))[c(1:14)]

anim_plot <- filter(anim_plot_data, t < 58)

#combine cv_data sets to get actual data
cv_data <- e_w1_df[c(1,7,8,9,10)]
colnames(cv_data) <- c("gpid", "t","xmap2", "ymap2", "cluster")

## need to get linear interpolations on cross validated data
join1 <- full_join(cv_data, anim_plot, by = c("gpid", "t")) #df with og data and cv data set
join_nocv <- filter(join1, is.na(xmap2)) #data not used in cv
join_cv <- filter(join1, !is.na(xmap2)) #data used in cv

join_cv$xmap<- NA #want data used in cv to be miss
join_cv$ymap<- NA

back_toget <- rbind(join_nocv, join_cv) #bring them back together so now have missing cv data in the dataset

#but in order
back_toget <- back_toget %>%
  arrange(gpid, t)

back_toget <- back_toget[-c(3,4,6,7,10,11)] #remove unnecessary columns

#second missing variable to help denote if cv data or not
back_toget$missing2 <- "no"
back_toget$missing2[is.na(back_toget$xmap)] <- "yes"

#linear interpolation
lin <- back_toget %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

#get data used in cv (not origingally missing, but missing now)
miss <- filter(lin, missing == "no" & missing2 == "yes")
colnames(miss) <- c("gpid", "t", "cluster" , "xmap_pred", "ymap_pred","imputed","missing", "missing2")
miss_join <- left_join(cv_data, miss[,-3], by = c("gpid", "t"))#combine with actual values dataset

miss_na <- filter(miss_join, !is.na(xmap_pred))#remove NAs if first day of week
miss_na <- miss_na %>% arrange(gpid,t)
miss_na1 <- miss_na %>% distinct(gpid,t, .keep_all= TRUE) #Remove duplicated data

#MSE
error_x_lin_actual_w1 <- sum((miss_na1$xmap2-miss_na1$xmap_pred)^2)/nrow(miss_na1) #2.375
error_y_lin_actual_w1 <- sum((miss_na1$ymap2-miss_na1$ymap_pred)^2)/nrow(miss_na1) #20.199

```


```{r lin-border-w2, cache=TRUE, message=FALSE, warning = FALSE}

e_w2_x <- cv_error_x_p(m5, anim_plot_data2, w2_xcvb) #12.526
e_w2_y <- cv_error_y_p(m5, anim_plot_data2, w2_ycvb) #10.599

anim_plot_data <- tmp %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

anim_plot_data$missing <- "no"
anim_plot_data$missing[is.na(anim_plot_data$xmap)] <- "yes"



e_w2_df_x <- cv_df_p(m5, anim_plot_data2,w2_xcvb)
e_w2_df_y <- cv_df_p(m5, anim_plot_data2,w2_ycvb)


#Need to think this through a bit

e_w2_df =  inner_join(e_w2_df_x, e_w2_df_y, by= c("gpid", "t"))[c(1:14)]

anim_plot <- filter(anim_plot_data, t > 57 & t < 65)

#combine cv_data sets to get actual data
cv_data <- e_w2_df[c(1,7,8,9,10)]
colnames(cv_data) <- c("gpid", "t","xmap2", "ymap2", "cluster")

## need to get linear interpolations on cross validated data
join1 <- full_join(cv_data, anim_plot, by = c("gpid", "t")) #df with og data and cv data set
join_nocv <- filter(join1, is.na(xmap2)) #data not used in cv
join_cv <- filter(join1, !is.na(xmap2)) #data used in cv

join_cv$xmap<- NA #want data used in cv to be miss
join_cv$ymap<- NA

back_toget <- rbind(join_nocv, join_cv) #bring them back together so now have missing cv data in the dataset

#but in order
back_toget <- back_toget %>%
  arrange(gpid, t)

back_toget <- back_toget[-c(3,4,6,7,10,11)] #remove unnecessary columns

#second missing variable to help denote if cv data or not
back_toget$missing2 <- "no"
back_toget$missing2[is.na(back_toget$xmap)] <- "yes"

#linear interpolation
lin <- back_toget %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

#get data used in cv (not origingally missing, but missing now)
miss <- filter(lin, missing == "no" & missing2 == "yes")
colnames(miss) <- c("gpid", "t", "cluster" , "xmap_pred", "ymap_pred","imputed","missing", "missing2")
miss_join <- left_join(cv_data, miss[,-3], by = c("gpid", "t")) #combine with actual values dataset

miss_na <- filter(miss_join, !is.na(xmap_pred)) #remove NAs if first day of week
miss_na <- miss_na %>% arrange(gpid,t)
miss_na2 <- miss_na %>% distinct(gpid,t, .keep_all= TRUE) #Remove duplicated data

#MSE
error_x_lin_actual_w2 <- sum((miss_na2$xmap2-miss_na2$xmap_pred)^2)/nrow(miss_na2) #4.157
error_y_lin_actual_w2 <- sum((miss_na2$ymap2-miss_na2$ymap_pred)^2)/nrow(miss_na2) #3.357
```


```{r lin-border-w3, cache=TRUE, message=FALSE, warning = FALSE}
e_w3_x <- cv_error_x_p(m2, anim_plot_data3, w3_xcvb) #9.19
e_w3_y <- cv_error_y_p(m2, anim_plot_data3, w3_ycvb) #8.749

anim_plot_data <- tmp %>%
  mutate(imputed = F) %>%
  complete(crossing(gpid, t), fill = list(imputed = T)) %>%
  ungroup() %>%
  arrange(gpid, t) %>%
  group_by(gpid) %>%
  arrange(gpid)

anim_plot_data$missing <- "no"
anim_plot_data$missing[is.na(anim_plot_data$xmap)] <- "yes"



e_w3_df_x <- cv_df_p(m2, anim_plot_data3,w3_xcvb)
e_w3_df_y <- cv_df_p(m2, anim_plot_data3,w3_ycvb)


#Need to think this through a bit

e_w3_df =  inner_join(e_w3_df_x, e_w3_df_y, by= c("gpid", "t"))[c(1:14)]

anim_plot <- filter(anim_plot_data, t > 64 & t < 73)

#combine cv_data sets to get actual data
cv_data <- e_w3_df[c(1,7,8,9,10)]
colnames(cv_data) <- c("gpid", "t","xmap2", "ymap2", "cluster")

## need to get linear interpolations on cross validated data
join1 <- full_join(cv_data, anim_plot, by = c("gpid", "t")) #df with og data and cv data set
join_nocv <- filter(join1, is.na(xmap2)) #data not used in cv
join_cv <- filter(join1, !is.na(xmap2)) #data used in cv

join_cv$xmap<- NA #want data used in cv to be miss
join_cv$ymap<- NA

back_toget <- rbind(join_nocv, join_cv) #bring them back together so now have missing cv data in the dataset

#but in order
back_toget <- back_toget %>%
  arrange(gpid, t)

back_toget <- back_toget[-c(3,4,6,7,10,11)]  #remove unnecessary columns

#second missing variable to help denote if cv data or not
back_toget$missing2 <- "no"
back_toget$missing2[is.na(back_toget$xmap)] <- "yes"

#linear interpolation
lin <- back_toget %>% group_by(gpid) %>%
  mutate(xmap =na.approx(xmap, na.rm = FALSE)) %>%
  mutate(ymap =na.approx(ymap, na.rm = FALSE))

#get data used in cv (not origingally missing, but missing now)
miss <- filter(lin, missing == "no" & missing2 == "yes")
colnames(miss) <- c("gpid", "t", "cluster" , "xmap_pred", "ymap_pred","imputed","missing", "missing2")
miss_join <- left_join(cv_data, miss[,-3], by = c("gpid", "t")) #combine with actual values dataset

miss_na <- filter(miss_join, !is.na(xmap_pred)) #remove NAs if first day of week
miss_na <- miss_na %>% arrange(gpid,t)
miss_na3 <- miss_na %>% distinct(gpid,t, .keep_all= TRUE) #Remove duplicated data

#MSE
error_x_lin_actual_w3 <- sum((miss_na3$xmap2-miss_na3$xmap_pred)^2)/nrow(miss_na3) #0.98
error_y_lin_actual_w3 <- sum((miss_na3$ymap2-miss_na3$ymap_pred)^2)/nrow(miss_na3) #1.54
```

```{r border-clust}
################ By Cluster #####################

c_w1_xb <- e_w1_df_x %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w1_yb <- e_w1_df_y %>% group_by(clust) %>% summarise(y = sqrt((sum((pred-ymap)^2)/n())))
c_w2_xb <- e_w2_df_x %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w2_yb <- e_w2_df_y %>% group_by(clust) %>% summarise(y = sqrt((sum((pred-ymap)^2)/n())))
c_w3_xb <- e_w3_df_x %>% group_by(clust) %>% summarise(x = sqrt((sum((pred-xmap)^2)/n())))
c_w3_yb <- e_w3_df_y %>% group_by(clust) %>% summarise(y = sqrt((sum((pred-ymap)^2)/n())))


lin_w1_x <- miss_na1 %>% group_by(cluster) %>% summarise(x = sqrt((sum((xmap_pred-xmap2)^2)/n())))
lin_w1_y <- miss_na1 %>% group_by(cluster) %>% summarise(y = sqrt((sum((ymap_pred-ymap2)^2)/n())))
lin_w2_x <- miss_na2 %>% group_by(cluster) %>% summarise(x = sqrt((sum((xmap_pred-xmap2)^2)/n())))
lin_w2_y <- miss_na2 %>% group_by(cluster) %>% summarise(y = sqrt((sum((ymap_pred-ymap2)^2)/n())))
lin_w3_x <- miss_na3 %>% group_by(cluster) %>% summarise(x = sqrt((sum((xmap_pred-xmap2)^2)/n())))
lin_w3_y <- miss_na3 %>% group_by(cluster) %>% summarise(y = sqrt((sum((ymap_pred-ymap2)^2)/n())))

```



Next, instead of a random hold-out of all of the data, we took a random hold-out of points along cluster borders. We are interested in points along the cluster border as those might be locations of irregular movement. Boundaries are defined in the code as points where some of its neighbors are from a different cluster. Otherwise the process remains the same as before. \cref{tab:border-cv-rmse} contains the RMSE values for our method and linear interpolation. The model with all the data (no intersection) is not used for comparison here as these models produced an error with issues in convergence of the log-likelihood. When just look at the weeks overall, our model only outperforms linear interpolation in Week 1 for y-values. Furthermore, this was broken out by cluster in \cref{tab:border-clust}. This shows similar results as the random hold-out, with our method outperforming linear interpolation in cluster 1 for Week 1 and 2. Additionally, in Week 1, our method performs best for y-values for all clusters expect for cluster 2. Once again, it should be noted that linear interpolation cannot estimate points if nothing before or after it, so we are working with less data in the RMSE calculation.

As in prior sections, 30 conditional simulations were performed in order to calculate the uncertainty. Using the standard deviation found from these simulations we calculated an interval. Then the proportion of intervals that contain the true value are calculated (\cref{tab:cp-table-border}). The average standard deviation values are also given to give an idea of typical interval width. Once again, as in \cref{tab:cp-table}, these values are low, with most intervals not containing the true value. In \cref{tab:cp-table-border-clust}, these values are broken out by cluster. The values do seem slightly higher in the instances where our method performs best, but still low. Finally, in \cref{tab:cp-table-border-ou}, found if the true values tend to be over or under the interval. Our interval is an underestimate of the true values, but still a high percentage of our interval is still an overestimate. 

```{r border-cv-rmse, message=FALSE, warning = FALSE, cache=TRUE}

ice_border <- data.frame(X1 = c(sqrt(e_w1_x), sqrt(e_w2_x), sqrt(e_w3_x)), Y1 = c(sqrt(e_w1_y), sqrt(e_w2_y), sqrt(e_w3_y)), X2 = c(sqrt(error_x_lin_actual_w1), sqrt(error_x_lin_actual_w2), sqrt(error_x_lin_actual_w3)), Y2 = c(sqrt(error_y_lin_actual_w1), sqrt(error_y_lin_actual_w2), sqrt(error_y_lin_actual_w3)))
#
kableExtra::kable(ice_border, booktabs = TRUE, caption = "RMSE for Interpolation Methods on Cluster Border Points",  col.names = c("$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$"), escape = FALSE) %>% pack_rows(
  index = (c("Week 1" = 1, "Week 2" = 1, "Week 3" = 1))
) %>% add_header_above(c("Intersection" = 2, "Linear" = 2))
```


```{r border-clust-tab, message=FALSE, warning = FALSE, cache = TRUE}

ice_border_clust <- data.frame(Cluster = c(1,2,3,4,5,6), X1 = round(c_w1_xb[,2],3), Y1 = round(c_w1_yb[,2],3), X2 <- round(lin_w1_x[,2],3), Y2 <- round(lin_w1_y[,2],3), X3 = round(c_w2_xb[,2],3) %>% add_row(x = NA), Y3 = round(c_w2_yb[,2],3) %>% add_row(y = NA), X4 = round(lin_w2_x[,2],3) %>% add_row(x = NA), Y4 = round(lin_w2_y[,2],3) %>% add_row(y = NA), X5 = round(c_w3_xb[,2],3), Y5 = round(c_w3_yb[,2],3), X6 = round(lin_w3_x[,2],3), Y6 = round(lin_w3_y[,2],3))
#
kableExtra::kable(ice_border_clust, booktabs = TRUE, caption = "RMSE by Cluster for Cross Validation Along Cluster Borders", col.names = c("Cluster", "$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$", "$X_{int}$", "$Y_{int}$", "$X_{lin}$", "$Y_{lin}$"), escape = FALSE) %>%
  add_header_above(c(" ", "Week 1" = 4, "Week 2" = 4, "Week 3" = 4))


```

```{r cp-table-border, message=FALSE, warning = FALSE, cache=TRUE}

cp_data_border <- data.frame(week = c("1", "2", "3"), cov_probx = c(0.32, 0.267, 0.277), sdx = c(0.942, 0.808, 0.775), cov_proby = c(0.344, 0.267, 0.283), sdy = c(1.069, 0.871, 0.785))

kableExtra::kable(cp_data_border, booktabs = TRUE, col.names = c("Week", "Proportion of X", "Avg SD of X", "Proportion of Y", "Avg SD of Y"), caption = "Proportion of Prediction interval containing observed and Average Standard Deviation for Ice Data") 
```


```{r cp-table-border-clust, message=FALSE, warning=FALSE, cache=TRUE}

cp_by_clust_sim <- data.frame(Cluster = c(1,2,3,4,5,6), w1x = c(0.592, 0.27, 0.347, 0.482, 0.157, 0.273), w1y = c(0.592, 0.288, 0.312, 0.301, 0.3, 0.396), w2x = c(0.467, 0.28, 0.337, 0.26, 0.149, " "), w2y = c(0.413, 0.278, 0.351, 0.251, 0.163, " "), w3x = c(0.422, 0.471, 0.184, 0.491, 0.283, 0.177),  w3y = c(0.357, 0.418, 0.245, 0.309, 0.264, 0.254))
#
kableExtra::kable(cp_by_clust_sim , booktabs = TRUE, caption = "Proportion of Prediction interval containing observed by Cluster for Cluster Border", col.names = c("Cluster", "$X_{s1}$", "$Y_{s1}$", "$X_{s2}$", "$Y_{s2}$", "$X_{s3}$", "$Y_{s3}$"), escape = FALSE) %>% 
  add_header_above(c(" ", "Week 1" = 2, "Week 2" = 2, "Week 3" = 2))

```



# Discussion {#discussion}


Our Bounding Box clustering method was developed to group trajectories by their movement, and to compensate for missing data. This method was shown through simulations and actual data that the boundaries of the clusters determined by the trajectory movements provide a reasonable estimation of the location of cracks. However, a major drawback of this method is that since the k-means clustering approach is used, our method relies on a pre-defined number of clusters, which is typically unknown. Furthermore, with having a set number of clusters, there is a limit on how many cluster boundaries representing cracks that can be identified. Thus, not all possible leads will be identified. Our method seems suggest that other approaches to clustering may be useful to extend this method in order to identify higher resolution cracks. Also, our method provides information about how the ice sheet is moving, which may also be important when developing climate models.  

Through our simulations and the use of the ice movement data, we have demonstrated some situations where our interpolation method may be beneficial. First, it takes into account the nonstationarity of the data, which generally shows slight improvement over using all the data to develop the model. Instances where the intersection model under performs the other model may be due to lack of data to develop a better model at that intersection. Secondly, it showed improvements, in terms of RMSE, over linear interpolation for curved and not highly sampled data with the ice data. Additionally, our method is able to estimate missing locations on the first and last day of a dataset when linear interpolation is not able to. Another benefit of our method is that since a model is created, we are able to get an uncertainty measure with the predictions. Our method should eventually be tested against curved methods, like cubic splines, to see if it will outperform simpler, curved-based methods. Despite the benefits our method, linear interpolation outperformed our proposed method in most cases. So using the additional information about the movement does not provide any benefit if the trajectories follow a linear pattern and only interested in location estimation. 

An issue with our interpolation approach is that when creating the intersections, if an object is not located within a spatial-temporal intersection, it is removed from the data used to develop the models, as only developing models within intersections. Thus, we could be losing valuable information to obtain the estimates. Typically this only happens to gpids on the edges of the ice pack. 

When it comes to developing the models with the Vecchia Approximation, some other issues arise. The first is that when the temporal range is large, especially in comparison with the spatial range, the model is not able to be developed as the log-likelihood does not converge. Generally when this happens the last Fisher Scoring iteration before the error has an exceptionally large temporal range value. Additionally, the covariance parameters estimated by the model do not seem to make sense practically even when the model can be successfully fit because all the parameter values outside of the nugget are large, even though the variance and spatial range values are sometimes more reasonable for the simulated data. For example, the large temporal range means that the data is not changing over time, and that there is a strong dependence over multiple lifetime's worth of data, which is probably not realistic. Using the simulated data as an example, in Simulation 1 the Y values for cluster two were generated with $\sigma^2$ = 80, $\phi$ = 50, $\tau$ = 10, $c_0$= 0. The values estimated for the model in intersection 3, time 3 were $\sigma^2$ = 77.91, $\phi$ = 458, $\tau$ = 686,239.164, $c_0$ = 0. In this scenario, the model is overestimating the spatial and temporal range parameters, with the temporal range being a much larger overestimate. This may be due to only working with a small amount of data that is not moving much over time. This is especially true when looking at the ice sheet data. With the simulated data, as more days were added to the datasets used to create the clusters and defines the intersections, the temporal range does slowly decrease. Also, @guinness_gaussian_2021 points out that when the range parameter estimates are much larger than the maximum distance between points, which we see with our data, the likelihood function has a hard time jointly identifying the range parameters and variance, causing issues in the calculation of our model. With the simulation data, we tried setting a value for the covariance parameters and found that if set $\phi$, then $\tau$ is still large. On the other hand, if set $\tau$, then $\phi$ is closer to the true value. Showing that the temporal dependence is posing an issue, which may be due to lack of days and lack of movement over the days. 

Another potential source for error when using a space-time covariance function is that variation in time is needed. Otherwise, if we only have observed data for one day of the three days used to create the models, the model is unable to be developed. Another issue, that was mainly seen in the models without the intersection in Week 1, the location columns were sometimes considered too linearly dependent to form a model. Generally, taking a random subset of the data allowed a model to converge. Finally, sometimes the estimated model can be developed, but an error occurs in the prediction. This occurs when we only have one location to estimate, as the nearest neighbor approach is used when computing the conditional estimates of the prediction. Thus, we need at least two missing points to create an estimate.  

# Conclusion {#conclusion}

Sea ice plays a vital role in the Earth's energy balance, as when cracks form in the ice, warm air from the ocean is released into the atmosphere. In order to determine where these cracks may form using only the movement of the ice sheet, a bounding box is created around each tracked trajectory and the features of the bounding box are used as input into k-means. Once each gpid is assigned a cluster, a plot of the results is created where the cluster boundaries would be the location where ice cracks may form. This method seemed to identify similar cracks as other ice crack detection methods. Next, using the information from the clusters, created a model for interpolation from spatio-temporal neighbors of the missing points. While linear interpolation typically outperforms our method, we showed some benefit with nonlinear and not highly sampled data. 

In the future, we want to determine the number of clusters through something other than the silhouette statistic. For example, @ossama_extended_2011 has a process for determining the number of unique directions, which may be a better estimate of k. Secondly, more work needs to be done in the univariate model development for the interpolation to facilitate convergence in a wider range of situations. Next, we would like to further investigate a bivariate interpolation model to estimate both x and y at the same time. Finally, at the moment our method is a two step process: find clusters and create model in each cluster. Eventually, we would like to turn this into a one step process, potentially using voronoi tessellations and a piecewise Gaussian Process. 